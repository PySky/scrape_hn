I've recently been exploring the exciting new world of asynchronous I/O libraries in Python 3 – specifically asyncio and curio. These two libraries make some different design choices. This is an essay that I wrote to try to explain to myself what those differences are and why I think they matter, and distill some principles for designing event loop APIs and asynchronous libraries in Python. This is a quickly changing area and the ideas here are very much still under development, so this text probably assumes all kinds of background knowledge and possibly that you live inside my head – but maybe you'll find it interesting anyway. I'd love to hear what you think or discuss further.

[Update, 2017-03-10: While the text below focuses on Curio, most of the commentary also applies to Trio, which is my new Curio-like library that came out of this blog post.]

So here's the tentative conclusion that spurred this essay, and which surprised the heck out of me: the more I work with curio, the more plausible it seems that in a few years, asyncio might find itself relegated to becoming one of those stdlib libraries that savvy developers avoid, like urllib2. I'm not saying that the library we'll all be using instead will necessarily be curio, or that asyncio can't possibly find some way to adapt and avoid this fate, or that you should go switch to curio right now – the practicalities of choosing a library are complicated. Let me put it in bold: This is not an essay about curio versus asyncio and which one is "the best". I'll talk a lot about those two libraries, but for present puroposes I'm profoundly uninterested in things like which one wins at such-and-such microbenchmark as of which-ever latest release, and I don't have any personal investment in either. The reason I talk about them is because they make good illustrative examples of two very different design strategies. The goal of this essay is to understand is the trade-offs between the "curio-style" design strategy versus the "asyncio-style" design strategy. So first, I'll try to articulate a conceptual framework for understanding what these two strategies actually are, and how they differ – this is something I haven't seen discussed elsewhere. Then to make that more concrete, I'll walk through some concrete examples using the two libraries, and see how these underlying design decisions play out in specific real world use cases. It turns out that in these examples, the "curio-style" produces better results; I'll try to pull out the general principles that explain why this happens, and that might give us hints on how to design or improve new APIs for both event loops and for the libraries that use them. Unfortunately, one of the conclusions I come to is that it's hard to see how these advantages could be "retrofitted" to asyncio – but I could be wrong, and at least once we understand them we can have a conversation about how to make Python's async I/O ecosystem as awesome as possible, whatever that ends up looking like; I'll conclude by sketching out some possible directions this could go.

The basic difference between asyncio and curio comes down to their attitude towards Python 3.5's new async/await syntax. But before we talk about the best way to use async/await, lets digress to talk about why async/await even matters. ...Actually I'm going to digress even more then that. Let's start by talking about what programming languages are for. It's easy to forget sometimes just how much work a modern language like Python does to guide and constrain how you put together a program. Like, just for the most basic example, consider how simply juxtaposing two statements expresses ordering: you know that won't start executing until has finished. Another example – the call stack tracks relationships between callers and callees, allowing us to decompose our program into loosely-coupled subroutines: a function doesn't need to keep track of who called it, it can just say to fire a value into the void, and the language runtime makes some magic happen so the value and the control flow are delivered simultaneously to the right place. Exception handling defines a systematic, structured way to unwind these decoupled subroutines when something goes wrong; if you think about it this trick of taking the call stack and reusing it as the unwind stack is really quite clever. blocks build on that by giving us an ergonomic way to pin the lifetime of resources – file handles and so forth – to the dynamic lifetime of these call stacks. Iterators track the state needed to bind control flow to the shape of data. These tools are so fundamental that they disappear into the background – when we're typing out code we don't usually stop to think about all the wacky ways that things could be done differently. Yet these all had to be invented. In functional languages, doesn't express ordering. Back in the 1970s, the idea of limiting yourself to using just function calls, loops, and if statements – 's hamstrung siblings – was *incredibly controversial*. There are great living languages that disagree with Python about lots of the points above. But Python's particular toolkit has been refined over decades, and fits together to provide a powerful scaffolding for structuring our code. ...until you want to do some asynchronous I/O. The traditional way to do this is to have an event loop and use callback-based programming: the event loop keeps a big table of future events that you want to respond to when they happen (e.g., "this socket became readable", "that timer expired"), and for each event you have a corresponding callback. The event loop takes care of checking for events and invoking callbacks, but if you want structure beyond that – like the kind of things we just discussed above: causal sequencing, delegation to and return from subroutines, error unwinding, resource cleanup, iteration – then you get to build that yourself. You can do it, just like you can use to build loops and function calls. Frameworks like Twisted and its descendents have invented all kinds of useful strategies for keeping these callbacks organized, like protocols and deferreds / futures and even some kind of exception handling – but these are still a pretty thin layer of structure on top of the underlying unstructured callback soup, and from the perspective of regular Python they're like some other mutant alternative programming language. That's why PEP 492 and async/await are so exciting: they let us take Python's regular toolkit for solving these problems, and start using it in asynchronous code. Which is awesome, because frankly, Twisted, I love you and deferreds are pretty cool, but as abstract languages for describing computation go then real-actual-Python is wayyy better. And with that background, then, I think I can articulate the key difference between asyncio-style event-loop APIs and curio-style event-loop APIs: Asyncio is first and foremost a traditional callback-based API, with async/await layered on top as a supplementary tool. And if you're starting from a callback-oriented base, then this is a great addition: async/await provide a major boost in usability without disrupting the basic framework. Asyncio is what we might call a "hybrid" system: callbacks plus async/await. Curio takes this a step further, and throws out the callback API altogether; it's async/await all the way down. Specifically, it still has an event loop, but instead of managing arbitrary callbacks, it manages async functions; there's exactly one way it can respond when an event fires, and that's by resuming an async call-stack. I'll call this the "async/await-native" approach. The main point I want to argue in this essay – the point of all the examples below – is that if you're using a hybrid API like asyncio, then you can ignore the callback API and write structured async/await code. But, even if you stick to async/await everywhere, the underlying abstractions are leaky, so don't get the full advantages. Your async/await functions are dumplings of local structure floating on top of callback soup, and this has far-reaching implications for the simplicity and correctness of your code. Python's structuring tools were designed to fit together as a system – e.g., exception handling relies on the call stack, and blocks rely on exception handling – and if you have a mix of structured and unstructured parts, then this creates lots of unnecessary problems, even if you stick to the structured async/await layer of the library. In a curio-style async/await-native API, by contrast, your whole program uses this one consistent set of structuring principles, and this consistency – it turns out – has pervasive benefits. What I'm arguing, in effect, is that asyncio is a victim of its own success: when it was designed, it used the best approach possible; but since then, work inspired by asyncio – like the addition of async/await – has shifted the landscape so that we can do even better, and now asyncio is hamstrung by its earlier commitments. To make that more specific, let's look at some concrete examples.

Clearly, something, somewhere, has gone wrong. What happened? Some of these issues with the asyncio streams-based API seem like easily-fixable oversights (but why are there so many?); others seem to point to something deeper. It's obviously not that the asyncio developers are just "bad at their jobs" or something – compared to curio, asyncio has more developers, with (as far as I can tell) a higher average degree of network programming experience, and has been put through considerably more real-world scrutiny and usage. So why does curio perform so much better in this comparison? Is there a generalization behind all these weird little problems? I think so. I propose that the difference is that curio follows one of the core principles of an async/await-native API, which is that it should respect causality. Which is a term I just made up. But what I mean by this is pretty basic: in Python, normally, if we write , then we know that won't start executing until after has finished. If this is true then we say that "respects causality". Causality is the fundamental property you rely on when writing imperative code, and Python is an imperative language. In Glyph's famous blog post Unyielding, he makes the point that if you have N logical threads concurrently executing a routine with Y yield points, then there are N**Y possible execution orders that you have to hold in your head. His point is that you can reduce this complexity by using cooperative threading like callbacks or async/await (= small Y) instead of pre-emptive threading (= large Y). Taking this further: Every time we schedule a callback – every time we call or or or or ... – then what we're implicitly doing is spawning a new logical thread of execution. Callback-based code has small Y, but large N. Which is another way of saying: traditional callback APIs show a flagrant disregard for causality. And this has infested even the async/await parts of asyncio. Most of our problems above happened because we started doing (reading the next chunk of data, shutting down the event loop, ...) while (writing the previous chunk of data, closing the socket, ...) looked like it had finished but was actually still going. Curio is different: every operation in curio is causal, except for the explicit concurrency-spawning primitives and [Edit: actually, I had this wrong – are actually causal as well!] Curio-style code has small Y and small N. When it comes to reasoning about code, implicit concurrency is... unhelpful. And callbacks by their nature are all about implicit concurrency. Of course it's possible to write causal APIs in asyncio, and non-causal APIs in Curio. But the underlying platform defaults have a major influence on what kind of APIs you'll tend to write, because causality is compositional: if some function is implemented using only causal subroutines, then it will necessarily also be causal. But if an API calls non-causal subroutines internally, then it will also be non-causal, unless it takes some explicit action to recover causality – which can be quite difficult. For example, here's the current implementation of , which lives on the async/await layer but inherits its non-causality from the callback-layer : We could (try to) make it causal by explicitly waiting for the write to complete before returning: Similarly in curio we could define a non-causal write function, equivalent to the asyncio version above: We can even define a version that spawns a child task and then waits for it, more-or-less equivalent to our hypothetical improved : But this would be extremely weird. Curio's decision to eliminate implicit concurrency and force all concurrency to start from an explicit makes concurrency a rare thing that you think about before using. It also has some more subtle consequences that together mean that when you do need concurrency, it's easier to manage: is an async function, which means that synchronous-colored functions cannot call it. Therefore, synchronous-colored functions are always causal – when a synchronous function call returns, we know its work has finished. Compare this to traditional callback-based APIs, where it's common to write innocent-looking code like but where the actual execution of and ends up overlapping. In curio you'd have to at least write . In asyncio, there are many different representations of logically concurrent threads of execution – callbacks, callstacks, callbacks, etc. In curio, there is only one kind of object that can represent a logical thread – – and this allows us to handle them in a uniform way. We'll see below that this greatly simplifies Event loop lifecycle management and Context passing / task-local storage. Because the spawn code is routed through , the event loop always knows not just what child task is being spawned, but which parent task is doing the spawning (i.e., it's whichever one emitted the magic ). Currently curio uses this to propagate task-local context from parent tasks to child tasks; in the future it could potentially track and expose these relationships to allow for powerful operations like "cancel this task and all of its child tasks, recursively". I'm not sure if being able to explicitly reason about and manipulate trees of worker tasks like this will ultimately turn out to be useful, but it opens up interesting possibilities. But of course main advantage of curio's causal-by-default approach is that we can write straightforward imperative code like our example proxy server and get something that works. These are extra benefits on top of that. Who needs causality, really? I'm throwing around these fancy terms like "causality", but does it actually matter in the real world? Here I'll take what we learned from studying the toy proxy example, and see how some production codebases handle these issues. Here's a simple stress test for an HTTP server: send it lots of GET requests, without ever reading the responses: It turns out that if you point this at a server running twisted.web, then our client will upload a few megabytes of data and then the server will crash. (Here's an example server in case you want to try at home; make sure to hit control-C before your laptop starts Death Swapping.) This is a backpressure bug: the server reads the first GET, generates the response, and writes that to its send buffer. Where it sits, because the client isn't reading. But the server isn't paying attention to the buffer, so as far as it's concerned the data has been sent, and it goes on to process the next request, generates another response, and appends that to the send buffer. Repeat until the send buffer swallows all available memory and the server falls over. We shouldn't exactly panic over this – this just is a denial-of-service attack, and it's impossible to fully defend against DoS. What makes this a bit embarrassing is the degree of amplification: if there's some URL on a server that returns, say, a 100 KB response body, then each ~1 MB of upload from a single client will permanently swallow ~2 GB of memory. Aiohttp (example server) is more robust against this particular attack – it will also crash eventually, but doesn't show the severe amplification. The reason is that aiohttp calls after processing each request, so the send buffer can't grow to unbounded sizes. The reason it still crashes eventually is a bit weird: it never applies back-pressure on the receive side, even though that's the case that will actually handle for you automatically... but aiohttp turns out to use the protocol API instead and its own ad hoc buffering code, which happily accepts and queues infinitely many request bodies without ever pushing back. So the amplification factor here is just 1x – this is a bug, but a relatively minor one. If an attacker is willing to upload a few gigabytes of data to an aiohttp server, they can crash it – but there are probably other things an attacker can do with a few gigabytes of data that are even worse (e.g. opening tens of thousands of connections), so meh. On the other hand, remember the problems we had with data being lost at shutdown? It turns out that aiohttp has a "graceful shutdown" mode, which gives all current connections time to finish before exiting – but as far as I can tell this uses and doesn't disable the low-water mark stuff, so it actually has no way to know when the connections have finished closing. I haven't verified this experimentally, but I strongly suspect that the "graceful" shutdown is randomly chopping off some of those connections before they're finished. If we turn to look at the the toy curio-based HTTP server that I wrote for some docs (yes, this says something about the relative maturity of the twisted/asyncio/curio ecosystems), then it avoids all of these problems. (Well, technically, it doesn't implement graceful shutdown, but if it did then it wouldn't run into the bugs.) Now, you've probably figured out by now that I'm the kind of paranoid person who worries about these things, so I won't pretend that I didn't think about this at all while writing that code. But when I thought about it, I realized that with curio, the most obvious naive implementation actually was correct, so I didn't need to do anything special. With twisted and asyncio, you have to do something special. And everyone makes mistakes. Consider a websocket server that accepts connections and then sends the client an ongoing, infinite stream of messages. This is a pretty common configuration – examples would include IRC-style chat apps, or a live twitter feed viewer. Now imagine what happens if we connect to such a websocket and then go idle, never reading: A naive server that doesn't respond to backpressure will keep trying to send us messages, buffer them in memory, and eventually crash. This attack effectively has an amplification factor of infinity: all we have to do is send two packets to set up the connection, and then we can walk away while the server slowly leaks to death. In fact, this can easily happen by accident when a client establishes a connection and then crashes or otherwise goes offline. Do any Python websocket servers implement this naive buffering algorithm? As far as I can tell, yes, by default, they all do. In fact, they all use a very similar API for sending messages that makes it difficult to avoid: they provide some kind of synchronous-colored method that queues a message and then returns immediately, and it causes the same kind of troubles that we had with asyncio's synchronous-colored method above. [Edit: it turns out there's an exception that I missed – the websockets package provides exactly the API that I advocate. Awesome!] aiohttp (example server): Doesn't respond to backpressure from slow clients by default. AFAICT has no API for doing so, so handling the disappearing client case is currently not possible without somehow changing the protocol use itself. The API for receiving messages does use and thus should be able to transmit backpressure upstream to clients who send too fast (though I haven't tested whether this works myself). autobahn (example server): Doesn't respond to or transmit backpressure by default. When running on twisted, then it is possible to get backpressure notifications for slow clients using the twisted API I mentioned early. When running on asyncio, this seems to be unimplemented, so responding to backpressure is impossible. Doesn't have any API for transmitting backpressure upstream to clients who are sending data too quickly. tornado (example server): Doesn't respond to or transmit backpressure by default. Has APIs available that allow a sufficiently dedicated programmer to implement both upstream and downstream backpressure. (Tornado FWIW is also immune to the GET flood attack described above – they've clearly put a lot of thought into these kinds of issues.) Even for the libraries where handling websocket backpressure is possible in theory, I couldn't find any mention of this in their examples, so it's doubtful that there are many deployments that actually take advantage of this. My goal isn't to shame the authors of these packages – obviously they're helping people solve real problems right now, and all I have to offer is some hypothetically-improved vaporware! Rather, I want to point out that there's a whole set of nasty edge-cases that are very difficult to handle when using the conventions of traditional callback-based APIs. But if these servers had exposed a causality-respecting API for websockets – one with methods like and – then these issues would simply go away. It's very rare that we can solve genuinely hard problems like this just by changing some API conventions – we should be excited! The tornado API is particularly instructive, because it's so close to what I recommend: their method returns a future. This means that in the asyncio/tornado async/await integration that allows to be applied to s, one can write code like and get correct backpressure handling. But, if we forget, and write instead... then the code will still seem to work! So despite the similarity, this isn't really a causality-preserving method, the way it would be if it were implemented as an async method that required to run. It's a method that spawns a concurrent thread of execution to do the actual work, while also providing an option to join that thread. Much better than nothing! But the end result is that if you look at their official examples, they don't actually check the return value. Of course tornado itself can't easily fix this, because they have to worry about backcompat and Python 2 support. But looking to the future, if we want to habitually write reliable software without breaking our brains, then causality needs to be opt-out, not opt-in, and the great thing about async/await is that it makes causal APIs just as easy to use as non-causal ones.

Timeouts are important, because they're ubiquitous – any code that does I/O and might be run unattended had better make sure that there are timeouts covering every single I/O operation. Yet, in many I/O frameworks, handling timeouts correctly is extremely difficult. For example, if you're doing synchronous I/O using the stdlib module, then you get the ability to set a timeout on each individual send and receive operation – but this is far too low-level. Curio instead offers a context manager that imposes a timeout on everything inside it. This means that we can straightforwardly take a function that performs some complex operation with multiple types of I/O, and impose a timeout on it as a whole: # A function that does some complex I/O # Read the server's interim response -- either telling us # to go ahead, or giving a final rejection: # Imposing a timeout on it from outside This approach is really brilliant. In the traditional system, every function had to manually implement complex timeout logic and expose it as part of its API. Here, the code living inside the context manager does need to handle cancellation correctly, but otherwise can be completely oblivious to timeouts. We use exactly the same API to impose a timeout on a primitive call and on a complex RPC operation. Timeouts can be nested. It's great. Of course this idea isn't unique to curio. You can implement the context manager style API in asyncio too. But – you can probably guess where I'm going to go with this – handling timeouts and cancellations in a hybrid callbacks+async/await system creates a number of unique and unnecessary challenges. First, since we can't assume that everyone is using async/await, our hybrid system needs to have some alternative, redundant system for handling timeouts and cancellations in callback-using code – in asyncio this is the cancellation system, and there isn't really a callback-level timeout system so you have to roll your own. In curio, there are no callbacks, so there's no need for a second system. In fact, in curio there's only the one way to express timeouts – kwargs simply don't exist. So we can focus our energies on making this one system as awesome as possible. Then, once you have two systems, you have to figure out how they interact. This is not trivial. For example, I suspect most people would find the behavior of this asyncio program surprising: # Start two tasks, and give them a chance to block on the same future. The fundamental problem here is that s often have a unique consumer but might have arbitrarily many, and that s are stuck half-way between being an abstraction representing communication and being an abstraction representing computation. The end result is that when a task is blocked on a , simply has no way to know whether that future should be considered to be "part of" the task. So it has to guess, and inevitably its guess will sometimes be wrong. (An interesting case where this could arise in real code would be two s that both call on the same ; under the covers, they end up blocked on the same .) In curio, there are no s or callback chains, so this ambiguity never arises in the first place. Next, there's the problem of actually implementing cancellation. For callback-based operations, this is certainly possible. It's just really difficult to do, and every cancellable operation has to carefully implement it from scratch. And in practice, basic primitives like don't support cancellation, which makes it very difficult to write cancellation-safe code on top of them. For example, here's the asyncio version of our HTTP upload example: # Pretend that StreamWriter has been fixed to attempt to expose a # causal API -- this example shows that that still isn't enough :-( # Read the server's interim response telling us whether # Imposing a timeout on it from outside There's an excellent chance that the timeout will fire after we've started the 10 MB upload, and are blocked in the call inside . If this happens, then will return early but the upload will continue, because it's happening in a logically concurrent thread! And note that this is really a another special case of a non-causality. Our function does manage to be causal so long as it completes normally. But if it gets cancelled, then from the perspective of the caller it has returned (by raising an exception) – yet the underlying operation is still going, which is the definition of non-causal behavior. There are ways to work around this, but in general, any function that calls any cancellation-unsafe functions is also going to be cancellation-unsafe by default, and it's hard to write much code in asyncio without calling unsafe functions like . I'm not even sure what a cancellation-safe version of would look like :-(. In curio, supporting cancellation isn't free, but it's much much easier: all primitive operations are cancellation-safe, so we start from a solid foundation, and then beyond that it basically comes down to writing code that properly cleans up in the face of exceptions. And this kind of exception-safety is a local property we can check on a function-by-function basis, is something we have to worry about anyway, and is often easy to handle because we can let Python's tools like context managers to do the heavy lifting. Remember how up above in our initial discussion of the different example programs, even before we got into the bugs in the asyncio versions, we noted that the curio version was simpler because it was able to take advantage of curio's system for shutting down the event loop when all non-daemonic tasks were complete? This system is possible because curio restricts itself to managing only full-fledged async function callstacks, not arbitrary callback chains. This means that curio has a complete high-level representation of what tasks are running, and a standard place to store metadata like which ones should be considered daemonic. And this isn't just a convenience: it also means that curio can guarantee that every task's cleanup code (e.g. blocks) are executed before shutting down. Asyncio doesn't really have any way to do the equivalent, even in principle. It can't tell whether a callback is "daemonic", i.e., whether it's associated with providing some background service like logging or whether it's some specific ongoing callback chain that's on the application's critical path; if it is a background service it has no way to tell whether it needs to be cleaned up somehow when the loop shuts down; and even if it does need some kind of cleanup, there's no way for the loop to "cancel" that callback chain and tell it to run its cleanup. Because callback-based execution threads are implicit, not reified, the user is left to keep track of these kinds of things manually. And any code that uses asyncio's protocol/transport layer is implicitly creating these kinds of anonymous callback chains. Obviously it's possible despite all this to write asyncio programs that correctly handle the event loop lifetime; the point is that because of asyncio's choice to use a hybrid design, it can't provide much help in solving these issues, so you're stuck doing it manually each time instead. Getting in touch with your event loop It's often useful to have multiple event loops in the same process. For example, we might want to spawn several OS-level threads and run an event loop in each, or we might want to give each of our tests its own event loop, so that we don't have stray callbacks left behind by test A firing while we're running test B. And this means that any code that does I/O has to have some mechanism to figure out which event loop it should be using. In principle, if you're using async/await, this should be trivial: async functions by definition have to be supervised by some sort of coroutine runner, and provides dedicated syntax for talking to this supervisor. In curio, the supervisor is the event loop itself; in asyncio, technically the supervisor isn't the event loop itself but rather an object, but objects hold a reference to the correct event loop. So if you're using async/await, this ambient event loop reference is always present and accessible in principle, and the language runtime makes sure that it's passed implicitly to callees without any effort on your part. But asyncio assumes that you might not be using async/await, so almost none of its APIs take advantage of this. Instead, it gives you two options. Option 1 is that you can manually pass along a reference to the correct event loop every time you call a function that might (recursively) do I/O. This is unpopular because it takes work and clutters up your code. (You can see a bit of this in the proxy examples above.) Plus it's frustrating since, as we said above, if you're a sensible person who uses async/await then this is forcing you to do redundant work that the runtime is already doing for you. Option 2 is to be lazy, and to grab the global event loop whenever you need it (which asyncio makes very easy to do – all asyncio API functions will default to using the global event loop unless you explicitly tell them otherwise). Of course the problem with this is that as we saw above, you can't assume that there is just one global event loop, which is why instead of having a single global event loop, the asyncio API instead allows you to specify a global object implementing the interface which encapsulates a strategy for introspecting the current context to determine which global event loop should be returned from each call to . So, you know, implement that and you're good to go. Just make sure the three different mechanisms for getting the event loop always give identical results and it'll work fine, because configuring redundant systems is always fun and certainly not error prone. I tease, of course. In practice this mostly works out well enough, and none of this is actually going to stop anyone from writing working programs with asyncio – this is probably the most minor issue discussed in this essay. But it's a source of ongoing friction, and causes real problems. OTOH, because curio is async/await all the way down, this friction just... goes away. Or rather, is never there in the first place. There are no global variables, no policy objects with LotsOfCapitalLetters, and nothing to pass around. If you need to issue some I/O, you call and the Python interpreter automatically directs your request to the right event loop. (Sorta like how values magically go to the right place.) Of course your test suite creates a fresh new event loop for each independent test and different tests never pollute each other's event loop by accident, that's just the easiest way to do it. Normally we don't even explicitly instantiate the event loop object; it's an internal implementation detail of . I don't see any way to fix this in asyncio, because it's – again – a sort of inevitable penalty for wanting to mix async/await and callback APIs in the same library. A small improvement would be to add a function like , so that leaf async functions could at least summon the correct event loop reference on demand whenever they were about to transition to the callback API, without it having to be manually passed down the call stack. But this still requires an error-prone manual step at that transition point, and that seems unavoidable as long as we have a callback-friendly API at all. I guess for the asyncio entry points that already are async functions (e.g. ) we could change the default so that does a call to rather than . But then, it could get pretty confusing if some API calls default to the right event loop, while others you have to make sure to pass it in explicitly because the defaults are different. [Edit: I may have been overly pessimistic! I'm told that asyncio's global event loop fetching API is going to be reworked in 3.6 and backported to 3.5.3. If I understand correctly (which is not 100% certain, and I don't think the actual code has been written yet [edit**2: here it is]), the new system will be: , instead of directly calling the currently-registered 's method, will first check some thread-local global to see if a is currently executing, and if so it will immediately return the event loop associated with that (and otherwise it will continue to fall back on the . This means that inside async functions it should now be guaranteed (via somewhat indirect means) that gives you the same event loop that you'd get by doing an . And, more importantly, since is what the callback-level APIs use to pick a default event loop when one isn't specified, this also means that async/await code should be able to safely use callback-layer functions without explicitly specifying an event loop, which is a neat improvement over my suggestion above. I think it's still illustrative of my general point here that asyncio required three Python releases in order to settle on a system that uses multiple layers of complex logic just to get back to the place where curio started. But as long as end-users aren't peeking under the covers then they shouldn't notice much difference anymore, at least in this regard.] Suppose we have a server handling various requests, where each request triggers a complex set of events – RPCs, database calls, whatever. When monitoring and debugging such a server, it's very useful if we can arrange for each incoming request to be assigned a unique id, and then make sure that all the logs generated deep inside (for example) the database library are tagged with this unique id, so that we can later aggregate the logs to get a complete picture of an individual misbehaving request. But how does the logging code inside the database library find this id? Ideally we could pass it down as part of an explicit "context" object, but this isn't always practical, especially given that the Python stdlib module doesn't provide any way to do this. What we need is some sort of async equivalent to "thread-local storage", where we can stash data and make it accessible to a complete logical execution flow. In callback-based frameworks, this kind of context propagation requires modifying every callback-scheduling operation to capture the context when the callback is scheduled, store it, and then restore it before the callback is executed. This is challenging, because there are lots of callback-scheduling operations that need to implement this logic, and some of them are in third-party libraries. In a curio-style framework, the problem is almost trivial, because all code runs in the context of a , so we can store our task-local data there and immediately cover all uses cases. And if we want to propagate context to sub-tasks, then as described above, sub-task spawning goes through a single bottleneck inside the curio library, so this is also easy. I actually started writing a simple example here of how to implement this on curio to show how easy it was... but then I decided that probably it made more sense as a pull request, so now I don't have to argue that curio could easily support task-local storage, because it actually does! It took ~15 lines of code for the core functionality, and the rest is tests, comments, and glue to present a convenient -style API on top; there's a concrete example to give a sense of what it looks like in action. I also recommend this interesting review of async context propagation mechanisms written by two developers at Google. A somewhat irreverant but (I think) fair summary would be (a) Dart baked a solution into the language, so that works great, (b) in Go, Google just forces everyone to pass around explicit context objects everywhere as part of their style guide, and they have enough leverage that everyone mostly goes along with it, (c) in C# they have the same system I implemented in curio (as I learned after implementing it!) and it works great because no-one uses callbacks, but (d) context propagation in Javascript is an ongoing disaster because Javascript uses callbacks, and no-one can get all the third-party libraries to agree on a single context-passing solution... partly because even the core packages like node.js can't decide on one. Incidentally, looking at Javascript makes me grateful again for the care and deliberateness of Python's maintainers. It's possible that asyncio and its hybrid approach might turn out to be a dead-end, but – if so, then, oh well, at the end of the day it's just a library. We can recover. Javascript OTOH is getting async/await any day now, but the language is all-in on callbacks, so Javascript's async/await will always be the hybrid kind, and AFAICT many of this essay's critiques apply. A libuv-backed curio running on PyPy 3 might someday make an extraordinarily compelling competitor to node.js. So this is another domain where asyncio's hybrid nature creates a challenge: of course it would be easy enough to implement a task-local storage system that works only for asyncio objects, but then we'd still have the question of what to do for callback-based code, and how to handle the hand-off point between them. Probably there is some solution, but finding it seems like it would take a lot of work, and the benefit isn't clear given all the other issues that callbacks create. Finally, I want to say a few words about internal implementation complexity. Just like you can implement anything with instead of structured programming, with enough work, it's almost certainly possible for asyncio to eventually fix these bugs, implement reliable cancellation for all of its callback-based primitives, and so forth, and eventually expose a curio-style API on top of some sort of callback-based infrastructure. And from the user's point of view, this would be fine – it doesn't necessarily matter to them what's happening under the hood, so long as the public semantics are right, and users could just ignore all that callback-based stuff. But... asyncio's internals are really hard to follow. This isn't a criticism of the authors. I've highlighted a number of problems here, but what you can't see is all the times that I was convinced that I'd found another nasty edge case bug, only to eventually realize that someone else had already noticed the problem and carefully arranged to handle it. Asyncio is very thoughtfully written callback soup; the problem is that callback soup is just too complicated for human minds – at least, my human mind – to understand. The end result is that I'd estimate it took me ~3x longer to understand the actual semantics of 's and 's public APIs than it did to read all of curio's source code, implement task-local storage, and diagnose a concurrency bug in . (By the way, I recommend reading through curio as an exercise; it's not perfect – as that bug indicates :-) – but overall it's remarkably small and straightforward.) And if that's what it takes me just to understand asyncio, then I wince to think how much energy was spent on implementing it and fixing all those edge cases in the first place. If you're an event-loop developer, or the author of a protocol library, then is this really how you want to be spending your time? On implementing complicated callback-based APIs, and then on implementing another complicated layer on top of that to cancel out all the problems introduced by the callback-based APIs, and then spending a lot of time trying to squash all the weird edge case bugs introduced by the impedence mismatch between the layers? It's all so unnecessary! It's 2016, and you don't have to live like that anymore! AFAICT going async/await-native is a direct path to fewer bugs and more happiness.

Review and summing up: what is "async/await-native" anyway? In previous asynchronous APIs for Python, the use of callback-oriented programming led to the invention of a whole set of conventions that effectively make up an entire ad hoc programming language, in the sense that they provide their own methods for expressing basic computational constructs like sequencing, error handling, resource cleanup, and so forth. The result is somewhat analogous to the bad old days before structured programming, where basic constructs like function calls and loops had to be constructed on the fly out of primitive tools like . In theory, one can do anything. In practice, it's extraordinarily difficult to write correct code in this style, especially when one starts to think about edge conditions. Now that Python has async/await, it's possible to start using Python's native mechanisms to solve these problems. Python's tools are, unsurprisingly, a huge improvement over the old system; but, when used in a hybrid system layered on top of the older callback approach, many of these advantages are blunted or lost. We've seen above that going to a fully async/await-native approach let us easily solve a number of problems that arise in the hybrid approach, like handling backpressure, avoiding bufferbloat and race conditions, handling timeouts and cancellation, figuring out when our program was finished running, and context passing, while reducing our need for global state – and the code is simpler too, both for the library implementors and the library users! These advantages come from consistently following a set of structuring principles. What are these principles? What makes a Python app "async/await-native"? Here's a first attempt at codifying them: An async/await-native application consists of a set of cooperative threads (a.k.a. s), each of which consists of some metadata plus an async callstack. Furthermore, this set is complete: all code must run on one of these threads. These threads are supervised: it's guaranteed that every callstack will run to completion – either organically, or after the injection of a cancellation exception. Thread spawning is always explicit, not implicit. Each frame in our callstacks is a regular sync- or async-colored Python function, executing regular imperative code from top to bottom. This requires that both API primitives and higher-level functions *respect causality* whenever possible. Errors, including cancellation and timeouts, are signaled via exceptions, which propagate through Python's regular callstack unwinding. Resource cleanup and error-handling is managed via exception handlers ( or ). These work together so that if each piece of our program follows the rules, we end up with strong global guarantees. For example, if an error occurs: (5) tells us that this raises an exception, (1) tells us that this exception will be on one of our thread's callstacks, (4) implies that the exception interrupts execution at a well-defined point in time (for example, we know that code that comes before the exception-raising point is no longer running once we start unwinding), and (6) implies that resources will be cleaned up appropriately as the exception unwinds. Or for any given resource, that analysis + rule (2) gives us confidence that the resource will eventually be cleaned up at an appropriate time. Rules (4) + (5) + (6) together justify the use of a -style composable timeout API. These might seem almost too trivial to write down, and indeed, if you delete the word "async" then these regular synchronous Python code generally follows all of these rules without anyone bothering to mention them. Yet writing them down seems useful – until curio, every asynchronous I/O library for Python violated all of them!|||

