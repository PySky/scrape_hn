Computers currently understand us by mapping what we say to symbols or vectors and then performing inference based on those data structures. As I argued in a previous post, their understanding is primitive because their models for inference are not grounded in the basic concepts that make up our world. How can we encode commonsense knowledge so that computers can use it for understanding?

Maybe computers can understand us by simulating our world using a 3D physics game engine, such as Unity. Imagine if a person said, “I walked home.” To understand this sentence, the computer could first place a token representing the person at an arbitrary starting position in a simulated space and then move it over time until it was inside a hollow cube labeled “home.” By running this simulation, it would infer that when a person goes home, the person is now at home and not somewhere else. Consider another example. If someone said “she pushed the table with the book on it,” the computer could generate a table with a book on it and then apply force to the side of the table. The computer could then look at what happened in the simulation to know the full meaning of what the speaker said. It would infer that when the table moved, all of the objects on the table moved.

These inferences seem mind-numbingly trivial to us, but that is precisely why they have been so hard to implement in computers. We humans learn these concepts before we start kindergarten, and since the number of such inferences is so numerous, AI researchers haven’t been able to figure out how to write them all down. The result is that our computers lack commonsense knowledge and they appear brittle and dumb.

The advantage of enabling computers to simulate our world is that the results of simulations are both rich and readily available for inference. By “rich” I mean that a lot of information would be available, should it be needed to understand a concept. For instance, if a human and a computer were discussing how a tree now blocks a second-story window when it didn’t five years ago, the computer could understand this by simulating how trees grow and instantiating a viewer and noting how a tree can block a view. To achieve these rich representations, AI practitioners would need to define all of the concepts that could be simulated and how to simulate them, but the practitioners wouldn’t need to anticipate all of the possible interactions between concepts or the inferences that the computer would need to make. Once the simulation capabilities were programmed in, the inferences would be readily available because the computer could simply look at the interactions that took place during the simulation. To know that the book moved when the table moved, the computer could just query the position trace of the book in the simulation.

Supporting evidence for the benefits of this approach comes from experiments showing that humans use simulations to understand language [2]. For example, the cognitive psychologist Benjamin Bergen [2] outlines how we activate the relevant parts of the brain’s motor system when we understand action words. Consider the example of someone saying that she was bitten while feeding a monkey. Most of us have never fed a monkey, but we all know where she was bitten [10]. The developmental psychologist Jean Mandler [1] argues that children encode perceptual information into a simpler and more accessible form using a representation called image schemas. We can think of image schemas as concepts that consist of a set of components that are related by definite structure [4]. Her theory is called perceptual meaning analysis, and in [1] she says that “the image-schemas that perceptual meaning analysis creates are analog representations that summarize spatial relations and movements in space” (p. 79).

The simulations discussed here could serve as computer implementations of image schemas. In fact, we can think of image schemas as little situations that need to be simulated. The most fundamental image schemas such as force and contact are derived from physical interactions with the world, which would come directly from the physics simulator. The physics simulator would also provide associated physical concepts such as gravity, friction, and blockage, which would all be relevant for making inferences about physical objects. Although many image schemas and associated physical concepts would come for free with the physics simulation, others would have to be programmed in. One example is attraction. It would be added to the simulator as a little program that exerts force on objects in the direction of another object.

We can generalize the idea of image schemas to being complex concepts that computers can understand through simulation. For example, we can enable a computer to understand a complex concept such as “chase” as in “the cat chased the mouse” using a pair of algorithms, one for chasing and one for being chased. To represent what might happen in a chase, the simulator could run those algorithms. The system could even be extended to recognize when one thing chases another in the world by mapping perceptual input to these algorithms.

We can see how simulation would be used to understand physical objects and events, but what about abstract things? Abstract concepts could be understood through metaphors to physical experience [3]. I mentioned that attraction is an image schema. Given an implementation of attraction, a computer could understand the idea of a person being attractive through a metaphor from the abstract to the physical implementation. A person who is attractive pulls other people toward him or her. I also discussed how complex concepts such as chase could be understood through simulation. Using an implementation for chase, a computer could similarly understand the old-fashioned notion of a boy chasing a girl through metaphor to this concrete implementation. If the boy and the girl were to fall in love, the computer could understand their relationship even more abstractly either through the metaphor love is a journey [3], if the computer had an implementation for journey, or, through the metaphor love is war [3], if it had an implementation for physical conflict. The metaphor the computer chose to understand their relationship would depend on the nature of their particular relationship, of course, as we can all relate.

Computer understanding through simulation is a promising approach because in addition to being rich and readily available for inference, simulations are composable [5]. To understand language, the computer would build simulations based on pieces of simulated concepts, and each simulation would be unique because it would consist of a different combination of concepts. This composability would allow the system to make inferences in situations it had never seen before, such as what would happen if a turtle were dropped out of an airplane.

There are many challenges. Even though it is efficient to factor the world into concepts and simulate individual concepts, coding up how to do pieces of simulation for each concept is still a huge task. We will need to encode world knowledge about objects, relationships, and dynamics. I mentioned the example of a growing tree. If a person says that she planted a tree fifteen years ago, to infer the current size of the tree, a computer would need to estimate how big the tree was when it was planted and how much a tree should grow every year.

In general, the value of a simulation is limited by the richness of the simulation. And the richer we want it to be, the more knowledge we have to build in. If a computer needs to infer how a person got into their house, the simulation needs to include the person going through the door. This is a big task. Ideally, a robot would learn how to simulate concepts through active experience in our world, just like children do. Autonomous learning would save the effort of having to code knowledge in, and it would enable the computer to have a more flexible intelligence. Unfortunately, effective machine learning requires that one first define a hypothesis space (a set of possible model representations) that aligns with the problem, and until we have the understanding necessary to define this space, programming simulations into a physics simulator seems like a good place to begin.

How do we go about getting started? We need to begin in a small world, but as I explained in my previous blog post, it has to be our world. To understand human language, computers need to understand the concepts we express in language. Complicating matters further, language is more than the literal meaning of utterances; computers need to understand the general situation so they can pragmatically understand statements in context [6]. There has been some great academic work toward using simulation for understanding, such as [7][8][9], but I think this problem is ripe for a startup. To commercialize this idea, the initial domain would need to be both immediately useful and relatively constrained. I have an idea for such a domain, and I’ll share it in my next blog post.|||

Computers currently understand us by mapping what we say to symbols or vectors and then performing inference based on those data structures. As I argued in a previous post, their understanding is…