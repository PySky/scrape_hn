People write source code, machines run machine code. A compiler turns one into the other—how? Somehow stones, taught to read our commands, obey, and the compiler acts at the heart of this magic: it’s the spell that interprets the spell.

Take a course following one of the excellent traditional textbooks such as the Dragon Book—whose cover art acknowledges the aura of the fearsome and uncanny around the subject—and you might spend all semester building one compiler, for a language much simpler than the one you write it in. Did you just call on a big genie to make a small one?

In this article, we’ll make a self-reliant genie—that is, a small compiler able to compile itself. We’ll write it in and for a subset of Python 3. The result—call it Tailbiter—will be a toy, but less of a toy than usual in an introduction: besides the self-compiling, it’ll include some details of debugging support. To make room for these emphases, we will ignore the topic of optimization—or, in plainer terms, of making the output code less stupid.

All you’ll need to follow along is comfort with recursion over a tree, nested functions, and reading a program dependent on some built-in Python libraries and types that I’ll survey but not delve into.

I wrote this article to accompany “A Python Interpreter Written in Python” by fellow Recurser Allison Kaptur. Her chapter explains the CPython bytecode virtual machine, which takes the output of a compiler and executes it. I’ve tried to make this article self-contained, but you’re encouraged to read the other as well. You may also find it helpful to read or run the code without all this prose around it.

So, where do we start?

When I began this project, I knew quite a bit about how to write a compiler, but not very much about the Python virtual machine which we’re targeting. Right away I hit the first snag the textbooks don’t talk about: inadequate documentation.

We’ll respond to this challenge by building in stages, from a seed just capable of turning the simplest source code into working bytecode, learning from each stage how to grow into the next. Start with an example of our input and our output: a trivial program, from its source text to parsed syntax and then to runnable bytecode. This technique of “growing” your program in small stages is useful when working in an uncertain domain.

The first step is to dissect this text and expose its grammatical structure: that is, to parse it. The parsed form is called an abstract syntax tree (or AST); Python’s function can produce it for us. How works would take a whole separate article.

We get an whose is a list of more of these objects—in this case two, an representing and an representing the call to —each of these being built out of further objects: a tree of them. Each object has fields proper to its type, plus a (line number) and telling where in the source text it was parsed from. We’ll sweat these details more when we get to them.

You can find all the AST classes and their fields in Python.asdl in the Python 3 source distribution. It’s 100+ lines long, with roughly one line per class; we’ll need to handle a fair fraction of them to achieve our goal of a self-hosting compiler.

Compiling the module (with Python’s built-in compiler, for now) yields a code object, which is an internal Python type:

It has a bunch of attributes prefixed with :

What happened to our program? Mostly it’s encoded into : a sequence of bytecodes in a bytestring. These bytecodes can be “disassembled” (made human-readable) using :

On the left are source-code line numbers (1 and 2); down the middle go bytecode instructions, each labeled with the address it’s encoded at; and on the right each instruction may get an optional argument. The first line, then, shows us the instruction at address 0: a instruction, with 0 as an argument. (For a the argument means an index into the attribute, whose 0th entry, above, is indeed .) This instruction appears in as : one byte which happens to mean , followed by two bytes encoding the integer 0. (Don’t worry yet about the exact details; we’re looking for a general impression of how the program is encoded.)

Since that first instruction took three bytes, the next ( ) appears at address 3. It also has 0 as its argument, but this time indexing into . (That is, at runtime the bytecode interpreter will take the value it just loaded and store it in the variable listed at index 0 of : in this case, .)

Those two bytecodes implemented the first line of source code ( ). The second line’s bytecode follows at addresses 6 through 18, with some new wrinkles in instruction arguments: has two, each of one byte; and has no argument, making the whole instruction fit in a byte. (What does, we’ll come back to; likewise the details of how ’s arguments get passed.)

Finally, the last two instructions (at 19 and 22) return from running the module.

Zooming back out, what did the compiler do? It converted a tree structure into a sequence of instructions to perform the operations in the right order. Each subtree of the parse tree turned into a subsequence of the instructions. Diagramming the AST together with the assembly code shows this correspondence:

Many of the nodes in this tree turned into one bytecode instruction each, in almost the same order. ( precedes its arguments in the tree, but follows them.) That’s a common pattern but with plenty of exceptions.

The bytecode we just saw was meant for CPython 3.4; in other versions of Python the bytecode varies, maybe even for this tiny example. It might crash other interpreters. The AST types can also change from version to version, though typically less, and with less drastic consequences (an exception instead of a crash or a subtle low-level bug). In this article we’ll stick to Python 3.4; you can find the tweaks for 3.5 or 3.6 in the code repository.

Now that we’ve defined our problem a bit better, we can take our first step: building a compiler that can compile . We will do this by somehow translating the ast produced by into bytecode.

To create the code object we saw above, it’d help to be able to write Python code that resembles the disassembly. Like this:

Later we’ll get to the support code that lets us write this; first let’s see how it’s used. tells which source line the following instructions derive from; it’s a pseudo-instruction, supplying debug info to the code object, instead of going into the bytecode string with and the rest. These symbolic instructions and pseudo-instructions are all Python objects that understand to mean concatenation. Their “sum” then is also an object representing assembly code (as in the Gang of Four “composite” pattern, or a monoid if that’s how you were raised). Thus, we can build our code in pieces to be strung together, like

which produces the same bytecode: it doesn’t matter how we chunk the assembly.

There are a couple of reasons why this intermediate “assembly language” is a useful place to start building our program. Since the bytecode specification is a source of major uncertainty for us, this layer isolates our tinkering with the bytecode to a very specific place in our program. Since the bytecode specification also changes more often than the AST spec does, this also provides us with an “insulating layer” between these two problem domains.

A higher-level assembly language could’ve been made where instead of this example would say , leaving it to the assembler to turn into an index into . Likewise would take a string argument, and so on. Such a design would better suit a general-purpose bytecode assembler; but Tailbiter will be ruthless in doing only what’s needed. It’s simple for the code generator to encode the arguments into integers, more so than for the assembler.

We want to build a code object, but we don’t know all the details that go into one. To face these uncertainties and start learning, let’s make a complete working system for a tiny core subset of the problem: just enough to run our .

Figure 0 lists the AST classes we’ll need to handle, and their fields. The second line, for instance, means that one type of statement is , which has a list of target expressions (the means a list) and a value expression. This represents statements like , with and as targets.

CPython uses a tool to generate all of the AST classes from these declarations—it’s another tiny compiler of a sort—though to us they’re just documentation.

Here is the whole compiler as a “literate program”: a part in angle brackets like stands for more code we’ll see later. When we do, we’ll show it starting with . Later we’ll get to two fancier versions of the same compiler, and they’ll sometimes use chunks like (replacing the earlier version, ) and (appearing in and also ; this lets us say to add to and not replace ).

Note that we’ve chosen to write a very small but working compiler as our first step—the code generator and the assembler will implement only fragments of the full job of a code generator or assembler, just the fragments needed by our first Python subset. Alternatively, we could have restricted our exploration to just one of the layers (e.g. translating our assembly to bytecode) and completed it before moving on.

The technique of writing a minimalist but fully-functioning prototype is often called a “spike.” This is because we are writing a program that “drives” through all the layers that we think we are going to exist in our finished product.

The goal of a spike is to detect any obvious problems in our overall design as early as possible. It is thus important that we accept we may have to throw all of this code away—in fact, there are many development methodologies that require you to throw away the code and start again, no matter how successful the spike was in proving the design.

Let’s further explore our proposed spike. removes the initial to leave the command-line arguments the same as if we’d run Python on the source program directly: thus (with the compiler in ) you can run , or , or (eventually) …

Throughout the compiler, (for “tree”) names an AST object (also called a “node”). Here gives us a tree, we make a code object from it with (to be written), then we let CPython execute the code to populate a new module object. These are the same actions as Python performs when you a module, except for calling on our new compiler in place of . (We won’t look into how Python’s import-hook machinery could let us make call on our loader instead of the default one.)

The last underspecified step in our spike is , which we will implement in . This function takes the module’s AST and converts it to bytecode. This means that we’re going to need some tools to help us traverse the AST.

We could try doing this from scratch, in a style like:

However, the visitor classes in Python’s module make our task a little more convenient:

This is called like so:

The visitor pattern separates the concern of traversing the tree (which is the same every time) from what we actually want to do in each traversal (which is situational). This lets us define separately-callable methods one a time rather than inserting s into a master function. These single-purpose methods are easy to understand, test, and reuse later if the need arises.

We’ve designed our compiler as a visitor that returns assembly code. As it walks through the tree, it remembers the names and constants it’s seen, so that the emitted instructions can refer to names and constants by index. After the walk it assembles the assembly code into a code object.

Let’s instantiate this code-generation visitor and set it to work in our main compiler flow:

Our visitor will have:

Recall the disassembly of a module, above: there was the body code, then of , then . We turn that assembly into a code object:

Note that we begin the traversal process by calling —this is because is a callable object. This means when you invoke an instance of CodeGen as if it is a function, Python will translate that into . So what does do on ?

It takes an AST node or a list of such nodes; when it’s a list we get back all the results concatenated. We could define separate methods for visiting a list vs. a node, but conflating them turns out to unify some code to come.

Once we visit a node, producing its assembly code, we can annotate the assembly with the node’s source-line number. This spares us from writing the same in every method.

We haven’t implemented yet, but we’re calling it here. This is the default, superclass implementation of the method: it calls the right method for the type of the node visited.

If this is missing, will then call the template method . This should never happen; however, we’re certainly going to make some mistakes during development. The default implementation of does nothing, which makes it hard for us to notice when we’ve tried to handle a node type we’re not ready for yet. So, let’s make some noise if this ever happens:

wants to make a code object that’s littered with fields:

These tables are built from defaultdicts that grow as we walk the tree, to appear as tuples when collected into the code object:

For and , the keys are the name strings; but it gets trickier for . Equal names get the same slot in a names table, but “equal” constants might not: for example, is true, but and are nonetheless distinct. Therefore we key on the type as well as the value of the constant:

There’s a further subtlety in comparing with signed floating-point zeroes, which we’ll punt on by expelling negative zeroes in constants from the subset of the language we’re going to handle. A module, not presented in this article, can be called to make sure the input program is in our subset.

Our first tiny mini-Python understands only assignment and expression statements, where the expressions may be names, simple constants, and function calls. A constant expression turns into a instruction:

A variable name also becomes a single instruction, but the choice depends on context: a name can appear as the target of an assignment, not just in an expression. Python ASTs use the same node-type for both roles, with a field to distinguish them:

Later, when we compile functions and classes, they’ll need fancier logic for loads and stores because names will live in different scopes; but for now all names are global:

Now, function calls: a call like compiles to

Evidently the load instructions stash their values somewhere for to use. That somewhere is the “stack:” a growing and shrinking list of values. Each load appends to it, and each call removes a function and its arguments from the end and replaces them with one value: the result of the call. This scheme gives for a more complex expression like a place for the partial results to live:

The assembly code for the full, compound call is a concatenation of the assembly for its parts: if you compiled just or , you’d get some of the same code above (except perhaps for the choice of indices assigned to the names and ).

We recursively generate code for the function, the arguments, and the keyword arguments (which in turn are built from the keyword name and the value expression), then chain them together with at the end. The stack made this code generation simple: if instead we’d been assigning values to numbered registers, for instance, then we’d have had to keep track of these assignments and generate code depending on them.

Executing a statement should leave the stack unchanged. For an expression statement (consisting of just an expression, typically a call), we evaluate the expression and then remove its value from the stack:

An assignment evaluates the expression as well, then stores it in the target; all the store instructions also pop the stack.

The complication here deals with multiple assignments like : there’s a list of targets ( and ). If we followed the expression’s code with two store instructions, the second would be stuck without the value to store, because the first popped it off; so before the first one, we insert a instruction, which will push a duplicate reference to the value.

We still need to create assembly code—instructions, pseudo-instructions, and concatenations—and to compute three functions of it: the maximum stack depth, the line-number table, and the encoded bytecode. The most direct and minimal stub represents an assembly instruction as its final bytecode sequence, makes the line-number table empty, and pretends the stack depth is, say, 10—don’t try it with too-complex nested calls.

We fill in so that and all the rest work.

And at last works. Hurrah!

Now that we have driven our spike through all of the layers of our compiler architecture, we can think about how to build out more complicated use cases. We don’t yet have any control flow in our Python subset, which is a pretty critical omission. We can’t either, meaning our programs can’t even get at Python’s standard library in the usual way. Let’s tackle these issues, and handle more of the basic expression and statement types, in version 1.

At this point, it’s worth asking ourselves if we’re going to face any problems in v1 that are materially different from what we faced in v0. What if we made a design decision in v0 that makes it impossible to proceed?

Indeed, control-flow constructs like - do present us with a challenge we haven’t faced yet: they require us to “jump around” in the bytecode.

For example, the statement

where does what it says: pops the value left by , tests it, and if it’s false jumps to index 13—that is, executes that instruction next. Otherwise execution continues to the usual next instruction, at 6. likewise jumps to index 17, to skip if we chose .

Why’s this a problem that our current design can’t handle? During code generation the compiler may not yet know the bytecode index where the jump target will be.

Our answer: we augment the assembly language with symbolic labels for the targets, to be resolved later by the function.

A label like can appear in two kinds of places: as an instruction’s argument (like ) or on its own (like ). will have to note the position where it appeared in the code on its own, and encode the bytecode address of that position for the instruction.

This design is not inevitable. Notice that the example’s argument was encoded as 4, when the target is at index 17: 4 means the distance, taken from the instruction right after the jump (at index 13), to the target. Encoded this way, it’s a “relative” jump.

Suppose all the jumps were relative. We could easily compute the offsets, just knowing the encoded size of each block of code: e.g. the offset from the to is the size of . There’d be a similar computation for the jump to . We could skip the symbolic assembly phase and keep generating bytecode strings directly, like Tailbiter v0.

Sadly for this dream, here encodes its target differently, as the absolute address 13. We could consider designing this part more like a “linker” than an assembler, but it is safer to keep to CPython’s design: it makes surprises less likely.

Let’s make labels work, and unstub the rest of the assembler: computing stack depths and line-number tables.

Assembly code is now some kind of object we’ll define—several kinds, for instructions, labels, , and a couple more. To turn it into bytecode, we first resolve the addresses of its labels (starting at address 0), then encode it into bytes using the addresses now known—a “two-pass assembler.”

For the maximum stack depth, we ask to compute the depth after every instruction, appending them all to . (This uses more space than needed, to make ’s job simple.)

Computing the line-number table calls on a method which yields pairs of (bytecode address, source-code line-number), the smaller addresses first. consumes them and encodes them into a bytestring in a format imposed by the VM interpreter, designed to try to keep the table small: usually just a couple of bytes per line. Each successive pair of bytes in the table encodes an (address, line_number) pair as the differences from the previous pair. With a byte’s limited range, 0 to 255, this design faces two problems:

And can’t be just anymore:

Assembly objects will need an method for all the code we’ve been seeing like . Python’s calls the same method.

The simplest assembly fragment is the no-op:

For ’s use, all our assembly objects hold a counting how many bytes of bytecode they’ll become. We’ll take it to be constant, since we don’t support the extended-length argument format. (Suppose there were a relative jump to an address over 65535 bytes away. The jump instruction would need to occupy more than the usual 3 bytes; if we’d assumed 3 bytes, this could imply cascading changes to other offsets.)

A is just like , except resolving to an address.

So is a except for adding to the line-number table.

There are four kinds of bytecode instruction: absolute jumps, relative jumps, and non-jumps with or without an argument. (‘Jump’ for our purposes means any instruction taking a label argument.)

computes the stack depth after the instruction as a change from the depth before it: for example, a instruction will remove two values from the stack and replace them with one, for a net change in the stack depth of -1. gives us this net change.

What about jump instructions? The stack depth right after a jump instruction could be unrelated to the depth before it: instead the depth should be the same at the jump target. CPython’s compiler therefore traces through the jumps, propagating the depths along every path and making sure they’re consistent; but Tailbiter is much simpler. We can almost get away with treating jumps like other instructions, because the code for a statement has no net stack effect: anything it will push on the stack it will later also pop. An if-then-else statement, as we saw above in , has a test expression, a then-block, and an else-block, with branch and jump instructions in between. The stack depth at the start of the else-block should be the same as at the start of the then-block, but that’s just what it would be if we got there by running the then-block instead of branching around it: we can use the above logic unchanged, as long as of the jump instruction is also 0, which it is.

The same nice coincidence doesn’t quite hold for an if expression (like ). If we compile this the same way—the code for , a branch, , a jump, and —then the stack after will be 1 deeper than before it. If we’re to analyze the stack depth as if this were all straight-line code with no branches, then we need an extra note that the stack at the start of is one slot less deep than this analysis would otherwise arrive at. We can hack this into shape by inventing a pseudoinstruction to include at that point in the code:

So an if expression is compiled exactly like an if statement, except for the added directive:

We’ll see appearing in a couple more places later for the same sort of reason.

A catenates two assembly-code fragments in sequence. It uses to catenate the label resolutions, bytecodes, and entries produced in the different methods.

Dict literals like turn into code like

The argument to gives the runtime a hint of the dict’s size. It’s allowed to be wrong, since dicts can grow and shrink—but not to overflow the two bytes allotted to an argument in bytecode. Thus the .

Like name nodes, subscript nodes can appear on the left-hand side of an assignment statement, like . They carry a field just like a name’s.

A list or tuple can also appear in both load and store contexts. As a load, becomes

where 2 is the length of the list. Unlike with the length must be exact.

A unary operator works just like a function call except it gets its own instruction, for efficiency. gets compiled to

Binary operators get compiled the same way:

compiling, for example, to

Comparisons, like , take a slightly different form in the AST, because Python treats a chain of comparisons specially: is a single expression as a single AST node holding a list of operators and a list of their right operands. Our subset of Python doesn’t cover this case, only binary comparisons like .

For and expressions, like , we need to be careful about the stack depth across different branches of control. compiles to

only pops the tested value if it came out false. At index 12, whichever way execution got there, the stack has the same height: two entries.

Since a collects all of the subexpressions of an expression like a and b and c (instead of Python representing this as a tree of binary s), we must over the list.

is needed here because, for some reason, for these two branch instructions gives the stack effect seen if the jump is taken, instead of at the next instruction.

We handle only the most common form of : .

The visitors are full of technical details I’ll pass over. We could instead have turned statements into calls and assignments, to pass on to the rest of the code generator; but that would take about as much compiler code, to produce worse compiled code.

Compiling and , for our subset, is much like . A loop coerces the value of the expression to an iterator object, via , leaving the iterator on the stack throughout the loop. When the iterator is exhausted, pops it off and jumps to ; there we must account for the pop in the stack-depth analysis ( ).

Now we have a runnable program again, that can compile nontrivial computations! We could flesh it out further with more node types— and , for example; however, those don’t add materially to the complexity of the kinds of programs we can compile. Consider them an exercise for the reader.

The next biggest gain of expressiveness will be in compiling functions and classes. As we’ll see shortly, this requires even bigger changes to our design.

Once again, we ask ourselves if our current design can handle the complexity we’ve added in this version. Tailbiter v1 jumped immediately into generating code, but now the code for a variable access will depend on how the variable appears in the rest of the source program. It might be a local or a global variable, for instance. As another issue, function definitions and lambda expressions have a lot in common; it’d be easy to end up with duplicated logic in generating code for them. The same goes for list comprehensions ( above) because they act like a nested function (to bound the scope of the loop variables). Therefore, before starting to generate code we’ll do some setup:

Do we need the complication of nonlocal variables? Maybe not: the final compiler uses just two lambda expressions. However, there are nine list comprehensions, and Python’s just no fun without them. To compile them correctly we can’t avoid nested scopes.

Python comes with a scope analyzer built in, the module—but we can’t use it! It requires a source-code string, not an AST, and ASTs lack a method to give us back source code. In Tailbiter’s early development I used anyway, as scaffolding; this forced it to take textual source code instead of an AST for its input.

Besides , Python’s module defines another visitor class, for not just walking through but transforming trees. It expects each visit method to return an AST node, which will be taken to replace the node that was the argument. The default behavior recurses on child nodes, leaving the node otherwise unchanged. Desugaring uses such a transformer:

For a start, we rewrite statements like into . (Rather, into an if-then-else with the in the clause: this is slightly simpler.)

visits and replaces the children of , as mentioned above. interpolates a source-line number for any new nodes introduced.

What was the point of this visitor? Without it, we’d need to define a in the code generator instead—OK, fine. This would then need to generate code to perform a test, a call, and a raise. To do this without duplicating logic within the compiler, we’d define code-generation functions for each of those, to be invoked by both the new and by the code generator’s , , and so on. That’s not onerous, but if we’re desugaring at all, this is a good use for it: it’s easier and clearer.

Lambda expressions and function definitions get rewritten in terms of a new AST node type we’ll define, called :

This is as if we turned into , except that a lambda expression can’t carry a name. Reducing to just one type of function node will save work not just in code generation but in scope analysis too, and give us a simpler expansion for list comprehensions. As a bonus, adding a new node type to the language feels like getting away with something.

List comprehensions also create a node, holding the loop, because the loop variables must be defined in their own scope, not polluting the current scope as they did in Python 2.

For instance, [n for n in numbers if n] becomes a call to a locally-defined function almost like below:

gets its starting value, a new , from the call to the local function. The actual tree we generate can’t quite be written as Python source: the function called above is actually anonymous (the name will appear only as the name of the code object, not as a Python variable), and gets named , a name which can’t occur in real Python source code and thus can’t clash with variables from the source.

CPython generates more-efficient bytecode directly from the comprehension, in a hairier way.

We round out with a few helpers:

Functions and classes introduce a new complication: local and nonlocal variables. Consider:

is a local variable used only locally within . The interpreter has special support for variables like this: for efficiency their values live in an array instead of a general dictionary structure. (Each call to the function gets a fresh new array.) Once our determines this, our revised code generator can emit or to access the variable at a known position in the array:

Recall that maps from local variable names to their positions in the list of such names; the interpreter will know to build an environment of the right size because we included this information in the compiled function’s code object, in .

What about the other variables in this example? is local like . (We’ll come back soon to consider the function it names.) is global, getting looked up by name at runtime: it has access type , above. CPython’s compiler can use a fourth access type, , as an optimization: the instruction looks up the variable name directly in the module’s global environment, but , which we use instead, first checks the local environment. We do it this way because Python will need the access type for the variables in class definitions, as we’ll see, and it’s simplest to use the same type for both.

Finally, there are variables like , which is not global, yet “less local” than in that it’s also used from another scope, the body of . Pretend for a moment as if Python didn’t support nonlocal variables. We’d have to transform our example to use only locals and globals, like

is now tightly local in each of its scopes, and the inside is explicitly passed into from the enclosing scope. no longer refers directly to the value , it refers to a data structure, the , whose contents is the value. This indirection makes it possible to pass into before we’ve assigned its contents. For this to work we had to transform each use of into . We also had to create the cell first, before the transformed code.

So, one way to compile our example would be to rewrite it in just this way using a like the desugarer, then continue, knowing that all the scopes now are “flat”. But we won’t, because the virtual machine was designed to be compiled to directly, with built-in support for a data type similar to our notional class. Each code object will get a tuple of its cell variable names (such as in ); before the interpreter starts running a function, it will automatically create the cells; there are and instructions corresponding to our uses of ; and finally, as we’ll see, the instruction to create a function can take a tuple of cells as an argument, corresponding to our . The upshot is that we can compile the body of a function just as we’ve been compiling module-scope code, except we’ll need to distinguish the variables like from the and variables.

There’s one more issue: classes also introduce scopes. Unlike a function scope, a class scope doesn’t get fast or deref variables—only its function-type subscopes do, such as its method definitions. The class’s variables and methods are variables instead. The interpreter will, after running the compiled body of the class definition, just take the resulting dictionary as the data to build the class object out of.

Tailbiter forbids nested classes, to avoid some of Python’s dark corners, and likewise forbids statements and explicit and declarations.

Now we know what we need out of our scope objects. We’ll build them in two passes:

The first creates subscopes for classes and functions and records the local uses and definitions of variables. We must walk the AST to collect this information, another job for an AST visitor. Unlike the last time, we’ll leave the AST alone: our visit methods should instead fill out attributes of the . That’s the protocol for a :

There’s a visit action for each type of AST node that either introduces a subscope ( and ) or references a variable (the rest). The variable references get collected into two sets, of definitions and uses.

With this information, the pass can then deduce the nonlocal uses of variables. It walks down accumulating the definitions from enclosing function scopes, then back up accumulating the references from enclosed ones:

A “cell variable” is a deref defined in the current scope; a “free variable” comes from an enclosing scope instead. In our example, is a free variable in the lambda expression but a cell variable in the rest of . The code generator will need to know about free variables in order to compile the creation of function objects.

The code generator uses and , plus this method:

Does scope analysis need to precede code generation? If we turned it around, first the code generator would generate a version of symbolic assembly with named variables, then we’d analyze the uses of variables in the assembly code, and finally emit bytecode, treating some of the s as , etc. This could be more useful as part of a bytecode rewriting system—for disassembling, transforming, and reassembling, it’d be nice to be able to transform without worrying about the mechanics of cell and free variables—and it might take roughly the same amount of logic. In varying from CPython’s approach, though, it’s riskier.

We handle both and the no-argument . (The latter is never used by this compiler.)

A node is an expression desugared from a lambda expression, function definition, or list comprehension. Compiling one takes two steps: first, compile its whole body into a separate code object. Second, generate assembly code that will build a function object out of this code object.

The new for this new code object requires a subscope of the current scope—previously computed by the scope analyzer, now recovered by .

Building a function out of a code object depends on whether it has free variables. compiles to

where the first constant is the code object for , whose code in turn should be

The code object loaded by the instruction at 6 (and not shown here) finally does the actual adding of and .

The outer lambda, with no free variables, becomes a ; the inner one becomes a passed a tuple of deref cells. ( will load ’s cell, instead of the contents of the cell as would do. The scope analyzer already arranged for the needed cells to be among the current scope’s .) The second in either case loads the name of the new function. (In “real” Python the nested lambda’s name would be , reflecting the nesting.)

Recall that wanted a code object for the function in question. On a new instance, it called :

When CPython wants a function’s docstring, it looks in the code object’s constants table at the first entry. We put the docstring there, in the first entry, by calling with it first, before we generate any code. (The actual instruction returned is discarded.) This depends on our table preserving the order that we add to it.

As with the docstring, the parameter names become the first elements of the table. (Remember they’re s: fetching adds to them as needed.)

We generate assembly that will run the function’s body and return (in case the body had no of its own); then we assemble it all into a code object.

Like functions, class definitions sprout a new to compile down to a code object; but the assembly for building the class object is a little bit fancier.

The class definition’s code object resembles an ordinary function’s with some magic local-variable definitions prepended. (The docstring doesn’t start the constants table, for these.) Python’s class builder ( ) will populate the new class’s attributes from the locals dictionary as of the point this function returns. This again is why scope analysis must not classify the locals of a class definition as : they need to live in a dictionary, not an array.

OK, so! Wind it all up and watch the tail-eating:

Or try it on a buggy program—i.e., most programs most of the time—and get a proper traceback.

We’ve taken considerable trouble to convert from one fairly-arbitrary representation to another. After so many mundane details, you might reasonably ask why you ever thought compilers might be cool. We’ve cost ourselves not just this work and complexity of translating, but also of translating back: debuggers and profilers must map what happens in bytecode to terms meaningful in the source. Why not interpret programs directly in their first form, the AST?

First, for the compact linear form of bytecode. An AST is fatter and distributed in memory, interlinked by pointers; the size and the pointer-chasing both would slow an interpreter down. One core job, then, was mere rearrangement: taking a data structure (the AST) meant for arbitrary viewing and changing, and laying it out just right for the interpreter, who will find each element ready to hand at the moment it’s needed—like, for us, reading a recipe and starting by laying the ingredients and pans onto the counter in a sensible order.

Second, to precompute. We analyzed the scopes and how they used variables, to find, ahead of time, the place in the runtime environment where a variable will live—letting the interpreter skip looking up the name.

A third kind of win is possible in rewriting the program as we compile it—“optimization.” Perhaps the compiler could notice that would go faster as . This is precomputation in a broader, open-ended sense (sometimes called the Full Employment Theorem for Compiler Writers). But isn’t this orthogonal to translating source to binary? Aren’t there independent source- and machine-code optimizers? Yes, but a compiler might improve the target code based on analysis of the source. An optimizer ignorant of the original source may have much more trouble understanding the machine code it’s trying to optimize; on the other hand, a source-to-source optimizer deals in the wrong language to even express some machine-level optimizations. A compiler can potentially unite a high-level view with grubby hands.

Well, that sounds compelling. Maybe. But CPython hardly optimizes at all. (PyPy is another story.) Could we get the first two advantages more easily? What if we ran the scope analysis and performed a generic kind of rearrangement: that is, re-represented the AST in a tight linear form, with the debug info pushed off to the side? The code generator could look vaguely like

With this “compact AST” you’d point to an AST node’s representation via a numeric offset into an array such as this method returns: so the passed in here becomes a subarray starting at index , then follows starting from , and so on. This form could be nearly as tight as bytecode (once we use bytes and not the general integers which were quicker to explain), while still viewable as just another form of AST, making the compiler and surrounding tools all simpler. In numbers, how much better is the bytecode virtual machine?

Exploring that question exceeds my scope here—but maybe not yours.

Where next? Since writing this, I’ve let it grow a little to be able to compile an interpreter (adapted from Byterun) of the CPython bytecodes we emit; you can find both together in the Tailbiter repo. There’s a start on a Python parser; finish that and add an implementation of the core Python runtime features like dictionaries and classes, and you’d be approaching a genuinely self-sustaining system like Smalltalk. (I think the biggest remaining lacuna would be exception handling.)

An optimizer’s yet unwritten. I can imagine one serving to prototype a successor for CPython’s peephole optimizer, someday. And how fast can we compile? Faster than I did, that can’t be hard.

Peter Norvig’s Paradigms of Artificial Intelligence Programming, despite the title, presents short compilers for Scheme, Prolog, and a pattern-matching rewrite-rule language. They all go to some trouble to produce efficient output. The code is of the highest quality.

Niklaus Wirth’s Compiler Construction details a simple compiler from Oberon-0 to RISC machine code. Wirth and Gutknecht’s Project Oberon: The Design of an Operating System, a Compiler, and a Computer elaborates it to the full Oberon language it’s written in, ending up at about 3000 lines.

Andrew Appel’s Modern Compiler Implementation in ML explains the ideas behind fancier optimizing compilers.

CPython 2 has a compiler module in 4500 lines of Python, seemingly included just for fun. For the compiler that’s normally run, see the guide, compile.c, and symtable.c; there’s also the optimizer peephole.c.

Our compiler compiled itself, but we stopped short of applying this ability towards anything remarkable. (Can you change the language yet?) Ken Thompson showed one surprising direction to take, in “Reflections on Trusting Trust”.

Thanks to Michael DiBernardo for editing earlier drafts of this chapter for 500 Lines or Less, and especially for his grace when I abandoned it; Andrew Gwozdziewycz and Chris Seaton for reviewing a draft of the code; Dave Long for conversations about assembly and code generation; Shae Erisson and Kevin Lee for beta reading; and David Albert and Rachel Vincent for shepherding this through to the end.

The tool used here for multi-version literate programming is a hacked version of handaxeweb by Kragen Sitaker.|||

