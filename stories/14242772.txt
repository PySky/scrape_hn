How to use transfer learning and fine-tuning in Keras and Tensorflow to build an image recognition system and classify (almost) any object

In the last post, I covered how to use Keras to recognize any of the 1000 object categories in the ImageNet visual recognition challenge. More often than not, however, the categories we are interested in predicting are not in that list.

In this post, I will show you how to use transfer learning and fine-tuning to identify any customizable object categories! To recapitulate, here is the blog post series we’ll be following:

It’s well known that convolutional networks require significant amounts of data and resources to train. For example, the ImageNet ILSVRC model was trained on 1.2 million images over the period of 2–3 weeks across multiple GPUs.

And it does astoundingly well! Razavian et al (2014) showed that by simply using the features extracted using the weights from an ImageNet ILSVRC trained model, they achieved state-of-the-art or near state-of-the-art performance on a large variety of computer vision tasks.

There are two approaches we can take:

There are two main factors that will affect your choice of approach:

*you should also experiment with training from scratch as well.

The figure above and the bullets below describe some general advice for when to choose which approach.

A powerful and common tool for increasing the dataset size and model generalizability is data augmentation. In fact, every competition winning ConvNet employs the use of data augmentation. Essentially, data augmentation is the process of artificially increasing the size of your dataset via transformations.

Most deep learning libraries have ready-made functions for typical transformations. For our image recognition system, the task you have is to decide which transformations make sense for your data (for example, X-ray images should probably not be rotated by more than 45 degrees because that would mean there was an error in the image acquisition step).

Go straight to the code on GitHub here!

We’ll use Kaggle’s Dogs vs Cats dataset as our example, and setup our data with a training directory and a validation directory in this manner:

Recall from our previous blog post on image recognition the importance of the preprocessing step. This is set by where is from the module.

The , , , , and parameters signal ranges for their respective data augmentation transformations.

Next, we’ll instantiate the network from the module.

We use the flag to leave out the weights of the last fully connected layer since that is specific to the ImageNet competition, from which the weights were previously trained. We’ll add and initialize a new last layer:

converts the tensor output into a tensor where is the # of channels.

Then we add on a fully-connected layer of size , and a softmax function on the output to squeeze the values between .

In this program, I’ll demonstrate both transfer learning and fine-tuning. You can use either or both if you like.

Doing both, in that order, will ensure a more stable and consistent training. This is because the large gradient updates triggered by randomly initialized weights could wreck the learned weights in the convolutional base if not frozen. Once the last layer has stabilized (transfer learning), then we move onto retraining more layers (fine-tuning).

When fine-tuning, it’s important to lower your learning rate relative to the rate that was used when training from scratch ( ), otherwise, the optimization could destabilize and the loss diverge.

Now we’re all set for training. Use for both transfer learning and fine-tuning.

We’ll use an Amazon EC2 g2.2xlarge instance for training. If you’re unfamiliar with AWS, check out this tutorial (instead of using their prescribed AMI, search the community AMIs for “deep learning” — for this post, I used in US-West-Oregon).

We can plot the training accuracies and loss using the history object

Now that we have a saved we can modify the same function we wrote in the last blog post to predict the class of a local image file or any file via a web URL. Checkout the github for the full program.

As an example, I trained a model on the dogs-vs-cats dataset using 24000 images for training and 1000 images for validation for 2 epochs. Even after only 2 epochs, the performance is pretty high:

Stay tuned for the next post in the series:|||

In the last post, I covered how to use Keras to recognize any of the 1000 object categories in the ImageNet visual recognition challenge. More often than not, however, the categories we are…