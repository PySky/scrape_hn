Metrics data is essentially timeseries data. Over the years, there have been a few applications built to record and view such data. Some of the more common systems used to record metrics are Graphite, InfluxDB, Prometheus, etc. Each of these systems comes with their own set of unique challenges as you try to scale them up to handle increasing volumes of metrics data. We record metrics into OpenTSDB and use Grafana as the viewer.

The diagram below shows our current metrics collection and viewing pipeline.

Open Time Series DB was originally built to store time series data on top of Apache HBase. It has since been extended to store data into Cassandra and Google Bigtable as well. It exposes a few different end points (telnet style socket, a HTTP API, bulk imports, etc) to ingest data. We use the telnet interface, which uses a simple one line format to collect metrics. A sample metric line for recording cpu utilization on a box might look like this.

The above line translates to:

The OpenTSDB project also releases a small collection of metric collection scripts called TCollectors, which measure a bunch of network and system metrics. We run these collectors on all our production servers. The stock tcollector master process can be configured to push metrics into an OpenTSDB telnet listener directly; it also logs them to stdout by default. In addition to running these collector scripts, we also have custom collectors that query and report on metric data from applications. As long as the metric you want to report follows the format above, writing a collector is fairly simple.

In addition to the above collectors, we also generate metrics from in-line application stats. All of these metrics are eventually recorded in OpenTSDB as well. Such application metrics are sent to a local statsd running on every production box. These metrics are different from tcollectors, in that they are usually generated by applications live. Some of these metrics can be computed after the fact, but reporting them in-line with the application gives us easy instrumentation and (near) live reporting ability to applications.

For instance, a typical statsd metric might be something like a counter representing the number of backend timeouts on the application server. To start reporting this value as a metric, you typically use one of the existing statsd client libraries in your application. There are multiple statsd libraries for practically every programming language. The application would update this statsd counter on every timeout or at some sample rate you decide. The statsd library then periodically reports this counter value to a statsd process outside the application (typically an external server running elsewhere). The articles linked earlier are excellent resources for the different types of statsd metric types, the typical use cases, etc.

We use a statsd server implementation called Statsite: it is very efficient and runs on all of our production servers. Statsite also ships with a sink to send statsd metrics into OpenTSDB, however that sink needs Nodejs to run, and we typically don’t run Nodejs on all our production boxes. The stock tcollector daemon (described earlier) happens to also ship with a local UDP listener that can be used to dump metrics into. We adapted the example binary sink that ships with Statsite to push these metrics from Statsite into the local tcollector’s UDP listener — after this point, these statsd application metrics end up as regular tcollector metrics.

We use Kafka as a central queue to collect all of our metrics and then push them into OpenTSDB. To get metrics from the production servers into Kafka we pipe them directly from the tcollector process stdout into a small Python script (kafka_dumper) that then pushes them to our Kafka cluster. Using this intermediate Kafka layer gives us a few nice benefits.

To ingest these metrics from Kafka into OpenTSDB, we use the stock console consumer that ships with Kafka. It fetches metrics from Kafka in-order, and prints them to stdout. We then do some sanity checking and use netcat to push them into the OpenTSDB telnet listener.

As we were experimenting with OpenTSDB, there have been times where we needed to trim old data in HBase, so we wrote a small helper script that deletes data older than some timestamp (argument passed as an unix epoch time). Note that this trimmer script assumes you are not salting your metrics in HBase and that you have a HBase Thrift server running locally. This script also does not clean up OpenTSDB UIDs and other metadata. It simply deletes your metrics data-points (HBase rows) older than a timestamp.

OpenTSDB ships with a basic viewer for metrics data as well, but it is a bit unwieldy and doesn’t give you features like dashboards, etc. Instead, we use Grafana to front the OpenTSDB cluster and are quite happy with the way it works. Grafana also gives us the ability to view AWS cloudwatch metrics in the same place, so it’s a great fit for us. Grafana makes it really easy to create per application dashboards, group related metrics together, and create templatized views. This combination has been working well for collecting and visualizing both system and AWS metrics. We use it extensively and highly recommend it.|||

The aim of this series of posts is to describe our metrics stack, the underlying components, and how we use/consume these metrics. We are also releasing some of the helper scripts used to manage this.