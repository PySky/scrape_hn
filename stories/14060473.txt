Today we’re going to explore how more advanced cache policies work: SLRU and ARC, aka segmented least recently used cache and adaptive replacement cache.

In caching, all problems can be solved by another least recently used list – the same as in programming, all problems can be solved by another level of indirection.

We will look at the inner workings of SLRU cache, and then explore its more efficient brother, ARC cache. Both of them work better than LRU cache, but both use LRU lists internally. Let’s dive in!

You’ll have to use your imagination.

Interestingly, by adding another LRU list to the existing LRU cache, we can get a cache with a better hit/miss ratio. Such basic thing is called SLRU cache because this cache uses two segments or two LRU lists.

The cache is built on top of 2 LRU lists with fixed capacity: unreferenced list and referenced list.

On a cache miss, a new item is added to the unreferenced list. The cache can use an “age” counter to track the recency of items and increase that age on every cache modification. Internally, the list is implemented as a minimum priority queue where item’s age acts as a priority. The older item is (or the smaller its age), the more probable it is to be evicted.

When the unreferenced list gets full, it starts evicting the least recently used items to accommodate new items. If that’s not enough and more space is needed, the referenced list also starts evicting its items. Eventually, when the actual cache cost is under its maximum value, the new item is added to the unreferenced list.

On a cache hit, an item is moved to the most recent position of referenced LRU list. Basically, it means that item gets the biggest age number and then it is added to the referenced list.

On one hand, when the hit item is in the unreferenced list, it moves into referenced list. With such, referenced list is getting filled. On the other hand, when hit item is found in referenced list, it just changes its recency (age).

At some point referenced list becomes full and it can’t fit more items. In that case, before adding a hit item, referenced list evicts as many items as needed into the unreferenced list. That operation leads to an exchange of item from the unreferenced list with (possibly many) items from referenced list.

That’s the basics of working of SLRU. Now, what happens if we make the partition of the cache into referenced and unreferenced lists dynamic? In other words, we are going to change maximum capacities of the lists based on the cache load. And that’s where ARC comes on the scene.

ARC, the more advanced version of SLRU, adds 2 more “ghost” LRU lists to the cache to decide when to grow or shrink the actual lists.

In ARC terms, “unreferenced” is called “recent” list, and “referenced” is called “frequent” list. At the creation time, those lists have half the capacity of the total cache size. Later, the maximum capacity of the lists can change.

Each of the lists has “ghost” counterpart initialized with half of the cache size capacity. Ghost lists do not change their capacity during cache working. These lists will be holding keys for items evicted from real lists. In that case, the cache will still remember item key, but won’t have the item itself, hence the “ghost” name.

The mechanism of working of ARC is exactly the same as in SLRU cache, with the only difference: when an item is removed from a real list, its key is still saved in the ghost list. Later, when an item with the same key as in ghost list gets added, the cache increases the capacity of a real list corresponding to the ghost list.

That change has an interesting balancing effect: cache is self-tuning to its load. When the cache is used inside a loop, it tunes itself increases the size of recent items. On the contrary, when the cache is used with many frequent item requests, it will grow the size of the frequent list, adapting to the situation.

When a ghost list becomes full, it removes least recently used keys first – just like a normal LRU list. Here, nothing interesting. Items from the ghost list are just removed and forgotten since they were removed from the cache before.

Both SLRU and ARC caches use 2 real LRU lists to track recent items and frequent items. Additionally, ARC cache has self-tuning mechanism based on tracking keys removed from real lists inside additional “ghost” lists. As a result, these changes bring improvement in terms of cache/miss ratio over basic LRU cache.

Would you choose to see SLRU and ARC in the source code, please visit the GitHub repo.|||

Today we're going to explore how more advanced cache policies work: SLRU and ARC, aka segmented least recently used cache and adaptive replacement cache. In caching, all problems can be solved by another least recently used list - the same as in programming, all problems can be solved by another level of indirection. We will…