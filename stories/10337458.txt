With almost 12 billion searches being made on Google (and that’s just a portion of the market) for content, products, and services on Google each year, it’s clear how search engine optimization could reap rewards for most any business.

Where in the pecking order a site or page is displayed in search results is dependent on a number of factors, which can generally be grouped into three categories: Technical, Off-Page, and On-Page.

On-page SEO is largely concerned with content, such as blog posts, and how well it is optimized. Technical SEO is largely concerned with how well a sites code can be crawled and index by search engines. Off-page SEO largely relates to backlinks from other websites to your site.

I feel I’m pretty strong on creating high quality content (and the audit below confirmed that), however I wanted to make sure I was doing on-page optimization as best as possible and make sure I wasn’t doing anything wrong on the technical side that might cause Google to downgrade my site.

Before getting this audit, I didn’t know exactly what I should be looking for, nor how to do it, so I wanted to get someone who really knows what they’re doing to give me an unbiased opinion.

So I hired Jason White to do an SEO audit of my site.

Jason White is VP of SEO and SMM at DragonSearch, a digital marketing agency. A keyboard jockey by day, Jason dreams about epic rides through high mountains which usually end at a swimming hole with burgers and a good beer. Jason has spoken at SMX, NYU, WordCamp and has work spread across print and digital.  You can find Jason via Twitter or on the DragonSearch blog.

Here are some of the most actionable and high leverage items from the outstanding report he delivered to me. I think you’ll find some ways to improve your sites technical and on-page SEO, as well as some general content marketing and website usability best practices. I’ll be implementing most of these suggestions and I think it will help me gain more traffic from google as well as improve the experience for readers.

When auditing a website, it’s always important to first start with the goal. Does the website sell products or services? Is it a content hub? Are we trying to earn whitepaper downloads or newsletter signups? Shaping an audit with the purpose of the website in mind seems like a simple thing but it’s often missed.

The approach to any Search Engine Optimization campaign should incorporate many on-site and off-site activities that help to form a well-optimized digital property. Optimal positioning is achieved by the methodical application of SEO best practices, combined with website usability and functionality, as well as social signals to the highest degree possible. Customized audits will forever trump cookie-cutter or automated auditing process because the auditor can review the website with an objective mind.

Organic SEO is a long-term strategy that takes time before benefits are seen. A holistic approach expands beyond traditional SEO and will not only ensure that the website is findable and accessible for indexation by search engine spiders but will also help improve your overall digital footprint, the website’s authority, its relevancy, and the overall user experience.

When we review websites, we pay close attention to first impression and usability. SEO is not the only consideration for an effective marketing strategy or website. It’s extremely important that the users are able to navigate the site easily and find the information they need. We don’t market to search engines, we market to real users as they have the cash to buy our products. While there are hundreds of ranking factors, at the core, Google is interested in serving the best experience for their query. When we keep usability top of mind and offer a rich experience, the search engines will follow.

The website features a fantastic quantity of rich content. The posts themselves are wonderfully written with an average of around 1,000 words per piece. Navigating to this content, viewing categories and finding the numerous ebooks is challenging because there is no main or sub navigation offered.

Users can find a handful of featured ebooks in the website’s footer which ultimately take the user “off site” to a ConverterKit page:

Users can find category and tag pages via links at the end of each blog post:

While the lack of navigation is aesthetically pleasing, this is far from ideal. Users want to quickly understand what topics a blog or website cover so that they can quickly drill down to relevant topics. A lack of navigation limits the lifespan of content and doesn’t offer the ability to show off the impressive series of ebooks which have been created.

A lack of navigation negatively impacts how search engines crawl and understand the website’s structure. When pages or blog posts are multiple clicks away from the homepage, it’s interpreted as content that is less relevant or not as important as compared to others. Navigation allows for structure and lowers the amount of clicks needed to find important pages/content.

Ideally, we’d like to see pages approximately 3-5 clicks from the homepage. Currently, this is how click depth is mapped out:

A canonical tag allows you to set a preferred URL for your content. Specifically, a “link rel=canonical” element of code is placed in the top of a web page to identify a specific URL that is the original source of the content on the page. If another site copies your content, this bit of code tells search engines which page is the original. It is also very valuable to address various existing and potential duplicate content issues on the site as it takes some guesswork away from the search engine..

As categories and tags get populated with new content, content management systems will add page numbers to the URL string. This can happen in other instances as well and is known as pagination and can be interpreted by search engines as duplicated content. We currently see this in your tag and category structure:

To give clear signals to the search engine that the resource or page is part of a series of pages, rel=prev and rel=next should be used. The tag pages contain the correct rel=prev, rel=next tag as seen below but the category pages are missing this tag.

I was not able to find any examples of duplicate content which was indexed but I did find duplicate content via a plugin called Revive Old Post which is a tool used to syndicate older content on social media. The issue is that it is promoting content to a subdomain which is splitting equity away from the main website while creating a duplicate content risk.

Further, these subdomains have canonical tags pointing back to themselves which could create further confusion for search engines should one of these pages get indexed.

It is important that website content is accessible and visible to the search engines. When a search engine spider crawls a website it is not able to see it the way humans do. If these spiders are repeatedly finding missing pages, redirects or other bad HTML site code, it’s likely you’ll be viewed less favorably with search engines.

Mike: The Quick Page/Post Redirect Plugin along with the Pretty Link Plugin can be used to implement this.

XML Sitemaps are feeds designed for search engines; they’re not for people. They are lists of URLs with some optional meta data about them that is meant to be spidered by a search engine. XML Sitemaps are especially useful for very large sites but every website made can benefit from a clean, accurate sitemap. They help improve spiderability and ensure that all the important pages on the site are crawled and indexed. Sitemaps give the search engines a complete list of the pages you want indexed, along with supplemental information about those pages, including how frequently the page is updated. This does not guarantee that all pages will be crawled or indexed but it helps feed the search engine and take away guess work.

An HTML site map is a simple html page that very clearly outlines the site’s hierarchy and how content is organized within it. It is a list of text links to each page. The HTML sitemap is useful for both human visitors and can help search engine bots verify the validity of your XML sitemap.

The HTML Code is the building block of a website. The search engines ‘crawl’ through this code to determine what the site is about, and to evaluate the relevance and value of the site in relation to others on the internet.

Building a site with well-developed information architecture and clean “spider-friendly” code will ensure proper crawling and indexation of the entire site. Search engine crawlers have a character or time limit they can spend on the site and at the same time, the spiders are getting smarter everyday and can understand complex experiences and actions. Keeping clean code and allowing the spiders to crawl the code will limit issues with search engines while making good use of the crawl budget.

In addition to crawlability, having clean and proper coding is very important so that the site loads quickly.

Site speed has been an increasingly important factor for successful websites, not just for visitors who lose their trust when sites are slow or don’t load, but also for search engines that have incorporated it as a ranking factor.

As an example of the importance of site speed, Amazon’s case study showed that an extra 1 second in load time cost them a 10% loss in conversions.

Site speed is cumulative, so ensuring that all the factors that affect load time are looked at, tested and improved on is important. It’s important to weigh the perceived benefits of improving website speed with the associated costs. Often, a cost to benefit analysis will show that the costs might outweigh the benefit

When considering speed factors that directly affect the ranking algorithm, the time it takes to load the first byte of data is key. Sources on Moz.com reports that “a clear correlation was identified between decreasing search rank and increased time to first byte”. The study shows that a jump between .38 seconds and .49 seconds represents an average drop of position 1 to position 7. Overall, an ideal target of less than .4 seconds for the first byte of data is the recommended benchmark.

A speed check on WebPageTest.org of www.mfishbein.com showed a 0.474 second delay before the first byte was loaded.

Due to the increased use of mobile devices for internet browsing, mobile compatibility must be considered. Any website that you view on your desktop should also be easily accessible on a mobile device.

In November 2014, Google announced the introduction of Mobile Friendly Labels in Mobile Search Results. The update, features mobile friendly websites in the search results.

Expanding upon this emphasis on mobile results, Google has revealed that they have updated their algorithm on April 21st , 2015- expanding mobile-friendliness as a ranking signal. The aim for this update is to make it easier for users to find relevant, high quality search results that are optimized for their device. Since the rollout of the update, it appears that largely, this update was a lot of hype. This isn’t to say that the update wasn’t important.

Over the last 30 days, 31% of your website’s traffic is from a mobile devices or tablet. This is a significant percentage that is likely to increase with time.

Schema markup is a collaboration between the major search engines that, through a specific markup, helps them to better understand the information on a webpage. This makes it possible to provide richer results on the search engine results pages often seen in the form of additional information, links to social accounts or a star review appearing under the title tag. Schema markup enables webmasters to embed structured data on their web pages for use by search engines and other applications.

The Schema vocabulary describes a variety of item types so that when a user searches for a phrase that relates to one of these item types (i.e. product, review, person, event, etc.) additional information can be displayed in the organic results on the search engine results page.

There has been a large debate regarding the usefulness of Schema Markups and their user facing results named Rich Snippets. Regardless of how Google visually interprets schema markups, schema remain a vital way to ensure that search engines can successfully interpret and understand the information and connectivity of relationships found on your website.

Having descriptive URLs with keywords can help with building relevancy for the pages and help with user click-through rates, as the keywords will suggest the content of the page and thus its relevancy.

Parameters, delimiters and special characters can make it difficult for the search engines to understand them, which may cause fewer pages to be indexed. In addition, they are very user unfriendly.

The real secret with URLs is consistency in the structure across all URLs. If the URL includes hyphens, ends in a trailing slash or .html, they should be consistent across the site.

URLs with and without a trailing slash or mixed capitalization can be seen by the search engines as two different URLs pointing to the same content, thus causing duplicate content concerns.

Content is the life blood of search engine optimization, requiring not only a keyword focus but also, a relevancy of the topics a website covers

Below is a screenshot of how your website is displayed in search results. It is important for the titles and descriptions of your pages to accurately reflect the content of each page.

Title tags are your front line of offense as they will slow the user’s scroll of the results and help convince them your answer is the best match for their query

Titles should be keyword-focused, unique for each page of the site, include branding, and be descriptive about the content of the page to entice searchers to click through.

Aim to use between 55 and 64 characters as this gives you the opportunity to stand out and encourage searchers to visit your site over the other results in the SERP.

Mike: This keyword to use can be determined via Google Keyword Planner and implemented via the Yoast Plugin.

Meta Descriptions do not increase search result ranking, but are a main factor in attracting users to click through to your website in the search results. They appear under the title in the search engine results pages (SERPs), so it is your second most important marketing tool following the title tag.

A well written and inviting meta description will entice users to visit your website. Meta descriptions should be around 155 characters long and crafted using keyword research but, at the same time, appealing to the searchers so they want to click through.

Body copy helps search engines determine the page’s relevance so it is important to have copy about the target topic on the page. While it is important to use keywords strategically throughout the copy, it’s more important to write good quality content that speaks to your target audience by using rich natural language with synonyms and related phrases. Stuffing your website with keywords, however, can be perceived as spam by the search engines which will ultimately affect your rankings while resulting in a bad user experience.

Header tags help structure content and lay out the hierarchy as well as break up content into more digestible blocks for users while providing search spiders hints as to what the page is about. Using Heading tags on the Home Page for your main content is a best practice not only for SEO reasons but also editorially to clearly communicate and structure the content hierarchically. In addition, as the Heading element’s inherent meaning is to define what the topic and sub topics of the content are, it is an important element for ‘semantic closeness’ to the rest of the copy (how semantically relevant your content is to the heading), which search engines use to analyze and understand content.

Optimizing your file names, including images, is an important part of on-site optimization. Well-optimized images not only support the optimization efforts on the pages but they can also rank on their own in image search and drive traffic to the site.

The “alt” attribute specifies an alternate text for user agents and should be used to comply with the Disabilities Act Section 508. Search engines are the ‘most disabled’ users who will ever come your website and cannot see images so using keywords in the file names, and especially in the “alt”, will help them know how to index the image.

The “title” attribute offers advisory information about the element for which it is set. It is not indexed by the search engines however the “alt” attribute can be supplemented with a “title” if it provides value to your users.

Demographics and Interest reports allow you to better understand who your readers are by collecting information such as age, gender, and interests.

Google Search Console allows Google to send troubleshooting sdata, site metrics and communications to webmasters who install it while providing other useful data and site statistics. When connected to Google Analytics, the Search Console provides richer data.

Site Search is currently found on your 404 page. As a best practice, it should be enabled site-wide to allow searchers to easily find specific topics or posts. When enabled, the queries that users search for can be tracked. This can help identify gaps in content or navigational issues.

Mike again: In conclusion, there are clearly many ways I can improve on my on-page SEO (and you could improve yours). But, as noted, some of these small changes won’t make that much of a difference in the long run and will take a lot of time to implement.

Even if I make all of these changes, my ranking may improve, but without amazing content that provides value to people, no one will want to come to my site or stick around. So, I will definitely be implementing many of these recommendations, but remember there’s not such thing as a free lunch and creating valuable content must be a part of any content marketing or SEO strategy.|||

Here are 9 on-page SEO wins I learned after I had an SEO audit of my site done.