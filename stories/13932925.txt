I have a confession‚Ä¶.my crontab is a mess and it‚Äôs keeping me up at night‚Ä¶.don‚Äôt worry, it‚Äôs not really keeping me up‚Ä¶.but you might know what i mean üôÇ

We have been using Google BigQuery as our main data mart (lake or whatever its now called) for almost two years. We have been loving this as it‚Äôs super powerful with very little overhead in terms of management and infrastructure.

However one thing i think is missing from Google BigQuery is a tool for managing and orchestrating your ETL jobs. Something like MS Data Factory but for Google Cloud.

Over the last year or so we have built up a few shell scripts to wrap around the bq command line tool and basically take a series of templated sql files, chain them together into a job and then schedule it all via cron. This has actually been working better then i would have expected, mainly because we try to build a lot of redundancy into these jobs where failures won‚Äôt kill any live tables and will usually be resolved the next time the job runs in a few hours or so.

However it‚Äôs always bugged me a little that this is not really a great way to go about things and as the number of jobs and pipelines has grown it has got a little painful to manage.

Enter Apache Airflow as the solution (hopefully) to all our problems.

Recently we have been playing around with Apache Airflow. This post is more about a concrete example of one way we have got it working for a specific use case that i did not really find any obvious existing examples of (there is actually great documentation and lots of examples but there is a layer of Airflow specific concepts and terminology one needs to nerd up on first).

So i won‚Äôt talk much about Airflow in general¬†except to give some references i found very good for beginners:

The specific use case i was trying to figure out was around creating a paramaterised pipeline that takes the templated sql files we already have and can run them for each of our lines of businesses or ‚Äúlob‚Äôs‚Äù (e.g. variety.com, hollywoodlife.com, wwd.com etc). This is a pretty common ETL design pattern so hopefully others might find this useful.

A concrete example at PMC would be some post processing we do of raw Google Analytics data we get¬†exported into BigQuery each day. The pipeline basically takes the raw [<view_id>.ga_sessions_yyyymmdd] tables¬†for each day, applies some transformations and enrichment‚Äôs and then creates a table in BigQuery for each lob for each day. So for example [wwd.ga_data_20170322] would be the end result which is a raw hit level table that we have enriched in various ways. This then is essentially one of our main analytical tables that powers lots of downstream analytics across all lob‚Äôs.

Below is an example of one way we got this working by having a single collection of SQL¬†template files (to represent the actual steps of processing we want to do in BigQuery) and a .py script to build and define the expanded DAG¬†with parallelism across all lobs while maintaining dependencies between each lob specific task.

Disclaimer: I‚Äôm pretty new to Airflow and one thing i‚Äôve found is that due to the programmatic nature of how we build DAGs there is lots of ways to do things (this is actually one of its strongest selling points). I‚Äôm sure there are better ways to go about what we are showing here (feel free to add suggestions in the comments üôÇ ).

So here is an example DAG definition python script which lives in it‚Äôs own sub folder in our Airflow DAGs folder. (Prettier formatting on Github here). I‚Äôve tried to go overboard on the commenting for line by line clarity.

So the above file makes reference to our sql template files which actually hold the business logic of what we really want to do in BigQuery, everything above is really just plumbing.

Below is what is in the ‚Äòmy_qry_1.sql‚Äô file ‚Äì its just dummy code for this example. (Again Github link¬†and apologies I don‚Äôt know how to properly format these blocks as code).

With the above code in a folder within our specified airflow DAGs folder we can see how Airflow picks up this DAG.

We can see the parallel nature of the same tasks but just broken out for each lob in the graph view.

If we look at the tree view we can see this in another way along with execution status over time.

And finally if we look at the gantt view we can see that we do indeed have the parallelism we were after with task 1 being run concurrently for each lob and then similar concurrency for task 2.

So if we trigger this DAG from the airflow cli with:

We can see the resulting data and tables in BigQuery.

And that‚Äôs pretty much it. It‚Äôs still early days but i‚Äôm hoping the fact we already have templated .sql files at the core of our existing approach should make porting over to Airflow easy enough (especially as most of jobs run in BigQuery so LocalExecutor should be enough).

We might do a follow up post in a month or two to share some deeper learning‚Äôs once we‚Äôve used it more. I‚Äôm particularly excited about using Airflow pretty much anywhere and everywhere i can. One neat example that jumps to mind would be in machine learning pipelines where we tend to use BigQuery for the data crunching and H2O for the model building and learning, Airflow seems like a great way to more cohesively stitch it all together.

As always, please feel free to add any comments or suggestions below.|||

