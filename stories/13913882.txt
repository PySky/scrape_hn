When it comes to clustering, the two giants, Kubernetes and Docker Swarm, are going head to head. It is all over the news, and here at Tuleap we have made our choice! But let’s back up a bit first!

Two years ago, before Docker 1.0 was released, we created our first “Tuleap cluster” from the Docker engine and duct tape. In short, it was a Docker daemon event stream watcher with an inotify-based nginx reload. Although it was functional, it was a nightmare to manage, extend, and scale. Last year, when the early versions of Docker Swarm were released, we created docker swarm prototypes to replace all of the duct tape.

Today, the balance has tipped in favor of Swarm for us, and we will tell you why!What does Docker Swarm actually do

We started working on our clusters in early May 2016, back when Swarm mode was not yet “a thing.” Back then our position was somewhere between standalone Swarm and OpenShift (Kubernetes + Redhat). Post-DockerCon is was fairly obvious that, even if we will have to support OpenShift and K8S (because Tuleap Enterprise customers and leads are asking us to do so), Swarm mode will become the de facto standard.

Swarm mode is buggy, but the UI is fantastic. It is hard to make something so complicated simple, but Docker has done it. Their developers were able to achieve a high level of abstraction and mask the complexity of distributing load across nodes. As stated above, we had already been running tests with standalone Docker Swarm (not the version directly and controversially integrated into the Engine). We were already familiar with Swarm and happy with our tests, so we saw an opportunity to leverage our knowledge and experience by staying with Docker Swarm.

To understand the problem a cluster of containers solves, it is important to go into a few of the technical details behind Docker Swarm. At Tuleap, we have two main needs that Docker Swarm addresses:

We used our demo site as a Tuleap cluster test case. The demo site is a full-featured Tuleap site. It is pre-loaded so that users can test it using real-world data. Because the demo site serves as a playground, the data is “cleaned up” every night to keep it operating property.

Our infrastructure consists of four physical servers hosted at Online.net. We decided to use three servers for our Swarm cluster, plus one server for data storage. We wanted to pool persistent data between the different nodes and containers of the cluster. All three nodes have access to data storage in NFS.

The demo.tuleap.org stack consists of four containers dispatched on three nodes:

To build our cluster we assigned three nodes. For reliability and cost-efficiency reasons we wanted the nodes in manager mode. We needed at least three manager nodes to ensure raft master election and to ensure uptime even if one server went down. However, having three dedicated managers is not the most efficient use of resources. For a first iteration it is a good enough trade-off. We will assign worker mode to any future additional nodes.

The nodes are composed of two network interfaces: one on the network (Internet) and one on a private network. This is the network interface used for our Swarm network in order to dissociate the two networks.

We started by initializing our Swarm cluster:By default Swarm provides a token to add one or more nodes in worker mode. The first node (the one created in the first step) is assigned manager status. We checked this with the following command:To have only manager nodes, we had to request another manager-specific token.Once we had this token, we could add the other two nodes, remembering to specify the private-network IP address.The nodes in our Swarm cluster then looked like this:

The Swarm cluster was set up. The next step was to deploy the Tuleap stacks across the nodes.

In this example, we deployed two Tuleap stacks and planned to run the commands directly on one of the nodes. In actual production, of course, everything is automated using Ansible. That will be covered in a separate post!

As a reminder, our stack is composed of four containers and one reverse-proxy that is shared. Data storage is built in NFS on /srv services.

As the containers were going to be spread across nodes, they had to be attached to a shared network that would work on multiple servers. Docker handles this with Overlay networks.

We created our Overlay network:We then created our first serviceIn this example, the following flags indicate:We then checked that our service had been created correctly:We repeated this step with, and

Our Tuleap stack was operational at this stage, but not accessible from the Internet. This was to be expected, as we did not have an exposed port. The reverse-proxy handles this. We decided to use HAProxy rather than Traefik or Nginx. Traefik is a good product, but HAProxy is more accurate in terms of speed and the ability to handle a heavy load. Traefik is also still young, so we will continue to keep a close eye on it. For Nginx, the decision was easy, as we had never used it for a Load-Balancer profile.

For the reverse-proxy we used other options to publish the ports:The following flags indicate:Again, the stack was operational at this stage, but we still had to validate it by testing the service on all nodes:It ran on all nodes.

To add a new stack, we simply spawned it and restarted the reverse-proxy services one by one:

At this stage, we had a scalable cluster, which means we could add nodes to the cluster and Tuleap stacks without stopping the service. Therefore, using Docker with Swarm gave us a high-uptime architecture.

Docker Swarm made our initial foray into distributed services easy. We encountered a lot of bugs with the early (1.12) version, but this improved with version 1.13. It is hard to say whether it would have been “better” with Kubernetes. Given the maturity of K8S, there may have been fewer bugs, but given our past experience with Docker, the learning curve would have been steeper in our case. Much of our work involved making our application and release management aware of “clusters of containers.” We had to replicate and test several cluster configurations to do this. This is where Swarm, with its simple setup, really delivered major benefits.|||

When it comes to clustering, the two giants, Kubernetes and Docker Swarm, are going head to head. It is all over the news, and here at Tuleap we have made our choice! But let’s back up a bit first!