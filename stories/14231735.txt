A TensorFlow QueueRunner helps to feed a TensorFlow queue using threads which are optionally managed with a TensorFlow Coordinator. QueueRunner objects can be used directly, or via higher-level APIs, which also offer automatic TensorBoard summaries.

To use a QueueRunner you need a TensorFlow queue and an op that puts a new thing in the queue every time that op is evaluated. Typically, such an op will itself involve a queue, which is a bit of a tease. To avoid that circularity, this example will use random numbers.

At this point if you evaluate in the session it will block, because nothing has been put in the queue yet.

Still nothing has been enqueued, but stands ready to make threads that will do the enqueuing.

If you put more enqueue ops in the list, or the same one multiple times, more threads will be started when things get going.

Using means we won't have to call for each thread ourselves.

There are two threads running: one for handling coordinated shutdown, and one for the enqueue op.

Now at last we can get at random values from the queue!

The feeding thread will try to keep the queue at capacity, which was set to 10, so there should always be more items available to dequeue.

Since we used a coordinator, we can shut the threads down nicely.

It's possible to work with QueueRunner directly, as shown above, but it's easier to use higher-level TensorFlow APIs that themselves use QueueRunner.

It's common for TensorFlow queue-chains to start with a list of filenames (sometimes a list of just one filename) to read data from. The function makes a queue using provided strings.

This is a , just as before, but notice we don't have an enqueue op for it. Like many things in , here TensorFlow has already done some work for us. A QueueRunner has already been made, and it was added to the collection.

You could access and run that QueueRunner directly, but makes things easier.

By default, the queue will go through the original items multiple times, or multiple epochs, and shuffle the order of strings within an epoch.

Limiting the number of epochs uses a local variable, which must be initialized.

Now the QueueRunner will close the queue when there isn't anything more to put in it, so the dequeue op will eventually give an .

A single-epoch queue will be helpful for illustrating an interesting thing about : it automatically adds a TensorBoard summary to the graph.

It's nice to have direct control over every detail of your program, but the conveniences of higher-level APIs are also pretty nice. The summary that gets added is a scalar summary representing the percent full that the queue is.

After opening up TensorBoard with and going to and and turning off plot smoothing, you'll see this:

This shows that the queue, with capacity four, started 100% full and then every time something was dequeued from it it became 25 percentage points less full until it was empty.

For this example, the queue wasn't being refilled, so we knew it would become less and less full. But if you're reading data into an input queue that you expect to keep full, it's a great diagnostic to be able to check how full it actually is while things are running, to find find out if loading data is a bottleneck.

The automatic TensorBoard logging here is also a nice first taste of all the things that happen with even higher-level TensorFlow APIs.

I'm working on Building TensorFlow systems from components, a workshop at OSCON 2017.|||

