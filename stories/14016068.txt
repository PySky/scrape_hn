In this post I’d like to introduce you to a powerful idea for thinking about concurrent programs. Synchronizable abstractions were developed more than twenty years ago by John Reppy in his work creating the language Concurrent ML. Despite that age, and their use in several large-scale systems, synchronizable abstractions remain little known.

One blog post isn’t enough to cover all of the detail of synchronizable abstractions. So my goals are less ambitions: to motivate the ideas by showing the problems they solve, to outline the primitives, combinators, and programming style they allow, and then to suggest resources if you want to learn more.

Let’s start by solving a small problem using Go-style concurrency, without the synchronizable abstractions primitives. In a typical Go program, many lightweight threads communicate with each other via synchronous messages sent over channels. When one thread wants to send or receive a message over a channel, it blocks until a suitable partner thread is ready to receive or send. This synchronous message passing makes it possible for messages to both communicate and synchronize between threads.

Let’s see an example of this style of program (and also get used to the ES6-like pseudo-code used in this article). We’ll implement a simple communication protocol where two threads atomically swap values. Our swap function accepts the value to send and returns the value received. Here’s how we want it to work:

Because channels only send messages in one direction, our implementation will need to send two message and make sure that they happen atomically.

A naive implementation of a swap channel might look like this:

It first constructs a plain channel and then returns a function that clients can call when they want to perform a swap. When a thread calls that function, it tries to both send and receive over the channel, blocking until another thread is also ready to swap. (Note the new / syntax, which allows threads to propose multiple communications, block until one of them succeeds, and then carry out the statements appropriate to that case.) The thread that succeeds in sending its value then waits to receive a value in return. The thread that first succeeds in receiving likewise then sends its value back.

Unfortunately, this code has a bug. If three threads all call at roughly the same time, we may not get an atomic swap between two of them, but instead a three-way cyclic exchange between all three threads. This is not the atomic swap we wanted.

Each line represents a thread, with time moving from left to right. Threads offer to receive (empty circle) or send (full circle) on a channel. They then block (dotted lines) until one of the communications actually occurs (arrow).

To fix this problem, we need to ensure that once two threads do the first step in the swap, they can only communicate with each other when doing the second step. The solution is to create a new channel, send it as part of the first communication, then use it for the second communication.

Even if a third thread comes along in the middle of the swap, it can’t interfere with the in-process transaction because it doesn’t have access to the private channel. It instead blocks and waits for another thread that wants to perform a swap.

Implementing this idea requires only small changes:

Notice that in this brief code, synchronous message passing is used for communication, synchronization, and security. And the correctness of our operation is encapsulated inside the returned function, preventing any thread from violating the protocol.|||

In this post I’d like to introduce you to a powerful idea for thinking about concurrent programs. Synchronizable abstractions were developed more than twenty years ago by John Reppy in his work…