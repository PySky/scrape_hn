I’d like to share something really special with you today.

I found myself idling a few months ago and wanted to tackle an issue that had been bugging me for a long time in Arachni, performance and resource utilization.

You see, Arachni has vulnerability identification, accuracy and coverage well in hand, but it has to do a lot of work to get those results and even though there are plenty of ways to optimize the system via configuration, I wasn’t completely satisfied with that approach. Any sort of substantial improvement had to come from the insides of the system, the way it approaches each web application, its very essence.

With the above in mind, I started redesigning and rewriting a few things in Arachni, but while I was doing that I ended up changing so much that the result was a new engine.

It looks basically the same from the outside (the REST and RPC APIs, interface options and reports are the same because there was no need to change them) which is good news because when the time comes for the new engine to be released, you won’t have to refamiliarise yourself with it or change any integration code you may have written.

Do not be fooled though from its appearance, because inside lies a different kind of beast. I won’t go into the technical details because they’re boring and I want to keep them as a surprise, but the gist is the following:

Talk is cheap though, so let’s look as some numbers:

As you can see, the impact of the improvements becomes more substantial as the target’s complexity and size increases, especially when it comes to scan duration and RAM usage — and for the production site the new engine consistently yielded better coverage, which is why it performed more browser jobs.

That’s all for now, but I’ll be keeping you updated on my progress.|||

