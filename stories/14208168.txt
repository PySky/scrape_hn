This is particularly true because a significant amount of funding has emerged for this research in the last two years, but the relevant organisations struggle to find people with the right skills, meaning that the area is highly talent-constrained. Beyond this technical impact, it can also be effective to work alongside technical teams to influence the culture towards greater concern for safety and positive social impact.

What follows are some of the best job openings within the leading organisations that undertake technical research relevant to the control problem.

To get these research positions, you will probably need a PhD in a relevant quantitative subject, such as computer science, maths or statistics, as well as familiarity with the latest research into AI safety. The engineering positions usually require at least a few years’ experience (except internships), and a varying degree of knowledge of machine learning. If you’re not at that stage yet, read our profile on how to enter AI safety research.

Google DeepMind is probably the largest and most advanced research group developing general machine intelligence. It includes a number of staff working on safety and ethics issues specifically.

Research Scientist

 Research Scientists at DeepMind set the research agenda in exploring cutting-edge Machine Learning and other AI techniques to solve real world issues. On the safety team, you would work closely with the DeepMind founders to explore how to ensure that as AI systems become increasingly powerful, they work for the benefit of all.

Program Manager

 Program Managers at DeepMind do what’s necessary to facilitate novel research, using their exceptional organisational skills to coordinate projects and manage people and deadlines. We see this position as important because i) DeepMind is an important organisation in AI safety, and ii) it has great potential for building your skills and using them to work with safety researchers.

Research Engineer

 Being a Research Engineer would be similar to being a Program Manager in the mechanisms for potential positive impact, but rather than providing organisational research support you would primarily be developing the algorithms, applications, and software used by the research team.

See all jobs at DeepMind.

UC Berkeley is one of the top schools for Computer Science, and the goal of CHAI is to ensure that as the capabilities of AI systems increase, they continue to operate in a way which is beneficial to humanity.

Postdoctoral Researcher

 Postdoctoral Researchers will have considerable freedom to explore topics within this area, allowing them to work on the control problem.

Successful candidates will work with the CHAI Director, Stuart Russell, or with one of the Berkeley co-Principal Investigators, Pieter Abbeel, Anca Dragan, and Tom Griffiths.

The Machine Intelligence Research Institute (MIRI) was one of the first groups to become concerned about the risks from AI in the early 2000s, and has published a number of papers on safety issues and how to resolve them.

Research Fellow

 Research Fellows work to make progress on the alignment problem, which involves novel research on a number of open problems in computer science, decision theory, mathematical logic, and other fields. MIRI are looking for Research Fellows for both their traditional and machine learning agendas. MIRI selects candidates strongly on math talent and weighs traditional academic backgrounds less heavily than other research positions in AI safety. So if you think you’d enjoy research in a non-traditional academic background, and get along with the team, this is an excellent path to contribute to solving the control problem.

OpenAI was founded in 2015 with the goal of conducting research into how to make AI safe and ensuring the benefits are as widely distributed as possible. It has received $1 billion in funding commitments from the technology community, and is one of the leading organisations working on general AI development.

Machine Learning (Researcher)

 This position is a research role with the a broad remit, so it’s an opportunity to do vital work on the ‘control problem’ of ensuring AI safety. To get this role, you need to have shown exceptional achievements in a quantitative field (not necessarily ML/AI), and have a shot at being a world expert. For this reason, this role is incredibly competitive, but if you can get it, it’s likely to be one of your highest-impact options.

Special Projects

 The Special Projects role involves working on one of the projects listed here, which are “problem areas likely to be important both for advancing AI and for its long-run impact on society”. They were formulated by experts we believe are trying to improve the long-run safety and consequences of AI/ML systems, and we see this as a concrete area in which those with the relevant background can contribute to their improvement.

See all jobs at OpenAI.

The Future of Humanity Institute at Oxford University was founded by Professor Nick Bostrom, author of Superintelligence. It has a number of academic staff conducting both technical and strategic research. AI safety is one of its focus areas.

Reinforcement Learning Intern

 Interns on the technical research team have the opportunity to contribute to work on a specific project in reinforcement learning (RL). Previous interns have worked on software for Inverse Reinforcement Learning, on a framework for RL with a human teacher, and on RL agents that do active learning. We see this as an outstanding opportunity to work with an excellent team, improve your technical skills, and explore ideas with some of the key players in AI safety, while strengthening your application to graduate school.|||

Find a high impact job with our list of some of the highest impact jobs available right now. Never miss one with our monthly newsletter.