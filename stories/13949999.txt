Most years at Sussex, I run a two-hour practical session for quantitative researchers on the topic of Regular Expressions (see slides & teaching materials).

This year, I got asked to cover some basic web scraping techniques too. This set me thinking — what are the best tools for web scraping? And are these also the best tools with which to teach it?

Web scraping is the process of extracting useful data from web pages — with appropriate regard to copyright and database rights — which commonly also requires automating a series of interactions with those pages.

I took to Google, refreshing my memory on programming language libraries like Scrapy (Python) and Mechanize (Ruby, Perl), native GUI scraping applications, and online scraping services.

And then I remembered that time last year when I copied all my shopping favourites from one Ocado account to another using only Google Chrome, and the answer hit me right in the face.

Arguably the best web scraping approach uses nothing but a common-or-garden web browser and its built-in developer tools.

All modern web browsers provide tools to help web developers design and debug their pages, styles and scripts. Two of these tools are especially helpful for web scraping:

We'll discuss exactly what code to write after considering the advantages and disadvantages of this scraping approach. Or you can dive straight in to the example JavaScript code.

This approach has several advantages for teaching.

There are also a couple of possible disadvantages:

This approach also has some key advantages in its own right, teaching or no.

There are also a few potential disadvantages relative to some of the other options.

So, what code do we need to write in the Console?

For exposition, I've divided the code into five parts. Happily, four of the five are supporting functionality that will probably remain untouched between different scraping tasks. I'll deal with these first.

We begin with a function that sets us up by making and styling two new page elements. These elements are deliberately allowed to leak into the global scope (by omitting ), so we can address them later.

Q: Why an ? A: We can't load new pages straight into the current window, because that would wipe out the code we are writing here. We could load new pages in another window or tab instead, but (a) this requires us to disable popup-blocking in some browsers, and (b) getting notified when a page has loaded in another window or tab is tricky.

Once we've created the , we ask to be notified every time a new page loads inside it (using ). Then we point it to our first scraping location. We set this location as the URL of the current page.

Q: Why the current page URL? A: Using the current URL is a nice, simple way to communicate our starting point to the script. But more importantly, browsers won't allow one website to access pages from another website, for reasons of security and privacy. So if we want to access a page to scrape its content, we need our script to appear to come from the same website (as judged by the domain — e.g. mackerron.com). In the console, our scripts appear to come from whatever page is currently loaded — so to make it all work, we need to load a page from the scraping target domain.

Unsurprisingly, we'll call this function when we're ready to begin.

This function writes its arguments into the , as the columns of one row of a CSV file. It operates sensibly with numbers, booleans, text, objects, and and . Text is 'escaped' so that the CSV format isn't screwed up by variables containing commas, newlines and double quotes. Dates come out in a UTC/GMT format that's readable by both humans and Excel.

The function also keeps the scrolled to the bottom (if we haven't manually scrolled it up), where we can see new data as it is added.

We call this function every time we've scraped a new row of data from a target web page and want to save it somewhere.

We call this function if we ever need to stop our scraping process prematurely. It stops the next action (and thus all subsequent actions) happening, whether that's processing a loaded page or loading a new one.

We include a couple of limited polyfills to stop up gaps in Internet Explorer's JavaScript support. The statements mean they have no effect in other browsers.

All our custom scraping work happens in this function. This is the one you will definitely need to change when applying this scraping technique to other sites and data sets.

The function shown here implements a simple example scraping job, which is to extract vote counts for UK government petitions. I've picked only those with a government response, which are spread across (at the time of writing) nine pages.

First, we take a reference to the document loaded inside the , and the current date, which we will output as a timestamp for the data scraped on this page.

Next, we select every page element with CSS class , by calling , and iterate over the returned elements — extracting the data we are interested in, and calling with the results.

Finally, we look for a [Next] button, again by CSS class.

If one is found, we simulate a click on it, to navigate the next page. But we insert a half-second delay before doing so, via , to play nice and avoid pummelling the server. Once the next page has loaded, the callback already attached to our will ensure this function, , is called again.

Alternatively, if no [Next] button is found, we have reached the end of the petitions list, and we call it a day.

Once this script has finished executing (as signalled by the message 'Finished.' appearing in the Console), we can simply copy all the text in the (click, Ctrl/Cmd-A, Ctrl/Cmd-C), paste into TextEdit or Notepad, and save with a .csv extension, ready to open in Excel, R, Stata, etc.

The complete example code is ready to paste straight into the Console and see working.

I hope you find this technique useful. Please do get in touch with any feedback, or submit an issue or pull request.

Regarding intellectual property, the example JavaScript code is MIT licenced. All rights in this tutoral text are reserved by the author (do contact me if you have some use in mind).|||

web-scraping-for-researchers - Press Cmd + Alt + I