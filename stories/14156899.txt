TLDR: Leverage Kubernetes annotations across your cluster to declaratively configure management of monitoring and logging.

Two of the largest surfaces of applications that make contact with infrastructure are monitoring and logging, and this post talks how to approach both these needs in a scalable, composable, and simplified way.

If you are familiar with the Kubernetes, Prometheus, Fluentd, and the ELK stack, feel free to skip the background.

Kubernetes is a popular cluster management software that takes some simple concepts and composes them in a beautiful way with a metadata-rich API.

A core concept is the pod, which is one or more co-located containers. A pod can be individually started, but if it terminates or the server it is running on restarts, there is not any procedure to ensure that the workload will come back online.

The real power of Kubernetes comes from the API types that describe what pods should look like and manage the scheduling (and querying) of pods. Kubernetes does this with the use of key-value pairs on API objects called Labels. As an example, when a Deployment API-object is created, the Kubernetes Scheduler ensures that a specified number of pods are running across the available servers.

If you submitted the the following Deployment to Kubernetes, 2 pods would start up on your cluster running Nginx exposing port 80.

One other set of metadata on API objects is annotations. Annotations are arbitrary key-value pairs similar to Labels, but rather than being used for scheduling, are available for other applications to make decisions about what they care about.

Fluentd is a log management application that takes streams of data from disparate sources, runs a set of modifications, filters, and slicing, and forwards those streams to a variety of aggregation, visualization, and storage systems.

To quote the documentation, “Prometheus is an open-source systems monitoring and alerting toolkit” that has become widely used by operations teams of all sizes as a default monitoring system.

Our use of Prometheus at Skuid has grown over the past year from monitoring key-infrastructure components like Kubernetes and Jenkins, to being where we monitor application health and performance.

The ELK stack is an open-source log management solution. Elasticsearch is a document search server, Logstash is a server-side log shipping software (similar to Fluentd), and Kibana is a dashboard and visualization server that queries Elasticsearch.

At Skuid, we use AWS Elasticsearch and Kibana, and rather than Logstash, we use Fluentd for log shipping. We also send our logs up to CloudWatch Logs, which triggers a lambda function, which passes the log messages on to AWS Elasticsearch.

When we initially configured Prometheus, the section looked something like this:

Essentially, for each service we wanted to monitor, we added a new entry in the configuration file. After digging into the Prometheus documentation, we realized the that Prometheus’ Kubernetes integration can include any annotations that we add! Using label replacing, we updated our Kubernetes services and Prometheus configuration to the following:

The result of this is that rather than individually writing configuration for each new application we want to monitor, we use the Kubernetes annotations to compose the target address. With this setup, any new service we want to monitor only needs to add the relevant annotations.

Along with monitoring, logging was a service management task where every time we created a new service, we needed to update some configuration to support that new service. Here’s an example of our logging configuration:

This worked for our first handful of services, but as you can imagine this quickly got out of hand as more services were added. Taking a cue from Prometheus, we started rethinking how we could declaratively define the forwarding of our logs. What we ended up with was adding an annotation to pods in our Deployments:

When a pod uses the above Kubernetes annotation, the kubernetes-metadata-filter plugin will add the following (along with other metadata)

We also wrote a Fluentd plugin, fluent-plugin-json-lookup, that will extract a key-value pairs from a JSON value. The plugin takes the above message, and outputs the following message:

The Fluentd configuration looks like this:

In the end we reduced our Fluentd configuration (short of ingestion and upload) to:

With a quick update to our lambda function, we were able to forward any logs we wanted from Kubernetes without having to touch our Fluentd configuration!

Now whenever we launch a new service, ingesting logs and adding metric scraping is fully automated in a declarative, composed way, and we can focus on the services rather than the management of them.|||

Two of the largest surfaces of applications that make contact with infrastructure are monitoring and logging. This post talks about how we simplified the configs for these with Kubernetes annotations.