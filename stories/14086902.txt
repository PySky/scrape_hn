This is the last piece of my “Data Coding 101: Introduction to Bash” series, where I introduce the very basics of how to use the command line for data science. If you read all the 6 articles – including this one – you will have a good enough base-knowledge to start your first data project. Today I’ll show you 4 more command line tools, that we are using very commonly in only data analyses: , , and .

 And if you haven’t read the previous articles, I recommend you to start here:

As we have discussed it before bash handles all your data as text. This means, that if you want to clean your data, you should think about the process as you would do it with a text file. stands for “stream editor” and it’s a very cool tool to modify text based on common patterns across your whole file.

Let’s see a concrete example. Go back to your flightdelays.csv file (If you don’t know, what’s that, check this article and download it!)

You already know, that will print the first 10 lines of your file to the screen.

As you can see the field separator is “,”. It’s not the most useful delimiter usually, so let’s try to change it to something else. First try to use spaces instead. Here comes into play:

Note: In episode 2 I’ve already explained the role of the (pipe) character. If you are not sure, what’s happening, read it first!

We have printed the first 10 lines, then modified the output with ! What happens step by step:

–» After you have to specify what and how would you like to modify in your file. You can do that with an expression between apostrophes. In this case: refers to “substitute” and it defines that we want to remove something and put something else instead of that. is usually just the delimiter between the different parameters of the command. is the character, you want to remove, and (the space after the second ) is the character, that you want to see instead. Eventually (stands for “global”) tells to bash, that this should happen for every appearance of the character. (If you don’t add , will change only the first in each line.) Explain the same in a picture:

Most of the people are using only for substitute characters, but there is so much more potential in it.

First of all, you can change whole strings. Eg. try this:



See? All the “SMF” strings have been replaced with “DATA36_TUTORIALS”.

Okay, maybe this is not the most useful stuff, you would do with your file… But you get the point!

One important thing, that you have to know about , that it works with regular expressions (also known as regexp or regex). I’ll write a whole article about this topic, but for now, here’s the wikipedia article about it. Please be aware, that if you want to do special modifications with (eg. change all the numbers to “ ”), you can do it, but you have to be familiar with regexp. (Or you have to be really good with googling! ;-)) By the way the above example (change all the numbers in your file to “x”) would look something like this:

 

 Where [0-9] is a regular expression for all the numbers.

It’s also good to know, that is not just good for substitution, but also to delete specific lines. Eg. in my recent Medium article (Learning languages very quickly — with the help of some very basic Data Science) I have removed all the empty lines from my file with this command:

–» where is a regular expression for empty lines and refers to “delete”

is a cool tool! Use it!

Note: this is the best online tutorial about sed that I’ve ever read: http://www.grymoire.com/Unix/Sed.html

A much simpler tool: . With this command line tool, you can literally print dates in every format. Here are 3 examples:

Note: I wrote this article on 2nd of April, hence it printed the current date and time.

As you can see, there’s a pretty wide range of formatting here. The only thing you need to do is to type , then a character (which means, that you want to add an attribute to specify your format), then just simply add the formatting options (eg. or ) and any other characters (usually between apostrophes), that you want to see there. Example:

I encourage you to go into the manual of with the  command line tool. Here you will find all the possible formats.

is a command that will be very useful in SQL as well. But now let’s stay with the command line. Here’s a little example. You can just simply follow me here, but you can also try this on your own data server, if you create these files:

Now apply for these 2 files…



…you will get these results:

So what happened here?

 In one sentence the two file have been merged based on the first column!

A bit more detail though:

 First things first, automatically recognized the delimiter (it was a space) and identified the columns. If your delimiter is not a space or a tab, you have to define this value with the option (same as it was with for instance).

 Secondly it automatically picked the first column of both files as the field, that we should on. That’s just perfect in this case. But if you would like to use another column you can specify that with an option as well (I’ll get back to that later).

And third: the did just happen. All the animals have colors and weights in one file. As you can see the monkey didn’t show up after the merge, this is, because there’s no monkey in the second file at all. And as you can also see, we have 4 zebras after the . This is because, this merge created all the possible combinations of colors and weights.

Note: the biggest difference between bash’s and SQL’s is, that in bash you merge text files and in SQL you merge tables. From our human perspective it looks pretty much the same, but from the computer’s perspective it’s quite different. Because of this – without going into further details…

…it’s very important to know, that bash only works properly with sorted files! If you don’t sort your files, you will get an error message – also fake results. Take away: before always .

For now it’s enough if you know only these very basic things about . Bit below in this article I’ll show you some more exciting thing regarding that.

But first, get familiar with one last command line tool:

is like another programming language on top of bash – and you don’t have to know this one in details either. But it’s still good to keep in mind, that you can call this language from bash and you can pipe it into your other command line tools.

 Usually is really great for advanced line-based processing. Eg. if you want to print only those lines, where the 3rd column is bigger than 2.500, your tool will be .

I’ll give you here only 2 practical examples of how will be useful for you.

Let’s say we want to have 4 specific columns from our good old flightdelays.csv file:

Cool. But the order of the columns would be better like this: Origin, ArrDelay, Dest, DepDelay. You might try to switch the columns in your command like this:



But I have bad news. With you can’t change the order of the columns.

See? Same order! Why is it so? Well, I don’t know, but this is the limitation of the command and you can simply solve it by . Try this:



Very similar to ! Except that here the option for the delimiter will be (standing for “field-separator) and not . And the way how you define the fields themselves is bit different too: . As you might have noticed the output’s separator won’t be comma anymore, but spaces – this is a default setting of . But you can add here anything similarly to or to , if you put it between quotation marks. Eg.

This is a more straight-forward one. Let’s say you want to summarize all the ArrDelays (like you would do in Excel with the SUM command.) First, you have to the 15th column. Then you have to make the sum with wk. Here’s the whole command:

You might ask, why have we used the command first, why didn’t we get done the whole stuff with one command – as we have just seen, that is good to cut columns too. The answer here is computing time. How works is a bit different, than . basically goes through your file line by line and always adds the next value to a variable (called in this case). That’s why is there for. This is okay, until you have only one column, but if have to find this one column in each line… Well that can take forever. So that’s why! We use the best tools for the given tasks – so we don’t have to wait forever to get the results…

Note: If you are curious, I still recommend to try to get this number purely with … And you will see the difference…

Nice. We got through on 4 new tools. It’s time to wrap them up in one ultimate exercise.

 It sounds like this:

Take the flightdelays.csv file and calculate the summary of all the ArrDelays happened on Mondays! For the sake of practicing, let’s say, you can only use these four columns:

Hint: You will have to use sed, date and awk for sure. And in my solution I’ll use join as well!

 .

 .

 .

 And the solution is here!

 Put the whole stuff into a script called arrdelay_calculator.sh

If you want to understand the script, run it line by line!

 But of course, I’ll give you a little bit of explanation here:

 

 First, I have the columns I need and formatted the output a bit more readable. Then I’ve printed the whole stuff into a temporary file.

I’ve printed all the dates in 2007 into another temporary file. I made sure that the date format is the exact same, what I have on my other file (2007-1-1)!



 I’ve sorted my temporary files and put them into other temporary files – and at the same time I removed every day, but Mondays from my date file.



 Eventually I made the and calculated the sum of the ArrDelays!



 I’ve also removed all my temporary files (this is gonna be important, when you want to run the script for the second time).

The result is by the way:

I promise I’ll have some more exercises on bash/command line in the future – maybe in a video format! But for now, I have good news! You made it! You went through all the 6 bash articles and you have learnt the essentials of the command line! Now I recommend to look after some pet projects and sharpen your skills via learning-by-doing!

 And at the same time stay with me, because very soon I’ll come up with my next initiatives: SQL and Python tutorials! They will be similar than this one: very easy to understand for everyone, and from the very basics! If you don’t want to miss it, subscribe to my Newsletter!|||

Today I’ll show you 4 more command line tools, that we are using very commonly in only data analyses: sed, awk, join and date.