GKE release notes will be posted here for each release. They will also be posted to gke-release-notes@googlegroups.com. To receive emails when new releases come out, use this link and click "Join group".

Kubernetes v1.5.7 is the default version for new clusters.

The following Kubernetes versions are available for new clusters:

To create a cluster at a version which is not the default, run:

The following Kubernetes versions are available for container cluster node upgrades/downgrades:

Kubernetes v1.5.2 is the default version for new clusters. The command-line tool and kubectl 1.5+ support using credentials for authentication. Currently, and configure kubectl to use Application Default Credentials to authenticate to Container Clusters. If these differ from the IAM role that the command-line tool is using, kubectl requests can fail authentication (#30617). With Google Cloud SDK 140.0.0 and kubectl 1.5+, the command-line tool can configure kubectl to use its own credentials. This means that if, e.g., the command-line is configured to use a service account, kubectl will authenticate as the same service account. To enable using the command-line tool's own credentials, set the property to false: The current default behavior is to continue using application default credentials. The command-line tool credentials will be made the default for kubectl configuration (via ) in a future release.

Kubernetes v1.4.3 is the default version for new clusters. Reminder that the base OS image for nodes has changed in the 1.4 release. A set of known issues have been identified and have been documented here. If you suspect that your application or workflow is having problems with new clusters, you may select the old ContainerVM by following the opt-out instructions documented here. Rewrote the node upgrade logic to make it less disruptive by waiting for the node to register with the Kubernetes master before upgrading the next node. Added support for new clusters and node-pools to use preemptible VM instances by using the flag. See and for more details.

Known Issues with v1.4.0 masters and older nodes

The release documented below is being rolled out over the next few days.

Clusters can now be created with up to 250 nodes. The Google Compute Engine load balancer controller addon is added by default to new clusters. Learn more. Kubernetes v1.1.1 is the default version for new clusters. Important Note: The packaged is version 1.0.7, consequently new Kubernetes 1.1 APIs like autoscaling will not be available via until next week's push of the binary. Users who want access before then can manually download a 1.1 from: And then to install it. Kubernetes v0.19.3 and v0.21.4 are no longer supported for nodes. New clusters using the machine type must contain at least three nodes. This ensures that there is enough memory in the cluster to run more than just a couple of very small pods.

Google Container Engine is out of beta.

Users must upgrade their configuration files to the v1 Kubernetes API before August 5th, 2015. This applies to any Beta Container Engine cluster created before July 21st.

Google Container Engine will upgrade container cluster masters beginning on August 5th, to use the v1 Kubernetes API. If you'd like to upgrade prior, please sign up for an early upgrade.

This upgrade removes support for the v1beta3 API. All configuration files must be formatted according to the v1 specification to ensure that your cluster remains functional. The v1 API represents the production-ready set of APIs for Kubernetes and Container Engine.

If your configuration files already use the v1 specification, no action is required.

Refer to the reference documentation for more details.

Kubernetes v0.8.1 is the default version for newly created clusters. Our v0.8.1 support includes changes on the 0.8 branch at 0.8.1. Removed support for creating clusters at Kubernetes v0.8.0. Existing clusters at this version can still be used and deleted. Service accounts and auth scopes can be added to node instances at the time of creation for all pods to use. The command line interface now renders multiple error messages across newlines and tabs, instead of using a comma separator. Machine type information has been fixed in the cluster details page of the Google Cloud Platform Console.

Kubernetes v0.6.1 is the default version for newly created clusters. Google Container Engine now reserves a /14 CIDR range for new clusters. Previously, a /16 was reserved. New clusters created with Kubernetes v0.4.4 now use the backports-debian-7-wheezy-v20141108 image. This replaces the previous backports-debian-7-wheezy-v20141021 image. New clusters created with Kubernetes v0.5.5 or v0.6.1 now use the container-vm image, instead of the Debian backports image. The Service Operations documentation has been updated to describe the option. A new command has been added to the CLI. This is a pass-through command to call the native Kubernetes kubectl client with arbitrary commands, using the command-line tool to handle authentication. The flag in all CLI commands has been renamed to . New and support for cluster operations.

The syntax for creating a pod with the Google Container Engine command line interface has changed. The name of the pod is now specified as the value of a flag. See the Pod Operations page for details. Clusters and Operations returned by the API now include a field and Operations also include a field, which contain the full URL of the given resource. Added support for Kubernetes v0.4.4 and Kubernetes v0.5.5. The default version is now v0.4.4. Refer to the Kubernetes release notes for information about each release. Our v0.4.4 support includes changes on the 0.4 branch from 0.4.2 through 0.4.4. Our v0.5.5 support includes changes on the 0.5 branch through 0.5.5. Removed support for creating clusters at Kubernetes v0.4.2. Existing clusters at this version can still be used and deleted.

New error message that catches cluster creation failure due to missing network. There is currently a bug preventing the default cluster name from working if the local configuration cache is missing. If you see a stack trace when omitting , repeat the command once with the flag specified. Subsequent commands can omit the flag. The default cluster name is set to the value of the new cluster when a cluster is successfully created. The command lists clusters across all zones if no flag is specified. The command ignores any default zone that may be set.

Google Container Engine is a new service that creates and manages Kubernetes clusters for Google Cloud Platform users.

Container Engine is currently in Alpha state; it is suitable for experimentation and is intended to provide an early view of the production service, but customers are strongly encouraged not to run production workloads on it.

The underlying open source Kubernetes project is being actively developed by the community and is not considered ready for production use. This version of Google Container Engine is based on Kubernetes public build v0.4.2. While the Kubernetes community is working hard to address community-reported issues as they are reported, there are some known issues in the v0.4.2 release that will be addressed in v0.5 and that will be incorporated into Google Container Engine in the coming days.

Known issues with the Kubernetes 0.4.2 release

(Issue #1730) External health checks that use in-container scripts (exec) do not work. Process, HTTP and TCP health checks work properly. Health checks that use in-container shell execution are not functioning; they always report Unknown. This is a result of the transition to introduced in Docker version 1.3. At this time process-level health checks, TCP socket health checks, and HTTP level health checks are functional. This has been addressed in v0.5 and will be available shortly. (Issue #1712) Pod update operations fails. In v0.4.2, pod update functionality is not implemented, and a call to the update API returns an unimplemented error. Pods must be updated by tear down and recreate. This will be implemented in v0.5. (Issue #974) Silent failure on internal service port number collision: Each Kubernetes service needs a unique network port assignment. Currently if you try to create a second service with a port number that conflicts with an existing service, the operation succeeds but the second service will not receive network traffic. This has been fixed, and will be available in v0.5. (Issue #1161) External service load balancing. The current Kubernetes design includes a model that does a 1:1 mapping between an externally-exposed port number at the cluster level, and a service. This means that only a single external service can exist on a given port. For now this is a hard limitation of the service.

In addition to issues with the underlying Kubernetes project, there are some known issues with the Google Container Engine tools and API that will be addressed in subsequent releases.

Kubecfg binary conflicts: During the Google Cloud Platform SDK installation, kubecfg v0.4.1 is installed and placed on the path by the Google Cloud SDK. Depending on your $PATH variable, this version may conflict with other installed versions from the open source Kubernetes product. Containers are assigned private IPs in the range 10.40.0.0/16 to 10.239.0.0/16. If you have changed your default network settings from 10.240.0.0/16, clusters may create successfully, but fail during operation. All Container Engine nodes are started with and require project level read-write scope. This is temporarily required to support the dynamic mounting of PD-based volumes to nodes. In future releases nodes will revert to default read-only project scope. Windows is not currently supported. The command is built on top of the Kubernetes clientâ€™s binary, which is not yet available on Windows. The default network is required. Container Engine relies on the existence of the default network (part of all new Google Cloud projects), and tries to create routes that use it. If you have deleted this network since creating your project, Container Engine cluster creation will fail. Go to the Networks page in the Cloud Platform Console and select your project. Click in the All networks list. Click Create new next to Firewall rules. Create a second firewall rule with the following values:|||

