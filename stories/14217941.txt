Summary: This is an introductory philosophy article, showing how a philosopher would approach a typical moral case related to autonomous driving.

Moral machine is a ‘platform for gathering a human perspective on moral decisions made by machine intelligence.’ The user is presented with moral dilemmas and has to decide which of multiple actions seems more (morally) acceptable.

Let’s look at one of the cases presented, and see how a philosopher would approach such a problem.

A typical scenario goes like this: A self-driving car with a sudden brake failure can either crash into a barrier, killing the people on board (1 male and 1 female athlete, and 2 female executives); or it can swerve and drive through a pedestrian crossing, killing: 1 boy, 1 pregnant woman, 1 baby, 1 dog and 1 cat. (You have to love the numerals: “1 baby”). All these people are crossing a red traffic light, and shouldn’t be there in the first place.

Obviously, there are multiple issues here:

The case clearly states that the traffic light is red for the pedestrians, and that, therefore, none of them should actually be crossing the street at this moment.

This might be relevant to some moral theories.

This kind of analysis applies only to rational, free adults. We cannot well apply this to children, severely mentally disabled people, or pets. Of course, one could say that children (and other groups that are cognitively unable to perceive the dangers) have no business taking part (unsupervised) in street traffic anyway. They should never have been confronted with a red traffic light and the free decision to obey it or not.

But this is unrealistic. A social environment must be reasonably safe and forgiving of errors, rather than being harsh and dangerous. If a child or pet ventures out of the supervision of an adult, we should have a reasonable expectation that this will not as a rule endanger its life. Bad things can happen, but they shouldn’t be the norm.

What is the moral significance of the car having a brake failure, rather than any other malfunction?

First, trivially, a brake failure makes sure that the car can continue moving forward, so that the pedestrians are at risk. This serves the purpose of the example.

Second, a brake failure is a rare malfunction that is usually attributable to bad servicing of the vehicle. A regularly inspected car should not experience brake failures.

This means that a brake failure, unlike other possible causes for this scenario (for instance, sudden driver death), puts a moral blame on the vehicle’s owner. Had the owner had the car inspected and serviced regularly, the accident would probably have been avoided. Assuming (as it would normally be the case) that the owner is also the driver of the vehicle, now the driver is to blame for the accident (to some extent). How does this change things?

I am not entirely sure what the significance of this is supposed to be. I’m not even sure that I want to know.

Is the idea that an athlete is more valuable than, say, a philosopher? Or, god forbid, someone who’s bad at sports? Or someone with a disability? Is an executive more valuable than a social worker? Or a housewife? Or is it the opposite entirely? Does the case imply that on the car are only executives and athletes, so crashing it wouldn’t cause much harm? In any case, that part of the specification is somewhat of a mystery.

Of course, different moral theories do have something to say about that:

Another interesting questions is: why “executives” and “athletes,” rather than mathematicians and composers, or calligraphers and hermits? It seems that the scenario here reflects the cultural prejudices of its creators: students and postgraduates at an elite US university, people who are going to be technology startup founders, and who, in agreement with their social environment, value two things: physical fitness and commercial success.

There’s nothing inherently bad about one’s choice of role models like that. But there is a danger here, that machines that are built to moral specifications like these are going to incorporate these value judgements as part of their code. And when Ford sells a car that incorporates these value judgements to a buyer in China or Qatar, the preference for executives and athletes (over poets and Imams) is going to be exported along with that piece of technology. Not explicitly and openly, but as part of some hidden value judgement, a priority statement in a life-or-death decision that might never actually be executed. But the preference will be there, and this is the danger of this kind of hidden moral imperialism.

Should we then build cars that favour Imams over executives when we build cars for Islamic audiences? Should we favour athletes over disabled, blacks over whites, women over men, or the opposites in each case? These are important issues that are not currently being addressed.

Similarly, the choice of victims also tells a story. Children and pets are not fully responsible for their actions, which complicates the jaywalking question (see above). Pregnant women (as well as children) usually symbolise people in need of special protection and care (this is why they get special seats assigned on buses). Of course, the elderly also do, but the case leaves them out. If the car has the choice between killing a pregnant woman and an old man, which one should it be?

We tend to favour the future, and we tend to protect the chances of children and the unborn. We would probably say that the old man “has had his life” already, and that we should therefore favour the young and the unborn. But again, this, if it is going to be a design feature of a self-driving car, needs to be a conscious choice, and not something left to chance or to the unreflected regurgitating of cultural stereotypes by some nameless US programmer.

One aspect is easily overlooked in the discussion of self-driving cars: that we should compare the car’s performance not with some Platonic ideal of a car that always acts morally right; but instead, we should compare it with the expected performance of a good, rational human driver. If the car performs consistently better than the human driver, then this is a strong argument in favour of the car. It doesn’t need to be perfect, just better.

Now what would a human driver do in the case discussed here?

Is it at all probable that he would even hesitate? Would any human driver consider killing himself in order to spare a handful of pedestrians who are illegally crossing a red light, and who shouldn’t have been in his way anyway? I don’t think so. The survival instinct of the human driver will clearly dictate to him that the only choice is to avoid the barrier and kill the pedestrians, and that’s it. An autonomous car that would act in this way would therefore act naturally and consistently human.

Let’s also not overlook the fact that this scenario is going to be very rare. Like all these made-up cases, there is a whole chain of conditions that must be fulfilled in order for this case to play out as designed:

A combination of all these factors is quite rare, indeed. And if an accident like this does happen once, does it mean that we must design self-driving cars with such freak accident scenarios in mind?

So perhaps we can say that, although there is a theoretical risk of a car (self-driving or not) killing some pedestrians in a very low-probability scenario like that, the likelihood is so small as not to warrant further consideration. In the same way, we don’t design (regular) cars to protect their passengers against driving off a bridge, driving into deep water, or coming into the explosion radius of a bomb. Perhaps the whole discussion is without merit, and we should concentrate on solving more realistic problems, of which there also are enough to keep us busy for a long time.|||

