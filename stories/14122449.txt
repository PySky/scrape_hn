Amazon Web Services (AWS) is a juggernaut in the public cloud industry, holding 45% market share according to Synergy Research Group's Q3 2016 data. They were one of the first to make cloud infrastructures available to the masses. DigitalOcean is an up-and-coming cloud host that offers SSD performance and a really simple API for quick and easy developer integrations.

I've used both and have found that each have their pros and cons, but one thing I was curious about was performance. To test, I used Nanobox to setup two identical test apps.

As with any experiment, it's important to remove as many variables as possible. Nanobox lets me deploy a single codebase to servers on both AWS and DigitalOcean. On deploy, Nanobox uses settings defined in the to build and configure my app's runtime and environment, both locally and on live servers.

Essentially it lets me easily create two identical apps on separate hosts.

Here's what the process looked like (feel free to duplicate it):

Since I really only wanted to test the baseline performance of the hardware, I didn't need a super-complex app to test against. I used a simple "to-do" app built with Phoenix (you can view the source code here) that includes basic CRUD functionality. The app is comprised of an Elixir runtime, a Phoenix webserver, and Postgres database.

Nanobox has official integrations with both AWS and DigitalOcean. In my Nanobox dashboard, I connected my Nanobox account to my AWS and DigitalOcean accounts so Nanobox could provision and deploy the apps on my behalf.

The Setting up a Hosting Account documentation walks through the process of connecting to a hosting provider.

In the Nanobox dashboard, I created two separate apps â€“ and . During the app creation process, Nanobox let's you select which of your providers and to use as well as a region.

I wanted the two apps to be as geographically close as possible to remove any significant latency potential, so I provisioned in Amazon's US West datacenter in Northern California and in DigitalOcean's San Francisco 1 datacenter.

In my local project, I added each of my apps as remotes and deployed to each.

Nanobox uses Docker to build and network all of the necessary app components on each server. Code and environment-wise, the two apps were identical. The only difference was the underlying hardware.

I also wanted to test the apps using both HTTP and HTTPS connections, so I used Nanobox to install LetsEncrypt certificates on each app. This process is covered in the Adding SSL/TLS documentation, but it only took about 3 minutes to install both.

There isn't a one-to-one relationship between DigitalOcean's and AWS's EC2's offering, but I used as closely spec'd servers as I could.

Amazon Lightsail does provide one-to-one server specs with DigitalOcean, but is still considered experimental and is not available in all AWS datacenters... yet.

I used Siege to test the two apps with simulated traffic. Siege provides a summary after each test.

I ran Siege in "benchmark" mode which disables the default behavior of including a delay between requests. Each test ran for 1 minute with 10 concurrent users. Below are the commands I used:

I ran each benchmark five times with breaks between each to allow the server time to return to its baseline resource usage. The data below represents averages of these tests.

Siege returned the following data after each test:

Transactions: 

 The number of server hits during the test.

Transaction Rate: 

 Number of transactions per second.

Throughput: 

 Average throughput throughout the duration of the test.

Concurrency: 

 Average number of simultaneous connections (the lower the better).

Longest Transaction: 

 Duration of the longest transaction throughout the test.

Shortest Transaction: 

 Duration of the shortest transaction throughout the test.

I'll be honest, I'm a little surprised at how one-sided the results are. I thought DigitalOcean's SSDs would give them a significant advantage, but it doesn't appear they have.

In many of the metrics, AWS had only a slight edge over DigitalOcean, but when it wasn't close, it wasn't close at all. This is most apparent in the HTTPS tests.

AWS's response times and transaction lengths were much more consistent than DigitalOcean's. While the majority of DigitalOcean's were low, I did see many more outliers than I did with AWS.

This is more interesting than significant, but in all the tests, the app on AWS consistently used 2-5% more memory than the app running on DigitalOcean. I don't think this would have a significant impact in a production app, but it is interesting.

CPU generally isn't a bottleneck for most web applications, but terminating SSL/TLS connections is a CPU-heavy task. In the tests using HTTPS, the DigitalOcean droplet consistently used 100% of its available CPU while the EC2 instance used somewhere in the 80% range.

The effects are evident in the results. AWS, hands-down, outperformed DigitalOcean when handling requests over HTTPS. Everything from response times and transfer rates to the number of transactions completed during the test.

AWS provides "burstable" CPU with a guaranteed baseline .25 Core. This means that you can (and do) use more when needed, potentially up to the physical limit of the bare-metal machine on which the instance is running. This means that the EC2 instance could actually have access to 1, 2, 4, or 8+ cores, depending on what the machine actually has and what other instances on the same machine are using.

AWS came out the clear winner in these tests. There are a lot of factors that could play into this including datacenter nuances, network I/O, network latency, etc. I'd be interested to see others run the same tests in other datacenters. Any takers?|||

A performance comparison between two identical web applications deployed on AWS and DigitalOcean using the Siege command line tool to simulate web traffic.