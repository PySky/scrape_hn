A long-ago cartoon in The New Yorker put it plainly: "On the Internet, nobody knows you’re a dog." If that cartoon had been written today, the caption might have read, "On the Internet, nobody knows you’re a fraud."

Scam artists, snake oil salesmen, sock puppets, bot armies and bullies - every time we look up, it seems as though we discover another form of dishonesty, grifting grown to global scale via the magnificent yet terrifying combination of Internet and smartphone.

None of that should surprise us. People are wonderful and horrible. The network we’ve built for ourselves serves both the honest and the liar. But we have no infrastructure to manage a planet of thieves.

Navigating this stuff goes well beyond ‘caveat emptor’, into the darkest secrets of spearphishing and social engineering playing on our higher selves for the basest reasons. It’s no longer an African prince offering you a hundred million dollars for your assistance; it’s a customer who carefully noted all her transactions and registration numbers on a Word document she’s enclosed in a very helpful email.

Security has been stretched to the breaking point. If things continue as they have, the costs of connectivity could begin to outweigh the benefits, and at that point, the post-Web civilisation of sharing and knowledge, already fraying, would unwind comprehensively, as people and businesses withdraw behind defensible perimeters and call it a day.

All of this served as subtext - never spoken, yet always front of mind - at the Twenty-Sixth International Conference on the World Wide Web. In some broader sense, this is all the Web’s fault - the shadow of its culture of sharing - so might it be a problem that the Web can fix?

This question obsessed the hundreds of research postgraduates presenting papers and posters at the conference. Insofar as papers presented by the Web’s core research community are a reliable indicator of the future direction for the Web, that future centers on learning how to detect lies.

Detecting false advertisements, bullies, and bots - all of these can be done with machine learning. It can even be applied to a politician's tweets - to find out if they’ve been fibbing about where they’ve been, and when.

This flurry of research hearkens back to one of the oldest problems in Computer Science - the Turing Test. Can you detect whether someone at the other end of a text-based connection is a person or a computer? What questions do you ask? How do you analyse their responses? Take those same ideas and apply them to a vendor on Alibaba or an account on Twitter - ask the questions, analyse and probe - then decide: truth or lies.

As Sir Tim Berners-Lee won the ACM A.M. Turing Award last week, the timing of this next evolution of his Web could not be more appropriate. The Web needs to grow a meta-layer of error-checking and truth-telling. That will likely slow things down a bit, even as it helps us feel more assured that the fake can be suppressed.

This will never be as true as we might want it to be. As soon as any system to detect lies goes into widespread deployment, the least honest and most clever will go to work undermining that algorithmic determination of truth, finding its weaknesses, and exploiting them. It was ever thus; over the long term, the search for truth will has always been an act of persistence and dedication.

Machines can help us in this battle - but machines will be used on both sides, deceiving and revealing deceit. Yet there is hope: there’s too much money on the table to allow the forces of darkness to gain ascendancy. Chaos is bad for business.

Any alignment of commerce with the greater good is a rare and potent combination, meaning the resources to fight this battle will be available into the foreseeable future. Those graduate students with their fraud and bot detection algorithms will be snapped up by those giant firms whose profits depend on a Web that is truthful enough for commerce. When it comes to truth, what’s good for Google and Facebook is good for the rest of us. ®|||

