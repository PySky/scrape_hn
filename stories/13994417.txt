"   #Here you have to paste your primary key

Helper function to process the request to Project Oxford

json: Used when processing images from its URL. See API Documentation

data: Used when processing image read from disk. See API Documentation

headers: Used to pass the key information and the data type request

if 'content-length' in response.headers and int(response.headers['content-length']) == 0:

result = response.json() if response.content else None

#Get arguments from the query string sent

#Get rid of preceding u in string

if result is not None:

AlexaSeeIntent what can you see

#Call the Python script that takes the picture

#Call the Python script that analyses the picture.  Strip newlines

#Turn the response into a JSON object we can parse

# Setting reprompt_text to None signifies that we do not want to reprompt

# the user. If the user does not respond or says something that is not

So super, super simple.  Call the API to take the picture, call the API to analyse it, pick out the description and read it out.

Here's the image that was analysed for the Teddy Bear video:

Now to think about what other senses I can give to Alexa...|||

Using a Raspberry Pi and the Microsoft Cognitive API to give Alexa "vision"