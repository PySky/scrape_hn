ROBOTS.TXT é um arquivo que pode ser criado através do bloco de notas e que pode ou não estar presente na pasta raiz de um site.

Ele é utilizado pelos robôs dos buscadores para identificar os conteúdos que devem ser indexados e representados como resultado de uma busca por determinada palavra-chave.

Se ainda tiver dúvidas de como o Google funciona, leia este artigo.

Continue lendo para saber mais sobre:

→ Quais conteúdos podem ser bloqueados no Robots.txt

 → É possível acessar o arquivo ROBOTS.TXT de algum site?

 → Como editar o arquivo Robots.txt?

 → Sintaxe do arquivo ROBOTS.TXT

 → O que é um User-Agent

Não deixe de compartilhar com seus amigos para que cada vez mais as pessoas possam saber o que é o arquivo Robots.txt e como ele funciona.

Você pode querer que alguns conteúdos sejam privados, por exemplo:

Visto que conteúdo repetido não é considerado bem visto para as técnicas de SEO e também pelos indexadores do Google.

Por isso você pode optar para que este conteúdo não seja indexado, por exemplo, nos casos em que você tem várias landing pages similiares.

As páginas de impressão são de certa forma conteúdo duplicado, e quando você passa a entender o que é SEO, percebe que conteúdo duplicado é banido pelos buscadores, o conteúdo deve ser sempre único e exclusivo.

Isso não agregaria valor a sua busca, por isso, é importante configurar as páginas de impressão para que não sejam indexadas pelos mecanismos de busca.

É possível ter acesso ao arquivo ROBOTS.TXT de qualquer site, você só precisa digitar o nome do site /robots.txt. Este conteúdo é de livre acesso.

Por exemplo, é possível também acessar o ROBOTS.TXT alguns sites bem conhecidos no internet:

É claro que este conteúdo é extenso de mais, e seu site não necessariamente será igual  ou similar ao do Google, por exemplo.

Cadastre seu email no campo abaixo para ser o primeiro a receber novas atualizações do site.

Você pode utilizar o bloco de notas para criar o arquivo .txt e posteriormente adiciona-lo a estrutura de pastas no servidor local ou servidor de hospedagem.

A extenção do arquivo deve ser .txt, e não necessariamente você precisa salvar o arquivo como nome.txt porque, o prorprio sistema operacional já o salva.

Para validar se está correto, você pode ir em propriedades do arquivo e em detalhes, você verá que o nome do arquivo já é robots.txt.

Caso você já tenha um site publicado, deverá criar ou editar o arquivo dentro do servidor de publicação.

Para isso, você precisa acessar o painel de controle (ou através do FTP), ir até a pasta raiz do site e editar ou criar um arquivo robots.txt.

A forma mais fácil de saber se ele já existe é fazendo uma pesquisa pelo nome.

Caso não encontre, você então pode criar o arquivo do zero no servidor ou fazer o upload do arquivo existente em um servidor ou computador local.

Alguns sites na internet permitem que você adicione algumas propriedades e geram automaticamente o código.

Dessa forma, você não precisa se preocupar muito com o código em particular.

Vale a pena dar uma olhada se quiser ser mais prático.

Para entender o funcionamento do arquivo ROBOTS.txt, vamos resumi-lo em apenas 4 comandos:

Para bloquear qualquer robô de indexação de conteúdo, você pode utilizar o código abaixo, digitando / na frente do comando disallow:

Para bloquear qualquer robô de indexação de conteúdo, mas ao mesmo tempo, permitir que apenas uma página específica seja indexada, você pode utilizar o código abaixo.

Note que você tem que no comando “allow”, você tem que digitar o nome da página e sua extensão. Caso a página que deseja indexar esteja em uma pasta, você deverá colocar o caminho completo por ex:

Para indexar uma pasta específica, basta apenas permitir o acesso a pasta.

Para bloquear apenas as imagens do site, você primeiro tem que identificar o que realmente deseja bloquear, por ex, se for apenas uma imagem, você pode entrar com o valor:

Lembrando de adicionar também a extensão da imagem, se é jpg, gif, png, etc. Caso ela esteja em uma pasta ou sub-pasta, deverá colocar o caminho todo até ela, por ex:

É possível também utilizar outro modo para bloquear que apenas um tipo de extensão seja bloqueado, por exemplo, apenas imagens do tipo .png e .gif:

Outra forma é utilizar o user-agent para fazer a configuração, por exemplo:

O user-agent  pode ser utilizado para listar quais robôs de indexação devem seguir as regras do arquivo robots.txt.

Por exemplo, caso que você queira que uma determinada regra só seja aplicada para o Google, não pelos demais robôs, assim como Yahoo e Bing, você deve especificar outros valores, por exemplo, começar como:

Neste caso, estará dizendo apenas para o Google que permite  o conteúdo y e proíbe que ele indexe o conteúdo x.

Você pode também utilizar outros user-agent e mesclar sua configuração, por ex:

Neste exemplo acima, está sendo permitida a indexação pelo Google mas negada para todos os outros indexadores.

Há diversos comandos para o User-agent que podem ser utilizados, e para cada indexador há um diferente. Vamos ver alguns exemplos para o Google:

Caso queira saber mais, vou deixar aqui os links com o conteúdo técnico de cada um:

Cadastre seu email no campo abaixo para ser o primeiro a receber novas atualizações do site.|||

O que é o arquivo Robots.txt e como fonfigura-lo. Como bloquear a indexação de um conteúdo para os buscadores.