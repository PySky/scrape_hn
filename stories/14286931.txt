We evaluate our model by detecting multiple events in videos and describing all the events jointly. We refer to this task as Multi-captioning. We test our model on ActivityNet Captions dataset, which was built specifically for this task.

Next, we provide baseline results on two additional tasks that are possible with our model. The first task, localization, tests our proposal model's capability in being able to adequately localize all the events for a given video. The second task, retrieval, tests a variation of our model's ability to retrieval the correct set of sentences given the video or vice versa. Both these tasks are designed to test the event proposal module (localization) and the captioning module (retrieval) individually.

We show examples qualitative results from the variations of our models in the figures on the left. In (a), we see that the last caption in the no context model drifts off topic while the full model utilizes context to generate more reasonable context. However, context isn't always successful at generating better captions; in (b), the middle segment overpowers the neighboring events and causes the full model to repeat the caption for the middle event. Finally, in (c), we see that our full context model is able to use the knowledge that the vegetables are later "mixed in the bowl" to also mention "the bowl" in the third and fourth sentences, propagating context back through to past events.|||

