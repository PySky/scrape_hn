Conventional wisdom says that appending to a file is atomic, up to a certain write size.  That means, for example, that you can have multiple processes write shared a log file and not have the log output crossed.  Recently, I was thinking about this “conventional wisdom” and decided to look for some proof.  After some Googling, and eventually landed on this StackOverflow question which discusses the question at hand.  The accepted answer has a lot of good information, but also uses the words “supposed to be” and “assuming” a little to much to be conclusive.

To settle this issue once and for all, I decided to write a short script to actually test the limit of atomic appends.  The idea is to fork a bunch of worker threads and have them write to the same file.  Each thread will write a unique signature, which we can then verify against corruption.  If user freiheit from StackOverflow is correct, then writes of 4096 bytes on Linux should be OK, and at 4097 bytes we should already see corruption.

The bash script (source at the end of this post) takes one argument – the amount that each worker should write at a time (a.k.a. the buffer size). The results from a CentOS 6.5 box are as follows:

So there you have it, empirical proof that Linux allows atomic appends of 4KB.

Luckily, we can use cygwin on Windows to run the same experimental script.  With some trial and error we can then determine that the maximum atomic write size is 1024 bytes:

If you’re writing cross-platform Windows/Linux code, and need to output to a shared file, make sure to keep your writes to 1KB or less.

#!/bin/bash ############################################################################# # # This script aims to test/prove that you can append to a single file from # multiple processes with buffers up to a certain size, without causing one # process' output to corrupt the other's. # # The script takes one parameter, the length of the buffer. It then creates # 20 worker processes which each write 50 lines of the specified buffer # size to the same file. When all processes are done outputting, it tests # the output file to ensure it is in the correct format. # ############################################################################# NUM_WORKERS=20 LINES_PER_WORKER=50 OUTPUT_FILE=/tmp/out.tmp # each worker will output $LINES_PER_WORKER lines to the output file run_worker() { worker_num=$1 buf_len=$2 # Each line will be a specific character, multiplied by the line length. # The character changes based on the worker number. filler_len=$((${buf_len}-1)) # -1 -> leave room for 

 filler_char=$(printf \\$(printf '%03o' $(($worker_num+64)))) line=`for i in $(seq 1 $filler_len);do echo -n $filler_char;done` for i in $(seq 1 $LINES_PER_WORKER) do echo $line >> $OUTPUT_FILE done } if [ "$1" = "worker" ]; then run_worker $2 $3 exit fi buf_len=$1 if [ "$buf_len" = "" ]; then echo "Buffer length not specified, defaulting to 4096" buf_len=4096 fi rm -f $OUTPUT_FILE echo Launching $NUM_WORKERS worker processes for i in $(seq 1 $NUM_WORKERS) do $0 worker $i $buf_len & pids[$i]=${!} done echo Each line will be $buf_len characters long echo Waiting for processes to exit for i in $(seq 1 $NUM_WORKERS) do wait ${pids[$i]} done # Now we want to test the output file. Each line should be the same letter # repeated buf_len-1 times (remember the 

 takes up one byte). If we had # workers writing over eachother's lines, then there will be mixed characters # and/or longer/shorter lines. echo Testing output file # Make sure the file is the right size (ensures processes didn't write over # eachother's lines) expected_file_size=$(($NUM_WORKERS * $LINES_PER_WORKER * $buf_len)) actual_file_size=`cat $OUTPUT_FILE | wc -c` if [ "$expected_file_size" -ne "$actual_file_size" ]; then echo Expected file size of $expected_file_size, but got $actual_file_size else # File size is OK, test the actual content # Only use newer versions of grep because older ones are way too slow with # backreferences [[ $(grep --version) =~ [^[:digit:]]*([[:digit:]]+)\.([[:digit:]]+) ]] grep_ver="${BASH_REMATCH[1]}${BASH_REMATCH[2]}" if [ "$grep_ver" -ge "216" ]; then num_lines=$(grep -v "^\(.\)\1\{$((${buf_len}-2))\}$" $OUTPUT_FILE | wc -l) else # Scan line by line in bash, which isn't that speedy, but is good enough # Note: Doesn't work on cygwin for lines < 255 line_length=$((${buf_len}-1)) num_lines=0 for line in `cat $OUTPUT_FILE` do if ! [[ $line =~ ^${line:0:1}{$line_length}$ ]]; then num_lines=$(($num_lines+1)) fi; echo -n . done echo fi if [ "$num_lines" -gt "0" ]; then echo "Found $num_lines instances of corrupted lines" else echo "All's good! The output file had no corrupted lines. $size" fi fi rm -f $OUTPUT_FILE # This script aims to test/prove that you can append to a single file from # multiple processes with buffers up to a certain size, without causing one # process' output to corrupt the other's. # The script takes one parameter, the length of the buffer. It then creates # 20 worker processes which each write 50 lines of the specified buffer # size to the same file. When all processes are done outputting, it tests # the output file to ensure it is in the correct format. # each worker will output $LINES_PER_WORKER lines to the output file # Each line will be a specific character, multiplied by the line length. # The character changes based on the worker number. "Buffer length not specified, defaulting to 4096" # Now we want to test the output file. Each line should be the same letter # repeated buf_len-1 times (remember the 

 takes up one byte). If we had # workers writing over eachother's lines, then there will be mixed characters # Make sure the file is the right size (ensures processes didn't write over # File size is OK, test the actual content # Only use newer versions of grep because older ones are way too slow with # Scan line by line in bash, which isn't that speedy, but is good enough "All's good! The output file had no corrupted lines. $size"

Update 11/12/2014: Updated the script to check the resulting file size as per glandium‘s suggestion in the comments.|||

