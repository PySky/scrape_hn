Recursion allow Neural Programs to devide and conquer a problem, and generalize indefinitely. By devide and conquer, each neural program component has a much reduced domain, making interpretations easier and training faster.

This paper builds on top of Reed & de Freitas' 2016 work on Neural Program-Interpreter (NPI), and uses the same architecture to show that recursion can fundamentally solve the generalization problem that plagues neural programs, while also providing clear path for provability. The authors argue that poor generalization performance of prior work is an important problem. Future neural programs need to be able to generalize well, from limited set of training data, and learn quickly. This is especially important (and difficult) considering that the space for potential solution is large.

The training process distinguishes this work from most of prior NP literature and is the same as Reed & de Freitas. Trainings sets is very small, and for the verification section it was further reduced to make a point.

Recursion allows the authors to verify the validity of the neural programs by:

Importance of recursion is also demonstrated by implementing with partially recursive networks. Performance contrast is stunning (ref. Table 1, 2, and 3, test are all done on 30 randomly selected DAGs. Table 1. reproduced here for bubble sort.)

Section 3.2 and 3.3 details the recursive formulations for these problems and the general approach to provable generalization. Seciton 4 covers the experiment result and offers the proofs.

We need a way to solve all these problems.|||

deep_learning_papers_TLDR - repository for my TLDR for deep learning papers (and SML papers!)