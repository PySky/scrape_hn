The quantum device used in our experiment consists of five superconducting transmon qubits, A, D , …, D , and seven microwave resonators (Fig. 1c). Five of the resonators are used for individual control and readout of the qubits, to which they are dispersively coupled.13 The center qubit A plays the role of the result and is coupled to the data register {D } via the remaining two resonators. This coupling allows the implementation of cross-resonance (CR) gates14 between A (used as control qubit) and each D (target), constituting the primitive two-qubit operation for the circuit in Fig. 1b (full gate decomposition in the Supplementary Information). Each qubit is measured by probing its respective readout resonator with a near-resonant microwave pulse. The output signals are then demodulated and integrated at room temperature to produce the homodyne voltages (see Supplementary Information for the detailed experimental setup).

To implement a uniform random example oracle for a particular k, we first prepare the data qubits in a uniform superposition (Fig. 1b). Preparing such a state ensures that all parity examples are produced with equal probability and is also key in generating a quantum advantage. We then implement the oracle as a series of CNOT gates, each having the same target qubit A and a different control qubit D for each k  = 1. Finally, the state of all qubits is read out (with the optional insertion of Hadamard gates, see discussion below). The oracle mapping to the device is limited by imperfections in the two-qubit gates, with average fidelities 88–94%, characterized by randomised benchmarking15 (see Supplementary Table S1). Readout errors in the register , defined as the average probability of assigning a qubit to the wrong state, are limited to 20–40% by the onset of inter-qubit crosstalk at higher measurement power (see data in the Supplementary Information). A Josephson parametric amplifier16 in front of the amplification chain of A suppresses its low-power readout error to η  = 5%.

Having implemented parity functions with quantum hardware, we now proceed to interrogate an oracle N times and assess our capability to learn the corresponding k. We start with oracles with register size n = 2, involving D , D , and A. We consider two classes of learning strategies, classical (C) and quantum (Q). In C, we perform a projective measurement of all qubits right after execution of the oracle. This operation destroys any coherence in the oracle output state, thus making any analysis of the result classical. The measured homodyne voltages are converted into binary outcomes, using a calibrated set of thresholds (see Methods). Thus, for every query, we obtain a binary string {a,d ,d }, where each bit is 0 (1) for the corresponding qubit detected in |0〉 (|1〉). Ideally, a is the linear combination of d ,d expressed by the string k (Fig. 1a). However, both the gates comprising the oracle and qubit readout are prone to errors (see values in the Supplementary Information). To find the k that is most likely to have produced our observations, at each query m we compute the expected ã  = d ·k mod 2 for the measured D = {d ,d } and the 4 possible values of k. We then select the k which minimizes the Hamming distance to the measured results a ,…,a of N queries, i.e., .7 In the case of a tie, k is randomly chosen among those producing the minimum distance. As expected, the error probability p of obtaining the correct answer decreases with N (Fig. 2a). Interestingly, the difficulty of the problem depends on k and increases with the number of k  = 1. This can be intuitively understood as needing to establish a higher correlation between data qubits when the weight of k increases.

Our second approach (Q) takes advantage of the quantum correlations between ancilla and data qubits at the output of the oracle. Instead of directly measuring the qubits as above, we first apply a Hadamard gate on each. These local operations generate quantum interference between terms in the superposition state, ideally producing the desired result (Eq. (1)). This technique is widely used in quantum algorithms to increase the probability of obtaining the desired outcomes.17 In this case, whenever A is measured to be in |1〉 (with 50% probability), the data register will ideally be projected onto the solution, |D ,D 〉 = |k ,k 〉. We therefore digitize and postselect our results on the outcomes where a = 1 and perform a bit-wise majority vote on . Despite every individual query being subject to errors, the majority vote is effective in determining k (Fig. 2b). We assess the performance of the two solvers by comparing the number of queries N required to reach p = 0.01 (Fig. 2c). Whereas Q performs comparably or worse than C for k = 00, 01 or 10, Q requires less than half as many queries as C for the hardest oracle, k = 11. We note that, while these results are specific to the lowest oracle and readout errors we can achieve, a systematic advantage of quantum over classical learning will become clear in the following.

So far we have adhered to a literal implementation of the classical LPN problem, where each output can only be either 0 or 1. However, the actual measurement results are the continuous homodyne voltages , each having mean and variance determined by the probed qubit state and by the measurement efficiency.13 This additional resource can be exploited to improve the learner’s capabilities. A more effective strategy for C uses Bayesian estimation to calculate the probability of any possible k for the measured output voltages, and select the most probable (see Methods). This approach is expensive in classical processing time (scaling exponentially with n), but drastically reduces the error probability , averaged over all k, at any N (Fig. 3). To improve on Q, we still postselect the oracle queries on the digitized outcome a = 1. Then, instead of digitizing the corresponding as above, we digitize their averages , obtaining our best guess for k (see Methods). This procedure simply replaces the majority vote between multiple noisy observations with a single observation, with variance reduced by the number of postselected queries. Using the analog results, not only does Q retain an advantage over C (smaller p for given N), but it does so without introducing an overhead in classical processing.

The superiority of Q over C becomes even more evident when the oracle size n grows from 2 to 3 data qubits (Fig. 3b). Whereas Q solutions are marginally affected, the best C solver demands almost an order of magnitude higher N to achieve a target error. Maximizing the resources available in our quantum hardware, we observe an even larger gap for oracles with n = 4 (data in the Supplementary Information), suggesting a continued increase of quantum advantage with the problem size.

As predicted, quantum parity learning surpasses classical learning in the presence of noise. To investigate the impact of noise on learning, we introduce additional readout error on either A or on all D . This can be easily done by tuning the amplitude of the readout pulses, effectively decreasing the signal-to-noise ratio.18 When the ancilla assignment error probability η grows (Fig. 4a), the number of queries (the average of N over all k) required by the C solver increases by up to 2 orders of magnitude in the measured range (see also data in the Supplementary Information). Conversely, using Q, only changes by a factor of ~3. Key to this performance gap is the optimization of the digitization threshold for at each value of η (see Methods). When η is increased, an interesting question is whether postselection on V remains always beneficial. In fact, for η  > 0.25, it becomes more convenient to ignore V and use the totality of the queries (Q′ in Fig. 4a).

Similarly, we step the readout error of the data qubits, with average η , while setting η to the minimum. Not only does Q outperform C at every step, but the gap widens with increasing η .

The computational advantage of quantum learning, which appears in the reduction of the number of oracle calls, is even more significant when accounting for the post-processing time. For example, finding k at η  = 0.44 with 1% error (Fig. 4a) takes C about 10 times longer in post-processing relative to Q. Moreover, the processing time for C grows exponentially with n, as the Bayesian solver must track probabilities for each possible k. Conversely, Q consists only of binary comparisons (for postselection on A), averages, and a final digitization (for D), thus scaling linearly with n.

Finally, to verify that our results are not limited to highly noisy systems, we have implemented all 4-bit k on a second device with lower gate and readout errors, particularly for . Whereas the required number of queries is greatly reduced for both learners, the performance gap remains in favor of Q (see Supplementary Information).|||

Article