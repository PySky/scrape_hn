The basic use of dbcrawler is to generate sql dump statements from tables in your database. It crawls the database using the information provided by the user.

User of the database which has permission to access the database. eg root,app etc.

Address where database is accessible. eg. localhost, 127.0.0.1 etc.

Database which is to be crawled . Eg sakila database in default example of this module. This sakila database can be downloaded from [here]: https://dev.mysql.com/doc/index-other.html

Constraints are just join conditions . Eg . The format acceptible is table1.column1 = table2.column2 . What this means is that use values of column2 of table2 to retrieve values from table1.

Seed means the initial data that has to be given to crawler to start the crawl. For eg. . Each seed consists of . Each two seeds are seperated from each other using as given in example above. Above seed will start from 2 points first from actor table using actor_id values 3,4,5 and from film_actor table using actor_id 5.

instead of passing constraint from commandline they can also be exported from json file. the format of the json will be like

Here list of constraints are kept as 'constraints' object inside main object. So from commandline point of view each constraint is of the form

This is the path of the file where all the dump statements are to be written.|||

dbcrawler - crawls database and makes insert statement for all the tables so that some data can be taken from all tables