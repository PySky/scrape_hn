The AI winter will come soon, since we have all the same hype we had in 1987. This hype is really damaging for AI. The domains in which AI is excelling today where never though of as impossible. In 1959 the Perceptrons where thought to do be able to do exactly what the convnets of today are able to do, and yet it was a formal problem (the group invariance theorem by Minsky and Papert) which destroyed the enterprise.

Today we have neural nets that can solve the XOR, but not the PARITY, and yet no one seems to mind it since it is a formal problem, not related to "real world applications". And it is not, and it also old news. But the moral of the story is different.

AI winters happen when AI is over-hyped and fails to deliver. It is a question of magic of a new technology giving way to its full understanding and seeing that it is a tool with limitations, not an enchanted artifact. But, once the winter sets in psychologically and rationally, a post facto rationalization is found in scientific results (much like the XOR of old).

Deep learning today is a collection of great classifiers, but they are no more than classifiers. AI need inference to work properly, and the problem might be the perception of the success in Machine Translation. Machine translation is great now, but it was a problem that was considered AI-complete. I believe MT is not AI complete, since it is possible for adequately long fragments to be translated without the need for inference. If we look at all possible sentences and "perfect" MT in every case, then it is AI complete, but as the old saying goes "the plural of cases is not proof" (and as Dilbert would paraphrase it, "the plural of anecdotes is not data"). So people see good MT on certain (usually very easy) documents and think of it as a solved problem.

If you want a popular AI topic which does require a lot of inference, think of a general-purpose chatbot. Suppose you talk to it and you say "John punched his boss in the face", a human could say "OMG, he has three kids, how is he going to feed them now?". For this answer you would have to know that punching one's boss usually means getting fired and sometimes going to prison, and that both of these outcomes prevents one from feeding his kids in a different sense, but in this context it is irrelevant. Moreover, compare this with "John stole 5000 bitcoins and was convicted", which does not entail that he will not be able to "feed his kids" in the same sense as the above example.

And mind you, this is not the kind of inference of a mathematical proof, it is beyond any doubt an instance of "common-sense inference".

Another problem for DL is that is not even a good classifier for some decision problems. Think of us wanting to have a DL-based early shutdown system in a nuclear power plant. We cannot really train it or use reinforcement learning since we actually want to avoid having "training cases" in the find-the-cat-in-the-video sense. Maybe you think this is farfetched, but actually the same goes for self driving cars. We do not want to have enough training examples for some activities the car would need to do such as planning, contingency management and so on. The moral of the story is not that DL is not good -- quite the opposite -- it is a great and fascinating tool (and a great AI approach), and a tool to which one could devote his life work to study it in detail, but so far it is not an AI system and there are severe and interesting challenges that need to be surmounted just to fulfill the promises we made so far. An we are promising more and more every day... if that is not the making of an AI winter, I do not know what is. And AI winters have been profoundly damaging to the field, so please curb your enthusiasm.

Currently, DL has no complex inference capability, and the problems we actually want AI to control are sometimes not trainable. Think of an AI system to control nuclear power plants...do we want it to learn over 50000 cases labelled with 0 and 1? Human intelligence learns trivial things (using the trial and error of ML), but also thinks and infers when we deal with important things. It is interesting to see a DL inference, but so far, we have not yet seen it in its full form.|||

