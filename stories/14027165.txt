Having confidence in your research and development environment is essential if you want to solve challenging problems. This post shows how to setup containers for deep learning, have accelerated and finally speculates about hosting in the cloud vs. on-premise.

To give you a bit of background, at source{d}, the ML team is running constant experiments with Python scripts and Jupyter notebooks which extensively use CUDA + NVIDIA GPUs. For example:

We decided to share some of our learnings and configuration files.

Typically, deep learning researchers run their stuff using Ubuntu as the host OS. If you want to scale or reproduce your work with the standard Ubuntu setup, you are left with these three options:

There is a modern, reliable way to solve the reproducibility problem: use containers, Luke. We can run something Spartan like Container Linux by CoreOS as the host OS and give researchers access to containers with their beloved Ubuntu. The containers are not persistent, and there is no need in messing with at all - instead, everybody can safely torture the OS in the container any way they want. All the instances of the container are the same initially, contain the same proper compiled and configured libraries, same tools and same access to the persistent disk storage. As a bonus, it becomes super easy to deploy the developed machine learning models using the same containers they were created in.

However, one needs to solve some technical issues to follow this path. The first step and most important one is passing CUDA devices inside the container. There is a solution from NVIDIA: nvidia-docker. In our opinion it is not the right approach. The drawbacks are:

Our solution is different. We take an intermediate container, compile and install the DKMS driver there, it. Shut down the container. Since the kernel is shared, the devices remain alive. We launch the payload containers with the needed userspace and mapped NVIDIA devices afterwards.

This is the for the intermediate container named . It is inspired by Mike Orzel and Luke Benson.

We are using the ONBUILD trigger here to lazily update the child containers. As you see, some versions are hardcoded which is unfortunate. This is the payload aka for one of our R&D machines (full file later in the post):

Our hardware setup has two GPUs which were inserted in the proper slots to enable peer to peer memory exchange. We initially hit a nasty bug with Intel IOMMU. Particularly, when we invoked , it hanged and showed

The container ran with so the problem was not with the permissions. We applied the workaround described in the bug report: add to the kernel boot arguments. It solved the problem and peer-to-peer GPU memory access started to work.

Finally, this is how we run the container:

We are using Python for our research. It is critical for us to achieve the best performance of and packages. The following explains how we compile and to link them to BLAS implementation in Intel Math Kernel Library (free for commercial usage) and dynamically override MKL with nvBLAS to get CUDA-accelerated BLAS. nvBLAS is not a complete BLAS implementation so we have to fallback to MKL.

We run to install MKL system wide without the need for hacking . However, we do need to hack to insert nvBLAS before MKL is loaded.

There are some additional perks:

The from above is the result of incremental improvements in container over the last 6 months. It builds in less than 15 minutes and gives a machine learning engineer and perfect control of the environment.

There is a number of gotchas for the users without any prior experience with Container Linux. I am putting the actual slides from the presentation to illustrate.

The answer to this question depends on the use case and may change from time to time depending also on cloud prices. Every option has it’s pros and cons: Cloud based hosting simplifies devops, is easier to monitor and more flexible. Your own hardware with GPU cards is more expensive at the beginning, but quickly pays off - according to our calculations, within one year. Anyway, here are two excellent links which can help you to make the decision:

Back in October 2016 our choice was to install custom hardware and we haven’t regretted it yet.

“science-3” GPU tower at source{d}. We use it for deep learning and other experiments.

NVIDIA dockerization was bravely performed by our VP of Engineering, Maximo Cuadros. I learned a lot from him about containers. Next time you see Maximo on our tech talks, ask for some wisdom!|||

Building the first AI that understands code