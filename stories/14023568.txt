Last time, I created a simple linear model using three ranking signals. Using TMDB movie data, I came up with a naive model that computed a relevance score from a title field’s TF*IDF score, an overview field’s TF*IDF score, and the user rating of the movie. Our model learned the weight to apply to each score when summing – similar to the boosts you apply when doing manual relevance tuning.

What I didn’t tell you was I was using Elasticsearch to compute those ranking signals. This blog post I want to take the simple model we derived and make it usable via the Elasticsearch Learning to Rank plugin. In a future blog post, we’ll load the same model into Solr. This will give you a chance to see a very simple 101 model in action with the two search engine’s learning to rank plugins.

If you recall, learning to rank learns a ranking function as a function of ranking-time signals. Classically these are referred to as “features” when discussing machine learning models. But I like to use signals to denote that they’re signaling us something about the relationship between a query and document. Plus, selfishly, it’s what we call ranking-time information in Relevant Search to differentiate between the features that exist purely on content or derived from queries.

In this blog post, we’ll use the Python Elasticsearch client library, but mostly I’ll just be showing off the basic queries I use to derive the signals. I’ve already loaded the TMDB movie data locally, if you’d like to have this data at your disposal follow the directions in the Learning to Rank demo README

Onto the action. Below, you’ll see our three queries we use to generate the signal values: , , and . The first two are straight-forward match queries. The latter is a function score query that just returns a movie’s rating which has no relationship to the search keywords.

If you recall, these three features were gathered for a set of judgments. Judgments let us know how relevant a document is for a query. So Rambo is a “4” (exact match) for the keyword search “rambo.” Conversely “Rocky and Bullwininkle” is a 0 (not at all relevant) for a “Rambo” query. With enough judgments, we logged the relevance scores of the above queries for the documents that were judged. This gave us a training set that looked like:

In that blog post, we used to run linear regression to learn which signals best predicted the resulting relevance grade. We came up with a model with a weight for each and a y-intercept. This model was:

The Elasticsearch learning to rank plugin uses a scripting format known as to encode models. Following the documentation for the ranklib scripting language we know we can encode a linear model that looks like:

So in Python code, we can format our model above in that format:

Following the documentation for the Learning to Rank Plugin we can upload this model as a ranklib script, and give it a name.

Elasticsearch has acknowledged our upload. Great! Now we should be able to execute a simple query!

You almost always want to run a learning to rank model in a rescore query, but the TMDB data set isn’t huge. We can use it directly with only a few hundred milliseconds to evaluate over the whole corpus. This is fun, because it let’s us informally evaluate how well our model is doing.

To query with the model, we create a function that runs the ltr query. Remember the model we built computes a relevance score for a document from three inputs that relate to the query and document:

To compute the first two, we need to run the and above for our current keywords. So we need to pass our model a version of these queries with the current keywords. That’s what happens first in the function below. We inject our keywords into the inputs that are query-dependent. Then we add our that’s only document dependent.

These three queries are scored per document are then fed into the linear model. Remember last time this model is simple: each coefficient is just a weight on each signal’s score. The model is simply a weighted sum of the scores of , , and using coefficients as the weight!

With this function in place, let’s run some searches! First a simple title search:

Hey, not too shabby! How long will our luck go, let’s try another one:

Wow lucky again. It’s almost like these aren’t lucky guesses but rather a prescient author is selecting examples that they know will look good! Ok, now let’s try something closer to home: a Stallone Movie:

Err, not bad, but a bit off the mark… Well this is actually a case, like we talked about before, involving the nuance that the linear model can fail to capture. The linear model just knows “more title score is good!”. But in this case, First Blood should be closer to the top. It was the original Rambo movie! Moreover Rambo III shouldn’t really before just Rambo.

Still, not bad for 40 odd examples of training data!

Let’s try something where we don’t know the title. Like an example from Relevant Search . Here we’re grasping at straws. Hoping “Space Jam”, a movie where Michael Jordan saves the world by playing aliens at basketball, comes up first:

Sadly, Not even close! To be fair, we didn’t show our training process examples of this use case. Most of our examples were direct, known-title navigational searches. This just goes to show you how it’s important to get a broad set of representative samples across how your users search for learning to rank to work well.

It also continues to demonstrate how the linear model struggles with nuance. Other models like gradient boosting (ala LambdaMART) can grok nuance faster, and aren’t constrained to our boring linear definitions of functions. We’ll see how these models work in future blog posts.

One of my colleagues will be taking Solr learning to rank out for a test spin for you. A simple linear model is very easy to understand, so it’s fun for these little test-spins.

I’d love to hear from you. If you’d like our help evaluating a learning to rank solution for your business, please get in touch! And I’m always eager for feedback on these posts. Please let me know if I can learn a few things from you!

This blog post was created with Jupyter Notebook: View the source!|||

This blog post let’s the simple model we derived in a previous post and make it usable via the Elasticsearch Learning to Rank plugin.