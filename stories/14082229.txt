With our internal teams set, we all got to work. The output from each team is described briefly below:

With the help of Udacity and a member of the open source community (acknowledgements at the end), within the first week we were able to put out a fairly realistic simulator in Unity using satellite images and elevation data that Kairos Aerospace had collected at the 2016 Self Racing Cars event. A video of the Kia Soul vehicle driving autonomously in the simulator is shown below. After the initial release, we continuously added features to the simulator throughout the six weeks. This simulator proved to be an incredible testing ground for our models and gave significant insight into what worked and what didn’t. Since we didn’t have the real vehicle until the days leading up to the event, this was a critical step.

Data Collection — The goal of our PolySync team was several-fold. First, we needed to extract data that PolySync had taken from the 2016 race in order to train our deep learning models. This process also set the stage for the data processing pipeline that we created for quickly processing data at the race. Getting the PolySync software up and running took several weeks of attempts, but the team at PolySync was extremely helpful. We extracted around 30 minutes of data from the 2016 event. and a snippet of that data can be seen in the video below.

Once we had the 2016 data collected, we worked to create a data pipeline that would allow us to quickly create .CSVs including all the necessary training data and corresponding image directories. This proved to be one of the most challenging tasks that we faced. With around 10 different team members working on end-to-end models, the format of the training data had to fit with everyone’s approaches. We also found out that much of the data that we collected from 2016, and also at the Thunderhill track, did not have throttle information. Our team quickly worked to create a modified “acceleration” variable that would mimic the vehicle throttling. After working with PolySync, we fixed the throttle issue and our final datasets all contained throttle information.

A pleasant surprise during the days at the race was the accuracy of the GPS data that we collected. One of the team members created visualizations of the training runs, and when you zoom in you can actually separate each lap around the track. An image of the GPS overlaid on the track is shown below.

System Architecture — The second goal was to develop an interface between our models and PolySync. Images and other information from the vehicle would be passed through PolySync to our models and then our models could send back steering, throttle, and brake commands through PolySync to the OSCC. The PolySync software is written in C++, but our team was able to create a Python wrapper that allowed communication from our Python-developed models. This created the most flexibility for the modeling team that was developing deep learning models using a combination of Keras and Tensorflow. Our system architecture for communicating with PolySync is shown below:

While these originally started out as separate teams, they quickly merged as we started to grasp the difficulty of getting our models to control all three key vehicle commands using only image inputs. While we tried many things, our developed models could mostly be categorized into three different bins as described below. The general approach was to first train these models on simulator data to the point where they could drive successfully around the simulator track. We then trained, validated, and tested the same models with real-world data from the vehicle, or used combinations of simulator and real-world data for the final models.

Convolutional Neural Networks (CNNs) can be used to autonomously drive vehicles by training the model with image inputs and corresponding steering, throttle, and brake targets. NVIDIA recently released a paper that demonstrated a successful implementation of this methodology for steering a vehicle. Udacity has also made this one of their projects for the Self-Driving Car Nanodegree program, and so many of us on the team had recently implemented this approach to drive a vehicle in Udacity’s Unity simulator. For those interested in the details, I wrote another post about the CNN implementation used for the Udacity project here.

The general idea is that the CNN architecture is able to determine patterns within the image pixels (lane lines, road/offroad) that correlate to certain vehicle controls. One of the key tricks with these models is that you can’t just train the models with the center camera images. If the model only sees training data driving down the center of the road, as soon you deviate slightly from this trained path, the model will see images that are different than those that it was trained on and will steer off-course. Our team used on the order of ~10 different types of image augmentations that modify the training images (and corresponding targets) so that our models are trained to recover from these types of situations. Another key trick is that the Thunderhill track data is biased to left turns as it makes a counter-clockwise loop. Flipping the images and corresponding steering targets is a requirement to reduce this bias. The CNN’s can also take in other inputs such as GPS data or vehicle speed, and we experimented with many different combinations of inputs and outputs. In general our CNN layer architectures were similar to the NVIDIA approach, as they proved to drive well in the simulator and have acceptable computational latency when running on the real vehicle. Descriptions and the code for many of the methods that we implemented can be found here.

The approach for Recurrent Neural Network’s (RNN’s) is similar to that of CNN’s, but instead of taking in single images/targets (single moment in time) for training, the model is trained on sequences of images and targets. For vision, instead of using a 2D convolution over the input images, the model now uses a 3D convolution with an extra depth dimension in time. The output from this 3D convolution is then passed to multiple stateful RNN layers. For our approach we used stateful Long Short-Term Memory (LSTM) and vanilla RNN upper layers. We based our approach on the methods developed by Ilia Edrenkin, a software engineer from Germany, who won the $10k 1st place prize in Udacity’s open source deep learning competition for vehicle steering. Any image augmentations must be implemented differently for the RNN models since it relies on seeing sequences of images.

After trying the CNN and RNN methods described above in the Kia Soul, we were making the occasional turn on the track, but were struggling to get something more promising. We were able to bounce our approaches off of George Hotz from comma.ai, and one idea that he mentioned during the conversation was a PoseNet-style approach. One team member was able to quickly implement a modified version of the PoseNet concept and all of a sudden we were making it around several turns on the track at ~25mph.

A PoseNet uses CNN’s for 6-DOF re-localization. In our approach the six dimensional GPS was replaced by a one dimensional value counting upwards from -1 to 1 from the start to the finish line. A detailed description of our methodology, including the code, can be found here. The architecture is shown below.

A possible improvement would be to not only predict the steering angle, but allow the network to do path planning.|||

Back in 2005, Sebastian Thrun and the Stanford Racing Team won the DARPA “Grand Challenge” by autonomously completing a 150-mile route through the Mojave Desert with their car “Stanley” in the…