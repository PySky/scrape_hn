Containers running on orchestration platforms like Kubernetes, Docker Swarm, DC/OS et al. offer powerful, versatile ways to deploy applications. Containers let you deploy isolated application instances, and you can launch multiple such instances to scale up your load serving capacity. You don’t even need to worry about individual server capacities and scheduling thanks to orchestration tool, which provide declarative deployments.

However, data analytics is still seemingly difficult to achieve with the cloud-native strategy. This is mainly because data analytics involves large scale processing that is tightly coupled to large amounts of data. A single container can’t possibly mount all the data and if there are multiple containers working on the data, it’s difficult to track data being processed.

Pachyderm is an open source framework that enables distributed data versioning and data pipelining via containers. We worked closely with the Pachyderm team, and are very happy to announce that Minio is now integrated as a backend data store for Pachyderm. In this post, we’ll share how to setup Pachyderm to create containerized data analytics backed by Minio cloud storage. But before that, let me explain why this is a good idea to begin with!

Minio is versatile enough to serve as the backend for both the application stack and data analytics platform. This way, you can plug your application to store unstructured data to Minio, while also running analytics on some of the Minio buckets parallely.

Minio is cloud-native, S3 compatible, robust, scalable and can be deployed anywhere — on premise or on cloud.

Pachyderm enables version controlled data and containerized processing. While the data may live in Minio, Pachyderm automatically assigns slices of data to different containers for parallel processing — helping you make the best use of container technology for data analysis.

To begin with, you’ll need Minio up and running. You can either install Minio on a cloud server or on premises. Checkout the details here. Once you have Minio ready, create a bucket (to be used as data source for Pachyderm) and keep the accessKey, secretKey, bucketName and the endPoint handy. We’ll need these while deploying Pachyderm.

Next step is to deploy Pachyderm. Note that Pachyderm needs Kubernetes and the command line utility as prerequisites.

Kubernetes is used to orchestrate your data processing via containers . You can deploy Kubernetes locally via Minikube or on cloud providers like AWS/GCP. enables interaction with the Pachyderm application running on Kubernetes. You can install by

Finally, you can deploy Pachyderm. Assuming you are deploying in one of the common cloud providers, you have Kubernetes running, and you have created a persistent disk on the respective cloud provider (used to store various metadata), execute the following command to deploy Pachyderm backed by Minio:

Note that, can be , or . You can even deploy Pachyderm on-premise. Refer this doc for on-premise deployment.

After a couple of minutes, Pachyderm will be up and running! Then forward the port to the running Pachyderm cluster, so can talk to the Pachyderm deployment.

Test if the communication is working fine by

You could also configure other object stores with Pachyderm. Follow this document for more information.

Now that you have the setup ready, lets see how you actually create data analysis pipelines with Pachyderm and Minio.

Add data to Pachyderm: Whenever you add data to Pachyderm, it backs it up to a Minio bucket (the one provided during Pachyderm deployment). But, it’s not simple backup, what happens is that, the data is version controlled by Pachyderm using a Git like system for data. This way any data manipulation is encapsulated via a commit. Your team members can reproduce your setup just by referring to a commit. They can even revert to old states if needed.

Pachyderm offers several ways to add data to a Pachyderm repo. Read more about it here.

Creating analysis pipelines: Pachyderm also allows you to create DAG pipelines, and it can shard your large data into parallel containers. Each container automatically has access to the data at . Data analysis code running in a container can simply access the data at and write the output at . Again Pachyderm takes care that the data output by each container is segregated and ends at proper place. This makes data analysis scalable and easy to manage.

Read more about creating data analysis using Pachyderm here.

In this post we learned how Minio and Pachyderm integration helps you setup a scalable and reliable data analytics pipeline. We saw that Pachyderm takes care of sharding your data across data processing containers, while Minio serves as the cloud-native, reliable, scalable backend for the processed and unprocessed data.

Since both Minio and Pachyderm are cloud native applications, and can run on orchestration tools like Kubernetes — there is not much Ops effort involved.|||

Containers running on orchestration platforms like Kubernetes, Docker Swarm, DC/OS et al. offer powerful, versatile ways to deploy applications. Containers let you deploy isolated application…