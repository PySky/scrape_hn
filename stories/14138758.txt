This article presents no new research or novel innovations, but rather paints a picture of what we could achieve with the proper technology.

Historically, humans have found computers useful because they were able to abstract problems in a specific way as to offload the more tedious aspects of problem solving onto the machines. Humans are then able to redistribute their efforts where they can truly make a larger impact. They do tasks that are not easy for machines to do automatically.

However, the capabilities of machines are not standing still. Mathematics, manufacturing, pricing, delivery / pick-up logistics, searching, and stock trading are a few examples that stick out to me. Each of these applications of computing power have (at least) one thing in common: they are all receiving problems as input and providing answers as output.

But, in my mind, it seems that there are two main methods by which computers have proven that they can solve problems: by bringing us to a pre-existing answer, or calculating a new answer given the inputs.

Google is the poster child for bringing answers to problem-laden humans. It combs through trillions of web pages and does buckets of interesting processing to bring you pages that seem to contain answers to the present question. This is currently really helpful for humans who need to quickly find answers to only the present problem. It is also a helpful abstraction for the programmers. Instead of trying to answer all the problems themselves (much as WolframAlpha does), Google only attempts to bring the answers that already exist on the internet to any given problem that a user supplies to said user’s attention.

This method of solving problems is in direct contrast with some of the other areas of computing that aim to calculate an optimal state for some large set of variables. As an obvious example, stock trading bots don’t need a link to article about how the stock market works, it needs to calculate a prediction for a number of stock’s performance of the next interval of time.

As previously mentioned in our foray into Google’s usefulness, the internet is currently estimated to have well over 60 trillion pages. That is more information than any human could ever hope to read or understand. It’s a testament to our achievement as humans in our mastery over information. We currently have and create more data now than ever before in the history of mankind. At last count (March, 2017), there were 300 hours of video being uploaded to YouTube every minute. Each minute, 350,000 tweets are sent (Aug 2013). Each year in the US, there are 600k-1M books published. These numbers are both meant to scare you and demonstrate that information is being created faster than humans can consume it.

With this massive influx of new data, how could we possibly hope to ever stay on top of it all? How can we possibly hope to continue to parse these ever increasing quantities of data? The answer, I would think, is that without new forms of technology meant to drive our ability to consume information to higher levels, we cannot. I feel that I am currently riding the line between informed and information overload. I don’t see myself ever pushing past the current level of 45–60 emails per day that require my attention.

We need new tools that sift out the fluff and provide us with minimized versions of the information we are trying to learn and understand.

There are some tools that are beginning to crop up that are meant extend the limits of human data consumption. It is proving to be an especially difficult problem that is being tackled from several different directions.

One of the more interesting vectors of attack in this problem space is actually changing the method by which data is consumed. Rather than creating summaries of datasets, or bubbling relevant data to the top, tools in this category are trying to augment natural human consumption abilities by offering a paradigm shift. Spritz is an interesting example of this type of tool. For the uninitiated, Spritz is a speed-reading utility can help you read at speeds up to 1000 wpm (although people generally don’t). It aligns words around a center locus and then quickly flashes the words at any speed you specify.

The most apparent method to increase the amount of data that humans can take in, is to reduce the amount of data that humans are required to take in. The now defunct Summly is an interesting example of this as it scans the news stories that are being posted on the largest news sources, and boils them down to the essentials and automatically produces sensible summarizations of their content. Summly could provide you with clearly and concise summarizations of a news story, albeit in a terser form.

But these approaches are not enough to meet the growing strain of parsing, synthesizing, and managing growing data streams. The algorithmic sorting of information that led to personalized “truth” silos of the 2016 election are not enough. Recent attention seems to be focused on bringing “interesting” articles, websites, or images to the feeds of users. We need tools that can synthesize and extract meaningful bits from disparate sources.

A great effort is being put together by Distill, by approaching the difficulties of understanding new research. It is a journal that aims at disseminating breakthroughs in Machine Learning with the masses. Researchers and writers work together to create interactive visualizations that will make clear presentations of the interesting pieces of recent research. This is a great step in the right direction. We need a re-emphasis on constructing a well-paved path up the mountain of information that we are currently building our newest industries upon.

Potentially the only way to a future where modern research is accessible to the uninitiated is through publications like Distill. However, I would like to make the argument for more interesting technology tools. Tools that aim to bring the most important (not interesting to you) pieces of an article, scientific publication, book to the surface.

I know there is a lot of brilliant research happening in the realms of text extraction and automatic text summarization, but we need to commoditize this research. There are currently only 17 companies on Angel’s “Content Summarization” list. How can it be that we have more people interested disrupting wedding planning than innovating on consuming information. We need to build tools that summarize, extract, and distill while still providing access to the more long-winded underlying source material.

For anyone who has recently attended college, you know that textbook prices are sincerely out of control, rising over 1000% since the 1970s. Textbooks are long, inaccessible, and boring. Allen Downey’s Textbook Manifesto is a brilliant critique of this trend, and focuses on admonishing students and textbook writers alike for their willingness to accept this situation. I think this trend in textbooks is as much a symptom of the modern conundrum of parsing useful patterns out of the vast sea of information that we are expected to parse on a daily basis.

What if we could quiet these stormy seas and replace them with modern tools for exploring information at various levels of detail? What if we could replace rely upon algorithmically generated chronologies of the latest PR scandal? Peruse bullet-point summaries of the State of the Union and quickly scan a generated, paragraph summary of what makes a research paper novel in the context of its field? Tools with these capabilities would have the potential to totally revamp the way that we think about education.

Software Engineering is somewhat a special case in the scheme of professional education, as you are required to keep up with modern technological advancements. But I think this state of professional re-education should and will become standard if these type of tools are created and properly commoditized. Artificially constructed barriers to information formed along economic or political axes could be dismantled.

I speak of a future that we haven’t quite found a path to yet, but I encourage my readers to consider the profound, positive implications that competent, algorithmic text summarization could have on our modern world.|||

This article presents no new research or novel innovations, but rather paints a picture of what we could achieve with the proper technology. Historically, humans have found computers useful because…