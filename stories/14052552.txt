P = NP is a simple and short notation for a problem that “asks whether every problem whose solution can be quickly verified by a computer (NP) can also be quickly solved by a computer (P)”. P = NP means yes, P != NP means no.

The golden rule is NP because the golden rule is individual. Each and every experience makes a new golden rule.

Unless P = NP, you can’t program the golden rule.

A computer with a procedural moral compass is a tool of the compass builder.

A computer with a machine learning based moral compass will kill people because people kill people.

The golden rule is a built in evolutionary feature. Societies build moral compasses. The question is why would you need a moral compass if you have the golden rule? Because the golden rule is a generic solver, while a moral compass is a particular one.

The three forms of the golden rule:

Autonomous AI becomes a problem when they have reach. The moment it becomes a self contained execution with reach we’re hitting a wall of perception: we’ll require of that AI the same behavior we expect from all other aware creatures. There are only humans that fit this model and humans have a reason to follow the golden rule and an incentive to follow the moral compass.

In computer programming, any kind of programming, adding reason or incentive is easy. The problem is computing.

In Asimov’s laws of robotics there is a great potential for problems in computing, like infinite recursiveness. For example:

What if a robot witnesses a human about to kill another human and the only option of intervention would kill the attacker? The robot can’t intervene because it may not injure a human being. The robot can’t not intervene because it cannot allow a human being to come to harm. This is not a strong problem, it may be solvable, with special cases, which makes of robots simple expert systems

This doesn’t work because we are working on feedback. Feedback is immediate. That reasoning above is complex. You cannot calculate that maxim everytime. That is why the trolley problem is hard, there is no enforcing feedback for either option, because you cannot measure success, unless you make another expert system where each human has a computed value, and by adding up points you get a decision by comparing numbers.

A moral compass, however, is completely programmable. You can make each vice or virtue we know into a piece of code, and it will compute easily what to apply to each, and at the same time we can generate a feedback system to to actually teach the way of the compass.

An autonomous AI with a programmed moral compass will be able to distinguish right from wrong. But it will never be able to exhibit the Golden Rule. That is why there will be no empathy from a machine. There will be, no matter how fine the software, a moment when the uncanniness shall shriek our souls into a horror of alienation.

Unless P = NP. Because if all problems are solvable in polynomial time then there is the possibility of finding a technology fast enough to compute the golden rule in real time at human level requirements.|||

P = NP is a simple and short notation for a problem that “asks whether every problem whose solution can be quickly verified by a computer (NP) can also be quickly solved by a computer (P)”. P = NP…