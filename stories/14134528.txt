How do you search for source code? A common answer is to enter a few keywords (ex: “python compare strings”) into your favorite search engine and click the first match on Stack Overflow. If that Stack Overflow post wasn’t exactly the code that you needed, you probably scan through the user comments for a link to a related issue. We love Stack Overflow too, but that won’t work when you need to search within your organization’s internal source code repositories.

In the beginning of 2017 we started Altair to explore whether Paragraph Vectors designed for semantic understanding and classification of documents could be applied to represent and assess the similarity of different Python source code scripts. Our primary focus was to enable semantically similar source code recommendations for algorithm and expert discovery. However, we believe that robust feature representation of source code could be applied to challenges in cyber security like determining authorship and automated vulnerability assessment.

Lab41 is a big fan of Word2Vec and we have had success using word embeddings for unique applications such as system security log files. If you are a regular reader of Lab41’s blog, you might recall that my colleague Alex previously examined word embeddings for source code, which he called Python2Vec. The Python2Vec experiment involved training a Word2Vec model on almost 10,000 Python source code files. The trained model showed promising results when querying for nearest neighbors of example words like ‘array’ and ‘range’. We wanted to expand on Alex’s work in two ways: 1) Leverage a lot more training data, and 2) Transition from word embeddings to document embeddings.

A known limitation of Word2Vec is that it needs a lot of training data. NLP researcher Christopher Moody highlighted in a post that word vectorization for a highly specialized vocabulary requires training on hundreds of millions of words. Python source code is highly specialized so we decided to significantly expand the training set for Altair. We used Google BigQuery to download more than 1,000,000 Python scripts from GitHub as candidates for our training data. Google Cloud’s platform (which includes BigQuery) offers a trial that allowed us to download a 30 GB dataset of Python scripts for free. Bazinga!

While Word2Vec is great at vector representations of words, it wasn’t designed to generate a single representation of multiple words found in a sentence, paragraph or a document. One well known approach is to look up the word vector for each word in the sentence and then compute the average (or sum) of all of the word vectors. Unfortunately this erodes much of the value that was obtained by training the Word2Vec model on your data. Our primary interest in Altair was to find a way to represent an entire Python source code script as a vector. That led us to experiment with Gensim’s Doc2Vec python library, which is an implementation of Paragraph Vectors.

The Paragraph Vector and Word2Vec concepts were both designed by researchers at Google. Paragraph Vectors can be viewed as an extension of Word2Vec for learning document embeddings based on word embeddings. Similar nomenclature is used in both techniques and this is probably why Paragraph Vectors are more typically referred to as Doc2Vec or Paragraph2Vec.

At a high level, a Paragraph Vector is a new token that the authors explained as ‘a memory that remembers what is missing from the current context — or the topic of the paragraph.’ At a tactical level, Doc2Vec was designed to recognize that a sentence taken from a document that contains words about physics is more likely to use scientific words. This type of document-level context was not part of Word2Vec. Doc2Vec is built on Word2Vec and Doc2Vec maintains a matrix of paragraph vectors just like Word2Vec maintains a matrix of word vectors.

Doc2Vec offers two approaches to leverage the Paragraph Vector, which are called Distributed Memory Model of Paragraph Vectors (PV-DM) and Distributed Bag of Words version of Paragraph Vector (PV-DBOW). They are very similar to how Word2Vec uses Continuous Bag of Words (CBOW) and Skip-gram, respectively.

Our take on Distributed Memory Model of Paragraph Vectors (PV-DM) is that the paragraph id is inserted as another word in an ordered sequence of words. PV-DM attempts to predict a word in the ordered sequence based on the other surrounding words in the sentence and the context provided by the paragraph id. The input word vectors are averaged, summed or concatenated as part of the classification process.

Distributed Bag of Words version of Paragraph Vector (PV-DBOW) approaches the problem from the opposite direction, similar to how Skip-gram works in Word2Vec. PV-DBOW takes a given Paragraph id and uses it to predict words in the window without any restriction on word order.

While the original paragraph vector paper suggested that PV-DM worked well for most tasks, an empirical evaluation by researchers from IBM and the University of Melbourne found that PV-DBOW was superior to PV-DM despite PV-DBOW being a simpler model. The authors of the Doc2Vec library in Gensim also found that PV-DBOW outperformed PV-DM. (Spoiler alert: So did we)

We used training sets sizes of of 100,000 and 500,000 Python source code scripts taken from the 1,000,000+ total scripts that we downloaded from GitHub Archive. Our first step in preprocessing was to remove all comments/docstrings from the scripts. We normalized the remaining source code with standard NLP techniques, including removing and splitting on any character that wasn’t a letter or a number.

You might be questioning why we removed all comments. Don’t they provide valuable context too? We originally intended on treating the comments and the source code as two different Doc2Vec/Word2Vec language models. The idea was to obtain a vector representation of the comments from one model and a vector representation of the code from another model. If we concatenated the vectors, it might provide a richer contextual representation.

That sounded great in theory but the reality of comments in code on GitHub is more complicated. We confirmed that 1 out of every 4 Python scripts that we downloaded from GitHub Archive had no comments whatsoever. Do we have to assign a zero vector to every script without comments? A significant portion of the remaining code with comments contained long, slightly different licensing banners or commented out code that would require a dedicated effort to accurately identify and surgically remove. Even then, the majority of those scripts only contained a handful of “true” comments acting as tactical explanations about the next line of code (ex: “#Initialize variables” or “#Catch deprecation error”) .

We decided to only use source code in our Doc2Vec training while we were manually reviewing a sample Python script that contained no comments. While talking about it we realized that we could comprehend the code because of the context and meaning of the variable names and function names purposefully chosen by the author. As an example of the semantic value of function names and variable names, I bet you can follow this code snippet from Lab41’s Pythia project.

Training a Doc2Vec model on 500,000 Python scripts over 20 epochs varied from 24 to 48 hours on a large multi-core (32+) server, depending on the parameters and data. Fortunately Gensim included multi-core support for training a Doc2Vec model.

Similar to what Alex did with Python2Vec, our first impulse was to review the underlying word vectors in the trained Doc2Vec models. It appeared that both PV-DBOW and PV-PM were learning from the Python scripts and forming similar word embeddings. The example below references the Python word ‘open’, which is used to read (“rb”) and write (“wb”) to files.

The word ‘poisson’ was interesting as PV-DBOW returned several other probability density functions (ex: laplace, weibull) as nearby words. PV-PM returned several general statistical keywords (ex: uniform, variance) as nearby words but its similarity scores were much lower. This was our first subjective indication that PV-DBOW might be making better representations of words in Python source code than the PV-PM algorithm.

We suspect that the embedding differences between PV-DBOW and PV-PM are accentuated for rare words, as ‘open’ occurred 207k times and ‘poisson’ only appeared 2k times. This might be due to PV-PM’s reliance on word order as a major feature in the model.

Here’s an example that really surprised us. When we asked one of our PV-DBOW models for the nearest words to the data visualization package ‘plotly’, the model returned several other visualization packages, including pylab, pyplot, seaborn, bokeh and ggplot! How did it learn that?

Finally, everyone knows the Word2Vec vector math analogy example of “king - man + woman = queen”. Identifying similar vector math analogies in Python was a challenge but we had one success with “try - except + if = else”.

Doc2Vec and Word2Vec are unsupervised learning techniques and while they provided some interesting cherry-picked examples above, we wanted to apply a more rigorous test. To do this, we downloaded the free Meta Kaggle dataset that contains source code submissions from multiple authors as part of a series of Kaggle competitions. Our premise was that two different source code scripts written by two different authors to solve the same Kaggle competition problem should share semantic similarities. More specifically, Doc2Vec’s vector representations of two source code scripts from the same competition should have a smaller cosine distance than a source code script from any other competition.

We found that the Python submissions in Meta Kaggle contained an incredibly high percentage (75%+) of exact duplicates and near duplicates. We used SequenceMatcher inside Python’s difflib package with a threshold of 0.5 to filter out the nearly identical submissions. Our final testing set contained 907 code submissions for 25 Kaggle competition classes and each competition had at least 10 entries. We evaluated Doc2Vec against the final testing set and compared it with traditional NLP-based techniques: Bag of Words (BoW), Term Frequency Inverse Document Frequency (TFIDF), and Latent Dirichlet Allocation (LDA). All of the models in the charts below were trained on 500k Python scripts from the GitHub public dataset via Google BigQuery.

It’s worth noting here that the PV-DBOW and PV-DM models performed equally well when trained on 500k Python scripts as shown above but we found that PV-DBOW slightly outperformed PV-DM when the models were trained on only 100k documents.

These Top 1 classification results gave us confidence that Doc2Vec was capable of generating similar representations for two Python scripts from the same competition and providing one similar source code recommendation. Most systems provide more than just a single recommendation. What if we examined the top 10 closest source code representations for a given submission?

The overall accuracy of the two Doc2Vec models hovered around 0.20 out of a maximum score of 1.0 (PV-DBOW was the best at 0.21) on the task of the top 10 closest source code representations for a given submission. While not a particularly impressive accuracy on its own, we noted the growing separation between Doc2Vec and the established NLP techniques as the classification difficulty increased. We also believe that Doc2Vec could be further optimized beyond our initial experiments.

Still interested? Source code can be found in Lab41's Altair repository on GitHub.|||

How do you search for source code? A common answer is to enter a few keywords (ex: “python compare strings”) into your favorite search engine and click the first match on Stack Overflow. If that…