Analyzers are made up of two main components: a Tokenizer and a set of Token Filters. The tokenizer splits text into tokens according to some set of rules, and the token filters each perform operations on those tokens. The result is a stream of processed tokens, which are either stored in the index or used to query results.

For this post, we will be using hosted Elasticsearch on Qbox.io. You can sign up or launch your cluster here, or click "Get Started" in the header navigation. If you need help setting up, refer to "Provisioning a Qbox Elasticsearch Cluster."

The default analyzer is the Standard analyzer and contains these components:

Elasticsearch ships with a collection of language analyzers that provide good, basic, out-of-the-box support for many of the world’s most common languages:

Each analyzer may also apply other transformations specific to its language in order to make words from that language more searchable:

The English analyzer is really just Lucene’s EnglishAnalyzer.  Lucene EnglishAnalyzer class uses following components:

The built-in language analyzers are available globally and need not be configured before being used. They can be specified directly in the field mapping:

The English analyzer increases recall as we can match more loosely, but it reduces our ability to rank documents accurately.

In order to get the best of both worlds, we can use multi fields to index the title field twice: once with the English analyzer and once with the standard analyzer.

We can now index some test documents to demonstrate how to use both fields at query time:

The response is as follows:

Even though neither of our documents contain the word host, both documents are returned as results due to the word stemming on the title.english field. The first document is ranked as more relevant because the word "not" matches on the title field.

The English analyzer could be reimplemented as a custom analyzer as follows:

The response is as follows:

In general, 10 tokenizers, 31 token filters, and 3 character filters ship with the Elasticsearch distribution, a somewhat overwhelming number of options. This number can be increased further still through plugins, making the choices even harder to wrap one's head around. Combinations of these tokenizers, token filters, and character filters create what's called an analyzer.

The English analyzer is one of many language analyzers that are predefined in ElasticSearch. Analyzers are the special algorithms that determine how a string field 



in a document is transformed into terms in an inverted index. Choosing the appropriate analyzer for an Elasticsearch query can be as much art as science.

Analyzers are clearly powerful and essential tools for relevance engineering. When starting with Elasticsearch, it is very important that the user get acquainted with the different filters and tokenizers to be able to seize and optimize their full potential.

It's easy to spin up a standard hosted Elasticsearch cluster on any of our 47 Rackspace, Softlayer, or Amazon data centers. And you can now provision your own AWS Credits on Qbox Private Hosted Elasticsearch.

Questions? Drop us a note, and we'll get you a prompt response.

Not yet enjoying the benefits of a hosted ELK-stack enterprise search on Qbox? We invite you to create an account today and discover how easy it is to manage and scale your Elasticsearch environment in our cloud hosting service.|||

