Vous avez un blog ? Un site Ecommerce ou un site d’entreprise (sur magento, wordpress ou prestashop) et vous vous demandez comment faire pour être mieux classé sur Google ou sur les autres moteurs de recherches ? Commencez par faire un fichier Robots txt !



Vous commencez une longue aventure qui ne s’arrête malheureusement jamais ! Mais bonne nouvelle, certains critères  sont connus et permettent de vous améliorer. C’est le cas du fichier robots txt, ce fichier est indispensable que vous soyez tout petit ou un mastodonte de l’info. Voyons comment implémenter le fichier robots txt sur magento, wordpress ou prestashop.

Dans cet article vous allez donc apprendre :

Désolé pour les autres, j’ai pris les trois plus sympa actuellement et surement les plus utilisés aussi.

Le fichier robots txt est un fichier qui sert à l’exploration des contenus d’un site par les robots des différents crawler et moteurs de recherche.

L’objectif du fichier robots txt est d’imposer aux moteurs de recherche de ne pas indexer certaines pages / rubriques de votre site sur site (ex: demander à google de ne pas indexer la page mentions légales qui n’a pas d’intérêt en soit pour un internaute sur google). Il sert également à indiquer le sitemap de votre site, en gros le mapping de vos pages et là où il peut venir récupérer le contenu intéressant pour lui.

 /!\ Ne pas indexer, ne veut pas dire que la page n’est pas visible, vous dites simplement à google ou aux autres crawler : « hey, ne mets pas cette page dans les résultats de recherche »

Vous allez surement me dire, plus il y a de pages indexées sur google mieux c’est. C’est vrai, mais seulement si votre contenu apporte de la valeur pour un utilisateur de Google. Si ce n’est pas le cas, alors vous faites travailler le robot de Google pour rien et comme tout le temps c’est de l’argent, les crawlers comme google n’aiment pas en perdre !

Le temps que le robot d’indexation prendra pour visiter votre page et l’indexer est du temps de perdu pour le robot si votre page n’est pas intéressante pour ses clients (je parle des clients de google (ou autre crawler) aka nous autres les êtres humains). Google crawle toute la journée des millions de sites, si tout le monde se met à faire ça c’est beaucoup de temps de perdu pour lui !

Tips : Lorsque vous faites une refonte de votre site internet pour ne pas faire de duplicate content, faites un fichier robots.txt et interdisez l’accès à l’ensemble de votre version en cours de développement. Une fois la version en développement terminée n’oubliez pas de modifier le fichier robots.txt sinon bye bye les moteurs de recherche !

Le fichier robots txt est un fichier texte (.txt) et peut être crée avec n’importe quel éditeur de texte. Le bloc note de windows fait l’affaire par exemple. Il doit être obligatoirement placé à la racine de votre site, c’est à dire monsite.com/robots.txt .

User-agent: * = « Svp tous les robots, ce fichier est fait pour vous les gars! « 

 User-agent: Googlebot  = « Hello google ce qui suit n’est que pour toi ! »

 Disallow : « Hey bloque l’accès à des répertoires a et aux répertoire b aussi ! Merci  »

Que ce soit sur Mangento, wordpress, prestashop ou n’importe quoi d’autre, il faut que le fichier soit à la racine de votre site c’est a dire : http://mondomain.fr/robots.txt. Ouvrez votre FTP avec FileZilla par exemple et envoyez le fichier dans le répertoire source au même niveau que votre page index home (votre page d’accueil).

Normalement magento ne gère pas le fichier Robots.txt par lui même il est donc important de le rajouter et de lui dire quoi bloquer !

 Ne copiez collez pas bêtement, adaptez le à votre site (ajoutez votre Sitemap)

# Website Sitemap

 Sitemap: blabla.xml << AJOUTER ICI

 # Crawlers Setup

 User-agent: *

 Disallow: /404/

 Disallow: /app/

 Disallow: /cgi-bin/

 Disallow: /downloader/

 Disallow: /errors/

 Disallow: /includes/

 Disallow: /js/

 Disallow: /lib/

 Disallow: /magento/

 Disallow: /pkginfo/

 Disallow: /report/

 Disallow: /scripts/

 Disallow: /shell/

 Disallow: /skin/

 Disallow: /stats/

 Disallow: /var/

 # Paths (clean URLs)

 Disallow: /catalogsearch/result/

 Disallow: /catalog/product_compare/

 Disallow: /catalog/category/view/

 Disallow: /catalog/product/view/

 Disallow: /catalogsearch/

 Disallow: /checkout/

 Disallow: /control/

 Disallow: /contacts/

 Disallow: /customer/

 Disallow: /customize/

 Disallow: /newsletter/

 Disallow: /poll/

 Disallow: /review/

 Disallow: /sendfriend/

 Disallow: /tag/

 Disallow: /wishlist/

 Disallow: /index.php/catalogsearch/result/

 Disallow: /index.php/catalog/product_compare/

 Disallow: /index.php/catalog/category/view/

 Disallow: /index.php/catalog/product/view/

 Disallow: /index.php/catalogsearch/

 Disallow: /index.php/checkout/

 Disallow: /index.php/control/

 Disallow: /index.php/contacts/

 Disallow: /index.php/customer/

 Disallow: /index.php/customize/

 Disallow: /index.php/newsletter/

 Disallow: /index.php/poll/

 Disallow: /index.php/review/

 Disallow: /index.php/sendfriend/

 Disallow: /index.php/tag/

 Disallow: /index.php/wishlist/

 # Files

 Disallow: /cron.php

 Disallow: /cron.sh

 Disallow: /error_log

 Disallow: /install.php

 Disallow: /LICENSE.html

 Disallow: /LICENSE.txt

 Disallow: /LICENSE_AFL.txt

 Disallow: /STATUS.txt

 # Paths (no clean URLs)

 Disallow: /*.js$

 Disallow: /*.css$

 Disallow: /*.php$

 Disallow: /*?p=*&

 Disallow: /*?SID=

 Disallow: /*?limit=all

 # Si tu veux pas que Google indexe tes images enlève # devant les deux prochaines lignes

 #User-agent: Googlebot-Image

 #Disallow: /



Même principe pour le Robots twt wordpress mais avec les documents que vous ne voulez pas indexer sous wordpress (exemple la page de login administrateur) :

User-agent: *

 Sitemap: blabla.xml << AJOUTER ICI

 Disallow: /cgi-bin

 Disallow: /wp-login.php

 Disallow: /wp-admin

 Disallow: /wp-includes

 Disallow: /wp-content/plugins

 Disallow: /wp-content/cache

 Disallow: /wp-content/themes

 Disallow: /category/

 Disallow: */trackback

 Disallow: */feed

 Disallow: */comments

 Disallow: /*?

 Disallow: /*.php$

 Disallow: /*.js$

 Disallow: /*.inc$

 Disallow: /*.css$

 Disallow: /*.gz$

 Disallow: /*.swf$

 Disallow: /*.wmv$

 Disallow: /*.cgi$

 Disallow: /*.xhtml$

Et enfin le fichier Robots txt prestashop un peu différent.  Prestashop génère par défaut le fichier en enlevant les urls qui ne doivent pas être publiques. si vous souhaitez le modifier attendez qu’il le génère et ajouter ce que vous voulez dedans.

Si vous utilisez un FTP, vous trouverez le fichier robots à la racine de votre site. Il suffit de modifier le fichier en ajoutant la ou les pages/répertoires que vous voulez bloquer.

Merci d’avoir lu l’article, n’hésitez pas si vous avez des questions. Dans un autre article, nous parlerons de la différence entre no follow, follow, index et no index !|||

Le Fichier robots txt sur Magento, Wordpress ou Prestashop ! Ca sert à quoi ? comment le mettre en ligne ? Comment s'en servir, Réponse en 2 minutes !