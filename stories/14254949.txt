Kube-router is a distributed load balancer, firewall and router for Kubernetes. Kube-router can be configured to provide on each cluster node:

We have Kube-proxy which provides service proxy and load balancer. We have several addons or solutions like Flannel, Calico, Weave etc to provide cross node pod-to-pod networking. Simillarly there are solutions like Calico that enforce network policies. Then why do we need Kube-router for a similar job? Here is the motivation:

Go version 1.7 or above is required to build kube-router

All the dependencies are vendored already, so just run make build or go build -o kube-router kube-router.go to build

Alternatively you can download the prebuilt binary from https://github.com/cloudnativelabs/kube-router/releases

Depending on what functionality of kube-router you want to use, multiple deployment options are possible. You can use the flags , , to selectively enable only required functionality of kube-router.

Also you can choose to run kube-router as agent running on each cluster node. Alternativley you can run kube-router as pod on each node through daemonset.

caveat: Please use unsecure API port when using kubeconfig file option for now. Need further work on documentation about setting up certificate for kube-router to access API server. Kube-router is in catch-22 situation where it can not use incluster configuration to access the API server as it has setup the IPVS for kubernetes service cluster API

This is quickest way to deploy kube-router (dont forget to ensure the requirements). Just run

Above will run kube-router as pod on each node automatically. You can change the arguments in the daemonset definition as required to suit your needs. Some samples can be found at https://github.com/cloudnativelabs/kube-router/tree/master/daemonset with different argument to select set of the services kube-router should run.

You can choose to run kube-router as agent runnng on each node. For e.g if you just want kube-router to provide ingress firewall for the pods then you can start kube-router as

You can clean up all the configurations done (to ipvs, iptables, ip routes) by kube-router on the node by running

If you have a kube-proxy in use, and want to try kube-router just for service proxy you can do

and if you want to move back to kube-proxy then clean up config done by kube-router by running

and run kube-proxy with the configuration you have.

Kube-router can be run as a agent or a pod (through daemonset) on each node and leverages standard Linux technologies iptables, ipvs/lvs, ipset, iproute2

Kube-router uses IPVS/LVS technology built in Linux to provide L4 load balancing. Each of the kubernetes service of ClusterIP and NodePort type is configured as IPVS virtual service. Each service endpoint is configured as real server to the virtual service. Standard ipvsadm tool can be used to verify the configuration and monitor the active connections.

Below is example set of services on kubernetes

and the endpoints for the services

and how they got mapped to the ipvs by kube-router

Kube-router watches kubernetes API server to get updates on the services, endpoints and automatically syncs the ipvs configuration to reflect desired state of services. Kube-router uses IPVS masquerading mode and uses round robin scheduling currently. Source pod IP is preserved so that appropriate network policies can be applied.

refer to https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/ for the detailed design details

Kube-router provides implementation of network policies semantics through the use of iptables, ipset and conntrack. All the pods in a namespace with 'DefaultDeny' ingress isolation policy has ingress blocked. Only traffic that matches whitelist rules specified in the network policies are permitted to reach pod. Following set of iptables rules and chains in the 'filter' table are used to achieve the network policies semantics.

Each pod running on the node, which needs ingress blocked by default is matched in FORWARD and OUTPUT chains of fliter table and send to pod specific firewall chain. Below rules are added to match various cases

Each pod specific firewall chain has default rule to block the traffic. Rules are added to jump traffic to the network policy specific policy chains. Rules cover only policies that apply to the destination pod ip. A rule is added to accept the the established traffic to permit the return traffic.

Each policy chain has rules expressed through source and destination ipsets. Set of pods matching ingress rule in network policy spec forms a source pod ip ipset. set of pods matching pod selector (for destination pods) in the network policy forms destination pod ip ipset.

Finally ipsets are created that are used in forming the rules in the network policy specific chain

Kube-router at runtime watches Kubernetes API server for changes in the namespace, network policy and pods and dynamically updates iptables and ipset configuration to reflect desired state of ingress firewall for the the pods.

Kube-router is expected to run on each node. Subnet of the node is learnt by kube-router from the CNI configuration file on the node or through the node.PodCidr. Each kube-router instance on the node acts a BGP router and advertise the pod CIDR assigned to the node. Each node peers with rest of the nodes in the cluster forming full mesh. Learned routes about the pod CIDR from the other nodes (BGP peers) are injected into local node routing table. On the data path, inter node pod-to-pod communication is done by routing stack on the node.

Kube-router is in active development, the most up-to-date version is HEAD.There are many more things to explore around IPVS and monitoring. If you experience any problems, feel free to leave feedback or raise questions at any time by opening an issue here.|||

kube-router - A distributed load balancer, firewall and router for Kubernetes