Idlewords.com recently posted an essay about AI risk, a subject I’ve written about before. The essay attempts to demonstrate that AI risk is misguided, motivated by religious impulses. You should read it. It’s an excellent piece of rhetoric. He raises many arguments, most of which I think are lacking. Here I reply to most of them.

“Intelligence” is often defined as follows in these discussions: the capacity of an agent to use its resources to achieve its goals in a wide variety of environments

Those of us who find the arguments for AI risk compelling do not make this claim. It does not follow that any smarter mind can emulate any and less-intelligent mind in any practical sense. What we do claim is a more intelligent agent, all else equal, will be able to out-perform a less intelligent agent; and that they will be able to infer competing agent’s preferences and abilities and use this to their advantage.

Steven Hawking is a brilliant mind in an environment. This environment has resources, including an assistant. Being a brilliant man, he formulates a plan. He wiggles his eyebrow and slowly types a letter on the screen. This letter says: “Please place that cat in its kennel.”

His assistant thinks for a moment and imagines a device. It is a long stick with a loop of rope on the end. He constructs it out of a broom and spare roping and uses this to restrain the cat and force it into the box.

Einstein (being even more brilliant than Hawking and his assistant) places a can of tuna in the kennel and then closes and locks the door once the cat, enticed by the tuna, is inside.

All these actions are intelligent actions, actions a monkey could not take. Hawking doesn’t model the cat’s brain in his mind. Instead, he models his environment, which includes his assistant, and decides the best means of getting the cat in the box is to write “Please place that cat in its kennel” to his assistant.

The assistant, too, uses his mind to achieve his goal. He constructs a device that allows him to force the cat into the box. Einstein, also, uses his intelligence to get the cat in the box. He does not emulate the cat’s mind in his brain, instead, he infers the cat’s preferences and intelligence from his experience with other cats, and exploits this knowledge to manipulate it into the position he wants.

If an artificial intelligence can persuade or even pay humans to take actions, it like Hawking will be able to influence the world quite effectively, even without actuators. Should it require tools to be developed, it could construct them if it has actuators or persuade or pay others to construct them. Though it would likely not be able to emulate our brains, it would, as Einstein did, be able to infer our preferences and capability and exploit this. As con-artists prove, similar techniques as were used on this unfortunate cat are perfectly capable of working on humans.

A fun anecdote, but silly to bring up. The “Emu war” consisted of two soldiers with a machine gun. The fact that they weren’t successful in killing all emus in Australia isn’t surprising. Humans have caused the extinction of uncountable species purely as side-effects of our industrial expansion. If our history of interaction with less intelligent species in any gauge, we should be very worried about creating a more capable form of life.

I agree. Building friendly AI looks very, very, very difficult. However, the universe is allowed to expose us to impossible challenges. If you’re right and friendly AI is very, very, very difficult or impossible, this does not imply unfriendly AI isn’t possible or is not dangerous.

Psychopaths do not care for other people. You can have intelligent conversations with one about Shakespeare, but this will not make him care about people other than himself. They seem to have a different utility function than most people, and so provide weak evidence for the orthogonality thesis. What evidence we have from infrahuman AI is that they do seem to pursue their utility exactly as literally as Bostrom predicts — see OpenAI’s recent post Faulty Reward Functions in the Wild. Your implied statement that all human-level and above AIs will develop a psychology very similar to a human’s seems to be both unjustified and unconservative. And even if true, human psychology is not particularly friendly.

These AI’s are not at the level where they can understand their own function and so are incapable of self-improvement, as most lack even a concept of self. The state of affairs you describe is exactly what we would expect to see if recursive self improvement were possible and if it were not possible. I would point out, too, that even if recursive self-improvement is very limited, a species of machine that is only modestly more intelligent than us could still be dangerous, provided its goals differed from our own.

We don’t understand the brain well enough to make interventions like this. If we did, we would. AI’s will be able to make copies of themselves, examine their own code, and test modifications trivially. Thus, it seems likely they will be able to achieve an understanding of their own cognition much more easily than we can.

Let’s assume this is all true. Still, an AI is just software. And software can be replicated trivially and exactly. If it takes 20 years to train an AI to super-human level, once this training is done it can be reused to manufacture many copies of that AI. Thus we are left with a species that is smarter than us and has a replication rate much faster than us and is in our same cognitive niche. Once trained, its mind can be copied instantly, and its hardware manufactured at scale. The history of natural selection suggests this will not end well.

AIs will have access to all of human culture through the internet. Any advantage this grants us, AIs will have, too. Also, you are using fictional evidence in this argument. Fictional evidence.

I am very sympathetic to this argument. I would point out, though, if you invoke epistemic learned helplessness it seems to me you shouldn’t be advocating for the case either way, just adopting the opinions of experts. Though a minority position, the AI risk arguments are taken seriously by some heavyweights such as Stuart Russell of Berkley and reputable computer scientists such as Moshe Looks of Google and Paul Cristiano of OpenAI. People who are concerned about AI are not lone cranks. There are credentialed experts in the field who agree with Bostrom’s worries. Scott Alexander, the author of that post on epistemic learned helplessness you quoted, compiled a list of them here: http://slatestarcodex.com/2015/05/22/ai-researchers-on-ai-risk/.

Even if we surrender entirely to epistemic learned helplessness, we should accord some respectable probability to Bostrom’s worries being well founded.

If transhumanism is a religion it is the only religion with a plausible mechanism of action.

What human flight really is is a form of dragon worship. People have called a belief in flight “dragon imitation,” and it’s true.

It’s a clever hack, because instead of believing in dragons from the outset, you imagine yourself building an entity that is functionally identical to a dragon. This way even committed dragon denialists can rationalize their way into the comforts of faith.

The proposed F-16 has all the attributes of a dragon: it’s capable of flight, impenetrable to sword thrusts, and can reign fiery destruction from the heavens.

Humans are able to become intelligent with only sparsely labeled data. Today’s algorithms are unable to do this well. Tomorrows algorithms may not share that limitation — they certainly won’t if they are are smarter than humans.

Bostrom is merely asking this question: what will happen should we succeed? This seems to be a very important question for any engineering field.|||

Idlewords.com recently posted an essay about AI risk, a subject I’ve written about before. The essay attempts to demonstrate that AI risk is misguided, motivated by religious impulses. You should…