help you compose scalable services out of a sea of small logical units. In our (an open-source project that powers YouTube's main database) as a way of turning MySQL into a scalable Kubernetes application. Our goal was to make scaling your persistent datastore in Kubernetes as simple as scaling stateless app servers - just run a single command to launch more . We've made a lot of progress since then (pushing over 2,500 new commits) and we're nearing the first stable version of the new, cloud native Vitess.

We've also been hard at work adding lots more . In particular, the rest of this post will explore one of our new walkthroughs that demonstrates transparent of a live database - that is, changing the number of shards without any code changes or noticeable downtime for the application.

the shards as being like the buckets of a hash table. We decide which bucket to place a record in based solely on its key, so we don't need a separate table that keeps track of which bucket each key is in.

To make it easy to change the number of buckets, we use . That means instead of using a hash function that maps each key to a bucket number, we use a function that maps each key to a randomly distributed (but consistent) value in a very large set - such as the set of all 8-byte sequences. Then we assign each bucket a range of these values, which we call

If you want to follow along with the new , you'll need to first bring up the cluster as described in the . Both guides use the same , which is a Guestbook that supports multiple, numbered pages. 



Next, you'll run a few Vitess commands to from the original shard. The key to live migration is that once the initial snapshot copy is done, Vitess will automatically begin replicating fresh updates on the original shard to the new shards. We call this , since it distributes DMLs only to the shards to which they apply. Vitess also includes tools that compare the original and copied data sets, row-by-row, to

Once you've verified the copy, and filtered replication has caught up to real-time updates, you can run the , which tells Vitess to atomically shift app traffic from the old shards to the new ones. It does this by disabling writes on the old masters, waiting for the new masters to receive the last events over filtered replication, and then enabling writes on the new masters. Since the process is automated, this typically only causes about a second of write unavailability.

Note that we never had to tell the app that we were changing from one shard to two. The resharding process was completely transparent to the app, since Vitess automatically reroutes queries on-the-fly as the migration progresses.

At YouTube, we've used Vitess to transparently reshard (both ) nearly all of our MySQL databases within the last year alone, and we have still more on the horizon as we continue to grow. See the full if you want to try it out for yourself.

The max throughput (QPS) for a given number of shards is the point at which round-trip write latency became degraded, which we define as >15ms on average or >50ms for the worst 1% of queries (99th percentile).

There's still a lot of room to improve the benchmarks (for example, by tuning the performance of MySQL itself). However, these preliminary results show that the returns don't diminish as you scale. And since you're scaling horizontally, you're not limited by the size of a single machine.

With the new cloud native version of Vitess moving towards a stable launch, we invite you to

and let us know what else you'd like to see in the final release. You can reach us either on our

, or by filing an issue on

. If you'd like to be notified of any updates on Vitess, you can subscribe to our low-frequency|||

