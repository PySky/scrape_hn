This module provides an implementation of a recurrent weighted average (RWA)

class to create a model that conforms with the recurrent neural network

num_units: int, The number of units in the RWA cell.

decay_rate: (optional) If this is a float it sets the

decay rate for every unit. If this is a list or

tensor of shape `[num_units]` it sets the decay

rate for each individual unit. The decay rate is

defined as `ln(2.0)/hl` where `hl` is the desired

(decay_rate) tf.Variable: Do nothing if the decay rate is learnable

`zero_state` is overridden to return non-zero values and

parameters that must be learned.

a_max ( ) tf.ones([batch_size, num_units], dtype) Start off with a large negative number with room for this value to decay

The scope for the RWACell is hard-coded into `RWACell.zero_state`.

This is done because the initial state is learned and some of the model

parameters must be defined here. These parameters require a scope and

because `RWACell.zero_state` does not accept the scope as an argument,

state learnable. See `RWACell.zero_state` for more details.

"The initial state of the model contains parameters "

"that must be learned and these parameters are not "

"in scope. Please make sure that `RWACell.zero_state` "

"is under the same scope as the other parameters of "

a tf.matmul(xh, W_a) The bias term when factored out of the numerator and denominator cancels and is unnecessary|||

cas - Recurrent neural networks with customized attention spans