ParlAI (pronounced “par-lay”) is a framework for dialog AI research, implemented in Python.

Its goal is to provide researchers:

Over 20 tasks are supported in the first release, including popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, WebQuestions, SimpleQuestions, WikiMovies, QACNN & QADailyMail, CBT, BookTest, bAbI Dialog tasks, Ubuntu Dialog, OpenSubtitles, Cornell Movie and VQA-COCO2014.

Included are examples of training neural models with PyTorch and Lua Torch, with batch training on GPU or hogwild training on CPUs. Using Theano or Tensorflow instead is also straightforward.

Our aim is for the number of tasks and agents that train on them to grow in a community-based way.

We are in an early-release Beta. Expect some adventures and rough edges.

End goal is general dialogue, which includes many different skills

Set of datasets to bootstrap a working dialogue model for human interaction

Display 10 random examples from task 1 of the "1k training examples" bAbI task:

Displays 100 random examples from multi-tasking on the bAbI task and the SQuAD dataset at the same time:

Evaluate an IR baseline model on the validation set of the Movies Subreddit dataset:

Display the predictions of that same IR baseline model:

Train a simple cpu-based memory network on the "10k training examples" bAbI task 1 with 8 threads (python processes) using Hogwild (requires zmq and Lua Torch):

Trains an attentive LSTM model on the SQuAD dataset with a batch size of 32 examples (pytorch and regex):

Dependencies of the core modules are listed in requirement.txt. Several models included (in parlai/agents) have additional requirements such as PyTorch or Lua Torch--any python requirements in these modules are listed in requirements_ext.txt.

First, clone the repository, then enter the cloned directory.

Linked install: Run to link the cloned directory to your site-packages. This is the recommended installation procedure if you plan on modifying any parlai code for your run or submitting a pull request, especially if you want to add another task to repository. All needed data will be downloaded to ./data, and any model files (currently just the memnn model) if requested will be downloaded to ./downloads.

Copied install (use parlai only as a dependency): Run to copy contents to your site-packages folder. All data will be downloaded to python's folder by default (can override via the command-line), and to make any changes to the code you will need to run install again. If you want to just use parlai as a dependency (e.g. to access the tasks or the core code), this works fine. If you want to clear out the downloaded data, then delete the and (if applicable) folder in .

After defining a world and the agents in it, a main loop can be run for training, testing or displaying, which calls the function world.parley(). The skeleton of an example main is given in the left panel, and the actual code for parley() on the right.

All agents (including teachers) speak to each other with a single format -- the observation/action object (a python dict). This is used to pass text, labels and rewards between agents. It’s the same object type when talking (acting) or listening (observing), but a different view (with different values in the fields). The fields are as follows:

Each of these fields are technically optional, depending on your dataset, though the 'text' field will most likely be used in nearly all exchanges.

For a fixed supervised learning dataset like bAbI, a typical exchange from the training set might be as follows (the test set would not include labels):

The code is set up into several main directories:

Each directory is described in more detail below, ordered by dependencies.

The core library contains the following files:

The agents directory contains agents that have been approved into the ParlAI framework for shared use. Currently available within this directory:

This directory contains a few particular examples of basic loops.

Over 20 tasks are supported in the first release, including popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, WebQuestions, SimpleQuestions, WikiMovies, QACNN, QADailyMail, CBT, BookTest, bAbI Dialog tasks, Ubuntu, OpenSubtitles, Cornell Movie and VQA-COCO2014.

Our first release includes the following datasets (shown in the left panel), and accessing one of them is as simple as specifying the name of the task as a command line option, as shown in the dataset display utility (right panel):

See here for the current complete task list.

Choosing a task in ParlAI is as easy as specifying it on the command line, as shown in the above image (right). If the dataset has not been used before, ParlAI will automatically download it. As all datasets are treated in the same way in ParlAI (with a single dialog API), a dialog agent can in principle switch training and testing between any of them. Even better, one can specify many tasks at once (multi-tasking) by simply providing a comma-separated list, e.g. the command line “-t babi,squad”, to use those two datasets, or even all the QA datasets at once (-t #qa) or indeed every task in ParlAI at once (-t #all). The aim is to make it easy to build and evaluate very rich dialog models.

To add your own task:

An important part of ParlAI is seamless integration with Mechanical Turk for data collection, training and evaluation. Human Turkers are also viewed as agents in ParlAI and hence person-person, person-bot, or multiple people and bots in group chat can all converse within the standard framework, switching out the roles as desired with no code changes to the agents. This is because Turkers also receive and send via a (pretty printed) version of the same interface, using the fields of the observation/action dict. We provide two examples in the first release, collecting data, and human evaluation of a bot.

The mturk library contains the following directories and files:

To add your own MTurk task and dialog model:

ParlAI is currently maintained by Alexander H. Miller, Will Feng and Jason Weston. A non-exhaustive list of other major contributors includes: Adam Fisch, Jiasen Lu, Antoine Bordes, Devi Parikh and Dhruv Batra.

ParlAI is BSD-licensed. We also provide an additional patent grant.|||

ParlAI - A framework for training and evaluating AI models on a variety of openly available dialog datasets.