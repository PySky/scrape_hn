And the winner is:

Dan has written an incredibly strong Rock-Paper-Scissors program, which simply dominated every aspect of the competition. Of the 25 independent tournaments run for the Open Competition, Iocaine Powder won ALL of them. In the six sets of 25 tournaments conducted for the "Best of the Best" competition, Iocaine Powder finished first every time.

In many ways, Dan's program is a generation ahead of its time. I believe it would have been a worthy winner of the second RoShamBo competition, after all the lessons had been learned and ideas shared from the first set of tournaments. By co-operating with his fellow alumni from Caltech, he greatly improved on a previous version of the program which was already strong enough to win the competition!

The program name refers to a scene from the movie "The Princess Bride". The Man In Black has added iocaine powder, a deadly poison, to one of two goblets of wine. Vizzini the Sicilian must choose a goblet, then each will drink their wine. It is a battle of wits to the death, as Vizzini must deduce what level of trickery the Man In Black used in choosing the goblet to poison. If you don't know the outcome, rent the movie (it is wonderful). Vizzini begins his reasoning with:

The competition can be summed up with a line from the same scene:

This report contains lots of details on all the programs and authors, but first, the results! There were 45 programs in the competition, by 39 different authors representing 10 different countries. In addition, there were 10 "dummy bots" -- programs that are intentionally weak or simplistic, as described in the original contest announcement.

There were essentially two divisions, differing in the way a match was scored. In the Open Competition every point counts, so it is important to maximize one's score against the weaker opponents, and minimize the loss to the stronger opposition. In the "Best of the Best" tournaments, only the match result (win, loss, or a draw) was used to determine the order of finish -- the magnitude of a win did not matter. A draw was defined by the expected range of scores due to randomness alone (which after 25 tournaments is +/- 10 points at the 95% confidence interval; 10.14 theoretical, 10.04 observed maximum for Random (Optimal)).

In the complete crosstables appended to the end of this report, each set of 25 tournaments is averaged and presented as a single tournament.

In the following summary, "Open" is rank in the Open Competition, "BoB" is rank in the "Best of the Best" series of tournaments, "Leng" is the approximate length of the program in terms of C statements (estimated by counting semi-colons), and "Nat" is the nationality of the program author. A "*" indicates a dummy bot. Each match was 1000 turns long. The overall standing is based on the average rank in the two divisions.

Before I get into the gory details, I'd like to thank those people who contributed in some way, by providing faster tournament code, alternate random number generators, funny "super-modified" entries, and so on. Thanks also to sexsmith and lousana, our faithful tournament machines.

Organizing the contest was a lot more work than I expected, but was also a lot of fun. And yes, there will be another! (perhaps in a few months)

There are many misconceptions about the game of Rock-Paper-Scissors, about game theory in general, and about the usefulness of this event. To be blunt, there are a lot of people who "just didn't get it".

On the low end, there were the usual uninformed commentaries and useless speculation seen in postings to newsgroups (I remember now why I stopped reading Usenet years ago):.

At the other extreme, some highly knowledgeable researchers in the field of Artificial Intelligence game-playing were amazed to discover the surprising complexity behind the seemingly simple task of writing a strong RoShamBo program. Here are some of the things we have learned.

Sure, the game has a simple optimal strategy (choose a move uniformly at random), but that has little bearing on the problem at hand. First, not all the players are optimal. This changes everything. To win a tournament where some players are known to be sub-optimal, it is absolutely essential to try to detect patterns and tendencies in the play of the opponent, and then employ an appropriate counter-strategy. A match consists of several turns, and this changes the nature of the game, as was seen in the famous Iterated Prisoner's Dilemma problem.

RoShamBo (and its even simpler cousin, the Penny-Matching game) is an example of a pure prediction game. The difficulty lies in everything else that is associated with opponent modeling, or trying to outwit an adversary.

There is a lot of theory that can be brought to bear on the problem, including but not limited to advanced game theory (the "best-response dynamic in fictitious play"), prediction models, information theory, statistics, encryption, and even philosophical meta-theory.

So what is to be gained by playing this silly little kid's game? Many other problems deal with some form of context analysis or meta-reasoning (thinking about thinking about ...). To quote Jason Hutchens, author of MegaHAL, which was probably the best pattern detector in the tournament:

Personally, I just want my poker program to play better. :)

The optimal strategy won't lose a match by a statistically significant margin, but it also won't win a match, regardless of how predictable the opponent is. Try winning a chess tournament by drawing every game!

Moreover, the statement isn't even true in a more fundamental sense. Opportunistic strategies can be theoretically better, having positive expectation under more realistic assumptions. People interested in advanced game theory may enjoy the recent book "The Theory of Learning in Games" by Fudenberg and Levine.

Myth: Since all non-optimal strategies can, in theory, be exploited, the result of a tournament will be a crapshoot. At the very least, the outcome will be highly sensitive to the exact composition of players (algorithms) in the tournament.

The premise is true, but the conclusion is false. Any non-optimal algorithm can be beaten, just by employing the same algorithm and adding one to the action (r -> p -> s -> r). But complex algorithms are not vulnerable to such an attack. In general, they can only be beaten by an opponent who does a superior job of analysis.

There are many levels of complexity for playing algorithms, which can differ in the way they use history (context), in their perceptiveness of the opponent strategy, and in their defensive ability (hiding their own strategy). By in large, the more information a program processes, the better it will play the game.

In hundreds of preliminary experiments leading up to the official event, the distinctions between different classes of algorithms was obvious. New programs were constantly added or deleted from the mix, and as dummy bots were tuned, they exhibited widely varying degrees of predictability and exploitability. Regardless of the composition of players, the top six programs almost never changed position, the order of second tier programs always changed amongst themselves, De Bruijn always finished ahead of Pi ahead of Random (Optimal), and the bottom four positions never changed. The results were remarkably robust, and increasing the match length to 10000 turns or decreasing it to 400 turns had a negligible effect.

In the 25 independent tournaments for the official Open Competition, Iocaine Powder always finished first and Phasenbott always finished second. Iocaine Powder's margin of victory was a minimum of 436 points and a maximum of 1636 points, averaging 1043 points overall. So as a guideline, if the difference between two programs in one tournament is 1200 points, there is less than a 5% chance that the weaker program would finish ahead of the stronger one. Of course, in the combined set of Open tournaments, the class distinctions are much smaller.

Here are some summaries of selected participants and their programs.

Dan is a former member of an ACM programming competition team for the California Institute of Technology, and other members of that team played a role in the development of Iocaine Powder. His approach was based on an algorithm mentioned to him by Rudi Cilibrasi (Shofar) a long time ago. Dan gave his "historybot" to other members of the Caltech ACM alumni, and challenged them to do better. Through open discussion and friendly competition, the group improved their ideas and constructed stronger programs. Other contributors were Wei-Hwa Huang, who built a large test suite of programs, and Joshua Schachter. (Joshua must have been amused with the surprising incompetence of his little Cheesebot, and I suspect he submitted the cellar-dweller for that reason).

During its development phase, Iocaine Powder was overtaken by some other programs, but Dan Egnor met every challenge. The version he submitted 10 days before the deadline won every single test tournament for a week, regardless of which other algorithms participated (including the very strong MegaHAL program). But he wasn't done yet!

Jakob is another Caltech alumni who based his program on one of the earlier versions of Iocaine Powder. His refinements to the basic idea resulted in significant improvements, and when he submitted Phasenbott, it became the new favourite, surpassing its mentor by about 800 points in the Open Competition. However, its reign was short-lived, as Dan submitted the newest version of Iocaine Powder the following day, which reclaimed the lead by a similar margin. There were many strong players entered in the competition, but these two are in a class by themselves.

So what is their secret? Well, I can't really say, because I haven't figured out the code yet. :) One key appears to be in the way the historical information is processed. Rather than just considering the consecutive pairs of actions, it is important to look at all pairings. More generally, one must look at all subsets of a given size, not just those that happen to be rigidly sequential.

Another key element is the generalized approach for out-smarting the opponent. This is probably the most critical feature, as neither program is as strong as other entries in terms of pattern detection and statistical analysis. Iocaine Powder and Phasenbott implicitly deduce the opponent's prediction method, and counter that, rather than just the actual moves played. In a sense, they try to beat whichever player is most predictable -- the opponent, or themselves!

I would not want to divulge all of their insights, even if I did fully understand them. If the Caltech boys wish to elabourate, or release the source code, that is entirely up to them.

Jason is a PhD student at the University of Western Australia, in Perth, and his range of expertise includes many aspects of prediction models. In 1996, he was the winner of the first ever Loebner Prize, which is a "Turing Test" contest, where each program has a (written) conversation with judges, and tries to be indistinguishable from humans.

He submitted his first RoShamBo program three days after the contest announcement, using an "off the shelf" predictor, based on a finite context Markov model. He later upgraded it to use an infinite context Markov model maintained as a ternary trie, and named it after his other championship program, MegaHAL. This was easily the strongest program received in the early going, winning dozens of preliminary tournaments in the week prior to the submission of Iocaine Powder. I considered it to be the early favourite, feeling it would be very tough to beat (and I still believe that is true!). It is probably still the best "context sniffer" in the competition, but it was not as strong as Iocaine Powder at deducing and staying ahead of the opponent's method of prediction.

Russ' program is another extremely good pattern recognizer, but using a more straight-forward application of integer arrays to store and process statistics. It is similar to the approach used by many contestants, but is taken quite a bit further, with multi-dimensional arrays of up to 3^7 in size. This provided excellent contextual analysis, combined with the program's other smarts. Unfortunately, one common idea that he failed to encorporate was a "bail-out strategy", whenever the program is getting badly beaten. In the Open Competition, RussRocker4 took massive losses to the top two programs and a large loss to RoShamBot. Had he employed such a fail-safe, he could have reduced those losses by more than 1000 points, and moved into third place ahead of MegaHAL.

Perry graduated from Stanford University, and is now with SportsRocket. His program, the RoShamBot, has been playing against humans on the internet for years. He is one of the "Tiltboys", a notorious group of gamblers well known to the rec.gambling.poker community. Among their many "accomplishments", they deserve credit for popularizing both the game and the name "RoShamBo". They also gamble heavily on the childish "Circle Game", but that is another story.

The RoShamBot uses good frequency stats and history analysis, and does an accurate evaluation of the resulting distribution -- one of the few programs to compute expected values correctly. Some of these ideas are discussed further in part two of this report, in the dummy bot section (Anti-rotn bot).

The program has an interesting personality, in that it plays quite defensively to avoid giving away information to the opponent. It is designed to win a match in the long run, rather than maximize the net score in the short term. In fact, it cannot score more than 500 points out of 1000, even against Always Rock! Naturally this greatly reduced its chances in the Open Competition, but it was the early favourite for any type of "evolutionary" competition, because it never lost a match. It always won its matches against MegaHAL, and all others, until the arrival of Iocaine Powder. Another program with a careful exploitation strategy was Markian Hlynka's Markov chain program, Majikthise. An also-ran in the Open Competition, it demonstrated its true strength in the "Best of the Best" event, finishing equal fifth just behind RoShamBot.

Perry changed a parameter of his program to be more maximal against weak opposition, and RoShamBot_Off performed much better than the original in the Open Competition. BTW, you can play the RoShamBot on the web at SportsRocket: http://www.sportsrocket.com/cgi-bin/roshambo/roshambot.cgi.

Dr. Schaeffer is the world famous author of Chinook, the checkers program which became the first (and still the only) computer program to win an official world championship against humans in a game of skill. [He is also an excellent supervisor, but don't tell him I said that:].

He heads up the Computer Games group at the University of Alberta, which is a world leader in that area of research. Jonathan couldn't resist the opportunity to show-up his students, so he took on the challenge of writing a RoShamBo program only a few days before the deadline. Biopic uses straight-forward statistical and historical analysis, combined with an expected value calculation for each of several plausible opponent strategies. It did well despite a simple bug that hampered its performance.

Dr. Beal is well known to the Artificial Intelligence game-playing community for his work in computer chess. His first submission was a remarkable achievement in the "most for the least" category, as it was sent only a few hours after learning of the competition, and is only 35 lines of very simple code! It does a standard pairwise analysis of the history of both players, but he appears to get more mileage out of that approach. Simple Predictor was a reliable benchmark during testing, as it only lost to the strongest programs, and was always the "best of the rest". It was also one of the few programs with a history decay function to favour more recent actions, which shows up in its results against the Add-drift and Copy-drift dummy bots. Later, he submitted the Simple Modeller, which takes into account the opponent's attempt to predict it, again in his characteristically succinct style. Experience shows, and there is no doubt that he could produce a much stronger program if he chose to spend more time on the problem.

Andreas just finished his PhD at the U of A (under the supervision of Dr. Schaeffer), and he is a leading authority in single-agent search. He was the first person to submit an entry (named after his son Robert, who was 4 days old at the time), and it remained one of the best.

Robertot uses a voting scheme, where different lines of analysis make a prediction, and give it a weight based on the statistical significance of the observation. If a particular pattern or tendency is highly aberrant, the associated weight for that line of reasoning can give what amounts to "veto power" for that prediction. If none of the analysis leads to a result that is more extreme than a random noise level, the program chooses randomly (which often enables it to draw against very strong opposition). The implementation of the idea is rather crude, but it is still highly effective.

Jack is a PhD student at the U of A (and the new star pupil to replace Andreas:). He is the author of a Hex program and an Awari program that are ranked among the best in the world. His RoShamBo idea was somewhat similar to Robertot, but is much more rigourous in design. In fact, Jack spent a week doing the math before writing a single line of code!

All of Boom's actions are based on a decision tree, using the computed statistical significance for each observation. It also does a thorough analysis of bail-out strategies, recent success in the match, and uses "gear shifting" to adapt to the opponent. Boom is definitely strong, but it would appear that all the extra precision is not as important as other aspects of the game, like out-thinking the opponent in the manner of Iocaine Powder. However, it should be noted that the program played significantly stronger in post-event testing, after changing a single parameter setting.

These three are associated with the Department of Psychology at Carnegie Mellon University. The ACT group studies the theory and architecture of cognition. The ACT-R principles have been demonstrated in a number of applications, one of which was Rock-Paper-Scissors. You can play the program on the web at: http://act.psy.cmu.edu/ACT/ftp/models/RPS/index.html.

This was another entry in the "most for the least" category, since it is only 20 C-statements in length. Despite its brevity, ACT-R does pairwise context analysis of the opponent's history, uses a negative exponential history decay function, and a normally distributed random noise function! ACT-R, Boom, and Shofar were the only entrants that used the C math library.

The second part of this report will discuss the programs and ideas in further detail. It will also explain the exact algorithms used for the dummy bots, including De Bruijn, the amazing fixed string that finished third in the "Best of the Best" competition!

I will also describe the "Unofficial Super-Modified Class" competition, featuring a collection of humourous programs that cheat outrageously. Comparing these approaches is kind of like arguing about which comic book superheroes would come out on top of a fight against each other. Nevertheless, there was a definitive winner.

The Second International RoShamBo Programming Competition will also be announced!         - Darse.|||

