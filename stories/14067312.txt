Uglier than a Windows backslash, odder than , more common than PHP, more unfortunate than CORS, more disappointing than Java generics, more inconsistent than XMLHttpRequest, more confusing than a C preprocessor, flakier than MongoDB, and more regrettable than UTF-16, the worst mistake in computer science was introduced in 1965.

In commemoration of the 50th anniversary of Sir Hoare’s null, this article explains what null is, why it is so terrible, and how to avoid it.

I call it my billion-dollar mistake…At that time, I was designing the first comprehensive type system for references in an object-oriented language. My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn’t resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years. – Tony Hoare, inventor of ALGOL W.

The short answer: NULL is a value that is not a value. And that’s a problem.

It has festered in the most popular languages of all time and is now known by many names: NULL, nil, null, None, Nothing, Nil, nullptr. Each language has its own nuances.

Some of the problems caused by NULL apply only to a particular language, while others are universal; a few are simply different facets of a single issue.

Statically typed languages check the uses of types in the program without actually executing, providing certain guarantees about program behavior. For example, in Java, if I write , the compiler will inspect the type of x. If is known to be a , the type check succeeds; if is known to be a , the type check fails. Static type checking is a powerful aid in writing large, complex software. But for Java, these wonderful compile-time checks suffer from a fatal flaw: any reference can be null, and calling a method on null produces a . Thus, can be safely called on any …unless the is null. can be called on any …unless the is null. can be called on any …unless the is null. Java is not the only culprit; many other type systems have the same flaw, including of course, AGOL W. In these languges, NULL is above type checks. It slips through them silently, waiting for runtime, to finally burst free in a shower of errors. NULL is the nothing that is simultaneously everything.

There are many times when it doesn’t make sense to have a null. Unfortunately, if the language permits anything to be null, well, anything can be null. It’s such a common idiom that C# adds Every time you write code that conflates null strings and empty strings, the Guava team weeps.

 – Google Guava Well said. But when your type system (e.g. Java, or C#) allows NULL everywhere, you cannot reliably exclude the possibility of NULL, and it’s nearly inevitable it will wind up conflated somewhere. The ubiquitous possibility of null posed such a problem that Java 8 added the annotation to try to retroactively fix this flaw in its type system.

Given that NULL functions as a value that is not a value, NULL naturally becomes the subject of various forms of special treatment. For example, consider this C++: myChar is a , meaning that it is a pointer—i.e. the memory address—to a . The compiler verifies this. Therefore, the following is invalid: Since 123 is not guaranteed to be the address of a , compilation fails. However, if we change the number to 0 (which is NULL in C++), the compiler passes it: As with 123, NULL is not actually the address of a . Yet this time the compiler permits it, because 0 (NULL) is a special case. Yet another special case happens with C’s null-terminated strings. This is a bit different than the other examples, as there are no pointers or references. But the idea of a value that is not a value is still present, in the form of a char that is not a char. A C-string is a sequence of bytes, whose end is marked by the NUL (0) byte. Thus, each character of a C-string can be any of the possible 256 bytes, except 0 (the NUL character). Not only does this make string length a linear-time operation; even worse, it means that C-strings cannot be used for ASCII or extended ASCII. Instead, they can only be used for the unusual ASCIIZ. This exception for a singular NUL character has caused innumerable errors: API weirdness, security vulnerabilities, and buffer overflows. NULL is the worst CS mistake; more specifically, NUL-terminated strings are the most expensive one-byte mistakes.

For the next example, we will journey to the land of dynamically-typed languages, where NULL will again prove to be a terrible mistake. Suppose we create a Ruby class that acts as a key-value store. This may be a cache, an interface for a key-value database, etc. We’ll make the general-purpose API simple: class Store ## # associate key with value # def set(key, value) ... end ## # get value associated with key, or return nil if there is no such key # def get(key) ... end end We can imagine an analog in many languages (Python, JavaScript, Java, C#, etc.). Now suppose our program has a slow or resource-intensive way of finding out someone’s phone number—perhaps by contacting a web service. To improve performance, we’ll use a local as a cache, mapping a person’s name to his phone number. store = Store.new() store.set('Bob', '801-555-5555') store.get('Bob') # returns '801-555-5555', which is Bob’s number store.get('Alice') # returns nil, since it does not have Alice However, some people won’t have phone numbers (i.e. their phone number is nil). We’ll still cache that information, so we don’t have to repopulate it later. store = Store.new() store.set('Ted', nil) # Ted has no phone number store.get('Ted') # returns nil, since Ted does not have a phone number But now the meaning of our result is ambiguous! It could mean: the person does not exist in the cache (Alice) the person exists in the cache and does not have a phone number (Tom) One circumstance requires an expensive recomputation, the other an instantaneous answer. But our code is insufficiently sophisticated to distinguish between these two. In real code, situations like this come up frequently, in complex and subtle ways. Thus, simple, generic APIs can suddenly become special-cased, confusing sources of sloppy nullish behavior. Patching the Store class with a method might help. But this introduces redundant lookups, causing reduced performance, and race conditions. JavaScript has this same issue, but with every single object.

 If a property of an object doesn’t exist, JS returns a value to indicate the absence. The designers of JavaScript could have chosen this value to be . But instead they worried about cases where the property exists and is set to the value  In a stroke of ungenius, JavaScript added to distinguish a null property from a non-existent one. But what if the property exists, and is set to the value ? Oddly, JavaScript stops here, and there is no . Thus JavaScript wound up with not only one, but two forms of NULL.

C++ is a great example of how troublesome NULL can be. Calling member functions on a NULL pointer won’t necessarily crash the program. It’s much worse: it might crash the program. When I compile this with gcc, the first call succeeds; the second call fails. Why? is known at compile-time, so the compiler avoids a runtime vtable lookup, and transforms it to a static call like , with as the first argument. Since doesn’t dereference that NULL pointer, it succeeds. But does, which causes a segmentation fault. But suppose instead we had made virtual. This means that its implementation may be overridden by a subclass. As a virtual function, does a vtable lookup for the runtime type of , in case bar() has been overridden. Since is NULL, the program now crashes at instead, all because we made a function virtual. NULL has made debugging this code extraordinarily difficult and unintuitive for the programmer of . Granted, dereferencing NULL is undefined by the C++ standard, so technically we shouldn’t be surprised by whatever happened. Still, this is a non-pathological, common, very simple, real-world example of one of the many ways NULL can be capricious in practice.

Programming languages are built around composability: the ability to apply one abstraction to another abstraction. This is perhaps the single most important feature of any language, library, framework, paradigm, API, or design pattern: the ability to be used orthogonally with other features. In fact, composibility is really the fundamental issue behind many of these problems. For example, the API returning for non-existant values was not composable with storing for non-existant phone numbers. C# addresses some problems of NULL with . You can include the optionality (nullability) in the type. int a = 1; // integer int? b = 2; // optional integer that exists int? c = null; // optional integer that does not exist But it suffers from a critical flaw that cannot apply to any T. It can only apply to non-nullable T. For example, it doesn’t make the problem any better. is nullable to begin with; you cannot make a non-nullable Even if were non-nullable, thus making possible, you still wouldn’t be able to disambiguate the situation. There isn’t a|||

In 1965, Sir Tony Hoare introduced the null reference. Fifty years later, we look at its drawbacks, its presence, and the better way.