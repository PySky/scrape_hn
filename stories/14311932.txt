[Taken from Part 3 of my talk at the Seattle Node.js meetup on 7/24/2015. Code. Direct link to original screencast. My original Dangerous Cliffs of Node.js post.]

Asynchronous programming is difficult to wrap your mind around: threads, semaphores, and deadlocks, oh my! On one hand, Node.js makes this a whole lot easier: no locking or mid-execution interruptions. But its event loop is foreign territory. Let’s explore by poking it and prodding it - let’s try to break it!

The first way to break the event loop is to execute a task that takes too long. Take a look at this short script:

We set up an interval to fire every 100ms, logging how long it has been since the last call. Then, after 250ms, we run which goes to disk asynchronously and prints how long that took. Finally, at 500ms we call again then immediately call , a very evil function which takes one whole second to do its synchronous work.

There are a few key things to note. First, that initial call takes 1ms to go to the filesystem and return its result. Next, the first couple calls are spaced out by a little bit over 100ms, as requested. These are both signs of a healthy event loop, always ready to take on a new task.

But it gets interesting around the call. First, note that no calls happen during that synchronous work - they are skipped entirely! Second, the call takes 1000ms to finish - it is started before the event loop is blocked, and finishes after the event loop is available once more.

It’s key to note that this also happens on a server. If one visitor to your Node.js server is able get a long task onto the event loop, all other requests will be delayed, just like was. Unlike Ruby on Rails, with its separate thread per request, a Node.js server allows one operation to monopolize all resources in the process.

The other way to break the event loop is to put too many tasks onto it. Your server may be working through each task very quickly, but there are just too many! The next script is a little more complicated, designed to replicate a server under load:

The two key variables to look at are and . Doing some quick math, you can see that 100 requests per second, each taking 8ms of work, adds up to 800ms of work per 1000ms. The server will be able to handle that load, and here’s the confirmation:

The server is keeping up. It’s getting work done, and the lag, the time it takes to get through the even loop, is quite low. Now, if we change to 5, doubling the load to 200 requests per second, the amount of work per 1000ms will now be 1600ms. Let’s watch the process struggle:

That is not a healthy event loop. Every iteration is getting slower, and the process never gets a chance to catch its breath under the constant influx of new tasks. This is what will happen to a Node.js server under heavy load. That call from the first script will take over a second on this server, because that’s how long it takes to get through the event loop. Once more, interaction between independent user requests can happen in a single Node.js process.

Breaking something is important for truly understanding it. As Neil deGrasse Tyson said:

Software is the perfect playground - it takes some real effort to produce impactful negative consequences from experimentation. No need to replace broken eggs, or clean up the mess. No pain or injury. Just revert the changes to source code or restore data from backup.

So, break that event loop! Know what happens when you forget a callback or call it twice! Test your apps under load! Experiment!

You’ll get better at both designing systems and debugging them.|||

