California wants to enforce laws around the deployment of autonomous vehicles, and rightfully so. According to the state's Department of Motor Vehicles (DMV), there have been 18 reported accidents involving self-driving cars since January 2016.

In March, the DMV allowed the public to comment on the proposed regulations, to which Uber offered its own opinion. The set of comments was finally released today, and provide a valuable glimpse into Uber's values.

A letter from Anthony Levandowski, the Uber executive accused of stealing proprietary data from Google's own autonomous car program, reveals much about the company's vision for its self-driving cars. "The suggestions are intended to enhance the flexibility of these rules so they can accommodate the various ways that autonomous vehicles will be used in the future," Levandowski wrote.

Uber rejects the DMV's proposal to collect certain test vehicle safety data. In order to obtain a deployment permit, Uber's self-driving cars would need to provide sensor data at least 30 seconds before and five seconds after a crash. All of this data, the DMV says, should be readable by a "commercially available" tool.



Levandowski contends that it's impossible to make this data "commercially available," considering the rate at which this technology is advancing. But Uber notoriously skirted a $150 DMV testing permit for its self-driving cars in San Francisco last year, likely to avoid submitting accident and software failure reports.

The DMV should know whether a person was driving during a crash, or whether it was the car's fault. Any objections to that seem suspicious. Earlier this year, the New York Times reported that an Uber car was driving itself when it sped through a red light in San Francisco—something the company had, until then, covered up. This information was only made available by the leaking of internal documents. If Uber had been submitting accident data, perhaps we might've known about this sooner.

I reached out to Uber for comment, but did not hear back.

Uber also insisted that passengers should have to pay to ride in its self-driving cars. This is opposed to the DMV's regulations, which state that companies can't shuttle people around if they're going to profit from it. Presumably, this statute would protect members of the public from being charged to beta test a technology that hasn't been deployed yet.

"From a safety perspective," Levandowski said, "it is immaterial whether or not a passenger pays a fare when the vehicle is operated by the manufacturer with a driver that meets the qualifications of the testing regime."

Uber alleges that people can suffer a "cognitive bias" when given free rides—potentially ruining the type of feedback that passengers end up giving. (For what it's worth, I couldn't find any scientific literature to support that notion.) Uber instead places the onus on us, quoting Trump-appointed Transportation Secretary Elaine Chao, and noted Uber supporter, who once said we need to "help educate a skeptical public" about autonomous vehicles.

The rest of Uber's requests follow suit—the company should not have to work with law enforcement to coordinate testing, and more should be done to protect Uber's proprietary data. It's unclear how much of the company's input the DMV will ultimately adopt.

When I asked the DMV how it plans to deliberate on Uber's requests, a spokesperson said the DMV is required "to respond to each commenter. If we make changes to our proposed regulations, we will hold an additional 15 day public comment period."|||

What's more important to Uber: proprietary data or rider safety?