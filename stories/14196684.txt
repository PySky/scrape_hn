As I'm sure you're aware, Docker is all folks in the IT world can talk about and while Moby(Docker) may be everyone's obsession, there are quite a few of us looking to break away from the oppressive registries and wannabe bash scripts found in the stratosphere. It's time to move further down the stack! This post is all about using the tools built into the system which will allow me to integrate into an existing network and place workloads into a container all without having to bow to Ahab.

The utilities packaged with(into) SystemD are quite powerful. This quick run down will demonstrate how to setup NetworkD for container and host connectivity, manage images using , and create a persistent container. By the end of this article we'll have a Plex container using Ubuntu Xenial(16.04) and everything will start and stop along with the system.

On this node, I have just one network interface and I will be moving everything over to NetworkD, however, this is an optional step. If you have a better way or an already working way or you want to do something different, you're free to skip these steps.

The first thing we do is ensure that Network Manager and the SysV init networking compatibility services are all disabled. We then enable the NetworkD so that it's managing everything.

Check to see that is running with the following command.

This is the base interface for my server, enp3s0.

Create the network on the bridge. If you're worried about losing network connectivity, set the [Network] section to that of a static IP address. This will ensure when NetworkD takes over it doesn't get a new address thereby breaking your connection. See the NetworkD manual for all available options within the Network Section.

Once these files are in place, restart the NetworkD service. You should be aware that it's quite possible that you'll lose network connectivity on this next step. If you do make sure you have console access to the server before running it.

Finally, move the old SysV interface file out of the way

These commands will start the systemd-resolved service and link the typical file the systemd version of the file.

Ensure that you still have access to the public internet after the resolver changes.

Enable the "machines" target so all of the containers we create are able to be started at boot via a service call. While the services may be started and enabled, if this target is not added the nspawn containers will not auto-boot.

The nspawn system will need a BTRFS volume at the path . If you have a volume that will need to be formatted and mounted, follow these next couple of steps. If you're unsure or know there's no extra volume on your system to format and mount, don't worry about it. SystemD will do what's needed should the path not be a BTRFS volume by creating a raw file at . This file will be mounted it as a loopback device and available on boot. In my case I'm going to provision a logical volume, format it BTRFS, and have it automatically mounted.

The first thing to do is create the logical volume. I annotated the command for clarity.

Once the volume is formatted, create a mount unit file. Do not be modifying fstab, this is all done in SystemD.

Reload the systemd deamon and start the mount

This will create the persistent mount where all of your containers will be spawned.

Now that the system is all setup lets pull an image.

is how all image management will be taken care of. Pulling, cloning, removing, exporting images is all taken care of in . To test everything is working run the command using the path of the cached image.

This will spawn a container and connect it to your bridge. If everything is working a new system will seemingly be created and you will be greeted with a login prompt. Because there's nothing setup at the moment I recommend just terminating the container by holding the Ctrl button and pressing the "]" key around 5 times rapidly.

Being that this is our base image and we want to be able to login into the container we're going to chroot into the containers storage directory and set the root password. This is an optional step and only needed for testing.

This command is not really needed, however, it is helpful when pulling the Ubuntu cloud image which has cloud-init baked in it. We don't need or want cloud-init so it's safe to purge it and it will mean that our containers will spawn instantly instead of waiting 120 seconds for cloud-init to fail.

Now with the password set if you run the command from before and you should be able to login to the container and run some commands.

Once ready set the base image as read-only so no further changes can happen to the it unless we unlock the image and allow them.

Check that the image is now by executing the .

Now that we've got everything up and running it's time to spawn our first container. To test that our container system is working and proving networking as expected We'll be creating a plex container which will host the plex app and be available over the larger network.

The first thing to do is clone our base image. We're going to clone xenial-base to a new copy on write container image named, plex.

With the plex container rootfs ready let's start the container.

Now verify that the container is running.

We can login to the container and mess about however before we can do anything meaningful we're going to generate a nspawn configuration file for the container, so stop the container for now . The following configuration file will enable the container to "boot" into full machine mode, bind mount my media library from /mnt/media to /mnt within the container, and will connect the container using a veth-pair onto br0 allowing me to access to the container on my internal network in an isolated manner.

Re-start the plex container with the following command , and then check it's status. If everything is right, the container will come up connected to the br0 bridge.

Lastly, we enable the container to start at boot by simply invoking the command just like we would with any other service.

Getting access to your nspawn containers is a choose your own adventure. For my purposes, I'm covering regular shell and ssh access.

If you want to run shell commands within the running machine container you can execute the following shell command.

If you want to have access to the container using ssh you can do so by running the following shell command.

This will attach to the plex container and create host keys on the box which is required to get ssh functional.

If you want ssh access to the container you will need to send over an ssh public key. Here I generate a key pair in the container and then append the local system key into the container.

Because this container is a "Machine" container, access to the container can be done in any way that suits you and the needs of the application. This could be ansible, chef, some other automation thing-a-me, or simply by hand, it's all up to you. Here I'm just following the upstream guide for plex installations and upgrades because there's not much to it.

Once the plex is installed, navigate to the web interface and begin enjoying it.

Now that we have a working system and we have nspawn working as we'd expect, let's see what it would actually take to spawn a new container for all future workloads.

So with this setup, you'll be able to get just about anything you need with containers directly out of the box, all without any additional dependencies. The container runtime is fast, there are no abstractions sitting in the way impeding something that you may want to do, and the ability to configure the system is nearly endless.

I'd invite you to have a look at everything that's possible within nspawn, NetworkD, ResolveD, machinectl and SystemD by pursuing the manuals.|||

