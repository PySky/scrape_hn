Israeli historian Yuval Harari is getting a lot of attention with a dramatic vision of the future, in which humans merge with technology to evolve beyond themselves and ultimately colonize outer space -- potentially making our generation one of the last of Homo Sapiens.

I'm hoping this is more of a cautionary tale than a roadmap for the development of our species.

Harari is a book-writing rock star, whose volume "Sapiens" won praise from the likes of Barack Obama and Bill Gates. In his follow-up, "Homo Deus," he makes a lot of speculative statements in a way that seems intended to alarm, as if they were bound to come true. In describing how an artificial-intelligence service like Siri might someday give us advice on whom to marry, for example, he writes that "in exchange for such devoted counseling services, we will just have to give up the idea that humans are individuals, and that each human has a free will determining what’s good, what’s beautiful and what is the meaning of life.”

Frankly, I find many of his ideas preposterous -- particularly the implication that our understanding of biochemistry, combined with big data, will soon subsume the concepts of free will and the human soul. I see scant immediate evidence beyond our infatuation with Siri, recent improvements in brain implant technology and the opioid epidemic. Yet these ideas come from a person who, by all accounts, is a highly intelligent and serious thinker. So what gives?

My theory is that he’s putting us on. He’s conducting a thought experiment on his audience, extrapolating what might happen if we continue on our arrogant trajectory and refuse to think about how our lives are being subsumed by our obsession with ourselves and our technology. We’re so oblivious, he’s saying, that before we know it we’ll all be robots and we’ll think it’s grand. Yet he doesn’t necessarily believe that the future he's describing will come to pass.

I take my theory in part from Harari's own discussion of why we should study history. He refashions the old Santayana quote about how those who forget the past are doomed to repeat it: Instead, he posits that those who wish to avoid doomed futures must predict them. He explains that we are better off considering how bad things can get, so that we can “make up our minds about it before it makes up our minds for us.” The best way to avoid dystopia is to imagine it.

In the potential future Harari describes, if we continue on this trajectory called humanism, it will lead us to create artificially intelligent godlets. Or, rather, it will lead us to become such godlets, augmenting ourselves step by step with technology in a process that will erase our humanity, turning us into machine hybrids that seek only pleasure.

I would put it differently: We will have to change our philosophy, and shed some of our self-absorption and inane love of gadgetry, if we wish to avoid such a future. If that's what Harari is getting at, I’m with him.

This column does not necessarily reflect the opinion of the editorial board or Bloomberg LP and its owners.

To contact the author of this story:

 Cathy O'Neil at cathy.oneil@gmail.com

To contact the editor responsible for this story:

 Mark Whitehouse at mwhitehouse1@bloomberg.net|||

Yuval Harari's vision of the future should serve as a cautionary tale.