Marathon (the Mesos framework) is an excellent tool for scheduling work across a cluster of machines and can re-schedule in case of a node failure.

Flocker does a great job of orchestrating data volumes around a cluster of machines and can move those volumes between nodes.

Combining these two tools means we can run stateful Docker containers and automatically recover from a node failure. In other words - Highly Available Stateful Containers!

Waking up in the middle of the night: your application is running smoothly - customers are purchasing your product and you are currently asleep dreaming of shipping containers and unicorns. It hits 3am and you are now in deep sleep - literally oblivious to the world.

Then, something terrible happens - the node you are running one of your database containers on has a catastrophic failure. Customers can no longer purchase and your phone starts vibrating with a 4 on the richter scale.

Being in a deep sleep, it takes you 10 minutes to wake up and get any idea what is happening. Some strong coffee and you spend the next 3 hours resolving the problem by bringing up another node, installing and configuring your application and restoring your backup.

As the sun comes up you remember that you have an important client meeting in 2 hours and think to yourself surely there is a better way?

Automation! If you had used Marathon and Flocker then instead of the rather horrible experience above - the following would have happened:

The master node is running the following components:

Installation of the AWS CLI is fairly simple. In most cases you can install AWS CLI by running:

Before we can proceed, you will need to configure the AWS CLI with your AWS API credentials.

You will need to input your AWS API credentials, you can find information on how to generate this information from the AWS documentation. There’s no need to specify default regions, however you may want to do this for other projects.

In order for us to communicate with the instances which you provisions, you will need to ensure your keypair is added to the region. We depend on this region as that is where our base AMI is located.

Here are the steps to create the key pair:

Login to your AWS control panel and click the button to manage your instances:

Next - ensure that you are in the region - use the drop down on the top right of the page to do this.

Then - click the menu option from the sidebar on the left.

Give the keypair a name and click . We will use this name later on in the tutorial.

The AWS control panel will prompt you to download the generated file. Save this file somewhere (normally in but can be anywhere you want).

Once you have saved the key to your local computer - you need to ensure that the permissions are set correctly. For example - if our key was saved into then this command would change the permissions:

Once you have created a keypair using the AWS control panel - you must define 2 variables as follows:

Optional: If you wish to run this demo multiple times at once, you will need to set a unique . This is the text that is used to identify AWS components in our Marathon cluster.

Once you have set your configuration, you can begin - run the following command:

This will build a Mesos cluster with three EC2 instances.

Once the cluster has completed building using , it will print some useful information which you should save for use later on in this tutorial.

Along with the EC2 instances, the Makefile will create an Elastic Load Balancer (ELB). This is so that we can always visit the application at the same address, regardless of the EC2 instance it is located on.

Next - we open the GUI for Marathon that shows the status of our application. To do this, copy the field from the cluster information and paste it into your browser.

Note there are zero deployments in our Marathon cluster, this is because we have not deployed anything yet. First thing we need to do is get an app running.

We have included a basic application manifest, within the repository. This configuration includes a single container with an attached volume running “Moby Counter”, the idea here is very simple:

The end result is a simple stateful application saving the places on a page where the client has clicked and displayed a Docker logo in its place.

To deploy this application we simply need to run:

This will talk to the Marathon API and deploy Moby Counter. If we reload the Marathon web interface again, we will see a new application is running.

Now we have our application running and its state is being stored on the Flocker EBS volume, lets write some state to it!

When we created the cluster, we also created an Elastic Load Balancer (ELB). Since this has been running it has been configured to check for when an application is responding and began routing requests to it when the application started responding.

You can reach the application by heading to the load balancer in your browser with the info which was provided when you created your cluster.

In your browser you can now click around and begin to write some state!

You can go ahead and reload the page to see the state is still there.

Because our Flocker dataset is stored on an EBS volume, we can kill the server which is hosting the application and it can be started up again on a different EC2 instance with the same data. So lets do that!

We have included a way to query the Marathon API to determine which server is hosting our Moby Counter application, and in turn destroy it with the AWS API.

Once you have done this, the instance will be destroyed and shortly after Mesos will notice the server is not responding. Marathon will see this and begin redeploying your application.

Once Marathon has redeployed your application, the ELB will soon detect that the other server in your cluster is now responding on the application port, and requests will be routed there. It’s as if nothing even happened!

Now we’re done with the demo, you can remove all your EC2 instances and the ELB by simply running|||

Using Flocker to build highly available Docker based services with Marathon and Mesos