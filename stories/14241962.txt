Hype song aside, I wanted to take a moment to share the lessons I learned while making Mischief Maker. This was a case of me pushing myself out of my comfort zone and trying to build something new and different. This pretty much meant that I started the project with an idea in mind of what I wanted it to be, but was in a total fog about how it would actually work. I didn’t know if it would actually work!

In this article we will walk through the basic thought structure I followed as I started to reason about how to integrate Angular with a midi controller and how I solved the problem. I would classify the code as “exploratory”, but I believe it could be helpful nonetheless.

The code to Mischief Maker is in the link below. One thing to note, this was a full page Electron application with my slides and demos integrated together for performance reasons. We are going to cherry pick the important bits that deal with midi.

Connecting to a controller is obviously the first place we needed to start. Interestingly enough, I had no idea how to do this when I started building this project. Fortunately for us, there is a Web Midi API specification that allows us to enumerate, select and interact with midi devices. To gain access to our midi devices, we use navigator.requestMIDIAccess() which will return a promise that contains an iterable of all our connected midi devices. Because I am a huge fan of observables, I use fromPromise to convert our response into an observable so that I can transform the response into just the device that I want. To make it easy to select the device I want, I use Array.from to convert the iterable into an array. I am then grabbing just the midi inputs in the next map operation.

We then assign the response to this.devices and then iterate over the collection and display them in our template.

Now that we have our connected devices, how do we capture a signal from them? This was a bit of an interesting step for me because I wanted to do something that would preserve the observable stream. In order to do this, I needed to find a way to wrap the onmidimessage event in an observable stream so I could pass the event object along. I accomplished this by creating a midiMessageAsObservable method which created a new observable subject that I could pass input.onmidimessage events down the stream by calling source.next(note).

We then return our subject as an observable that we insert back into our messages$ stream with flatMap. From there, we are just adding the raw note data into an array so that we can visualize it.

So we have raw midi data but how do we convert that to something that represents an actual musical note? This part was pretty straight forward as I just had to create a map that would transform the key value to a musical note. There are a few things within the midi stream itself that were pretty constant in the context of my example such as a status of 144 being a key press event and so I hard coded it as such. You will probably need to adjust this depending on the device you are using.

What was interesting to me is that I realized that I could map these keys to anything. There is no reason that I could not map middle C on my keyboard to make an API call to turn on a light halfway across the country when the key is down and turn it off when the key is up. When you start thinking of midi controllers as not just musical instruments, but rather just a big switch that you can map the keys to whatever you want, that is when things start to get really interesting!

We have gone from connecting to listening to converting, but it would be nice to get to actually playing notes. I tried my hand at a few approaches until I found the amazing Tone.js library that streamlined a ton of stuff for me. I highly recommend investing some time into reading through their documentation and playing with their samples because it is a really powerful library. In Tone.js, the basic element of playing music is the Synth which can be expanded upon with a few additional versions of it. Because I wanted to play more than one note at a time, I created a PolySynth instance and then configured it to have six channels and use the fatsawtooth oscillator. I also created two analysers that I added to the synth which we will discuss in a moment.

With our synth created, we can call this.synth.triggerAttack to play a note and this.synth.triggerRelease to end the note. We are also calculating velocity to tell the synth how hard the key was pressed.

We can now play music from our controller, but how do we record it? For this challenge, I added Recorder.js to this mix. What was awesome about this is that I was able to initialize a Recorder instance by passing in my Tonejs synth instance. With an instance of Recorder.js in hand, I had some fairly intuitive methods at my disposal. To start recording, I called this.audioRecorder.record after clearing my previous recording with this.audioRecorder.clear. To stop recording, I called this.audioRecorder.stop and this.audioRecorder.getBuffers to handle the audio buffer I just created.

And to convert the buffers into a WAV file, I called this.audioRecorder.exportWAV which conveniently encoded the file for me.

Once the file was encoded, I introduced one more player to the party by literally introducing a player. To display what we just recorded, I am passing our sound blob into a Waversurfer.js instance. I will talk about this more in the next section, but Wavesurfer.js gives us a really nice way to visualize and play an audio track.

Perfect segue into the section about how to play a midi track! Because I know that we are eventually going to want to play multiple tracks, I created a TrackComponent that I could just pass in our audio data and it would render and play it. To initialize this.waveSurfer, I need to give a value to the container element that the track will be rendered in. There is a canvas element in the template called trackWave that I am referencing using @ViewChild.

I also want our recorded track to loop, so I am adding a listener for the finish event that calls this.waveSurfer.playPause() to tell it to start playing again. You will also notice that I have an ngOnChanges event that is watching changes to the playing property and toggling whether our track is playing or not. I did this so that we could control the playback on multiple tracks at once from the parent component. The idea is that if we had more than one track, they would listen to the same playing property and start or stop playing as a group whenever it changed.

This simplicity of this next example is why I love component driven architecture. By having a solid track component, all we need to do to create multiple tracks on our stage is to keep a collection of them and loop over them with ngFor.

When we are done recording a track, we add the audio track to our collection. And because they are all bound to the same isPlaying property, we can play or stop them simultaneously.

If someone were to extend this further, the next thing to do would be to control the timing of the tracks so they the playback was synchronized with a locked timespan.

This entire project would have never happened without a huge push in the right direction from my buddy Ken Rimple. I was googling “How do I keep from destroying my career by learning Angular and midi in time for ng-conf!?!?!?!?!” and I ended up on a repository that was surprisingly helpful and lo and behold! It was Ken’s project! Fun fact… the very first conference I ever spoke at was Philly ETE which is organized by Chariot Solutions which Ken works for. I have since spoken at a bunch of conferences, but Ken was the one who helped me do a tech check the night before I was about to embark on something new and scary and totally awesome.

Here are two repositories of Ken’s that are especially interesting to this post.

Also, Ken does awesome training on the East Coast and so I highly recommend checking him out if you and your team are looking to level up.|||

