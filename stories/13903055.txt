This work builds upon Deep Convolutional Generative Adversarial Networks (DCGANs), which builds upon Generative Adversarial Networks (GANs) respectively.

Previously, GANs had only been applied to single images, as opposed to videos. In a GAN, two deep networks compete against one another. One network ("the generator") attempts to generate a synthetic video, while another network ("the discriminator") attempts to discriminate synthetic versus real videos. The generator is trained to fool the discriminator.

The discriminator is shown above.

For the generator, the input is a vector of gaussian noise of length 100. In terms of video modelling, spatiotemporal up-convolutions (2D for space, 1D for time) are used.

Another innovation is that the generator models the background separately from the foreground. The background is treated as a static frame, while a dynamic foreground is overlaid on top of it.

A further innovation is future prediction: that is, the ability to generate predicted future frames based on a single frame. This was achieved by bolting on an encoder to the input of the generator. The encoder converts an input image frame into a vector of length 100, which is then fed into the generator.|||

