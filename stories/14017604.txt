When I came across the quote above by R. A. Fisher while reading a paper on probability theory, I couldn’t help but think about the vast corpus of documents that contend for survival (relevance) for a search query. To add to that the ever increasing bar of expectations of the results to be in the top 10.  On average, 71.33% of searches resulted in a page one Google organic click. Page two and three get only 5.59% of the clicks. On the first page alone, the first 5 results account for 67.60% of all the clicks and the results from 6 to 10 account for only 3.73%. As you can see in the chart below:

It becomes almost imperative to think in terms of fitness function that is required to survive the natural selection of organic search results.  There are many ways to define a fitness function; After a few hours of reading through Manning, getting some Math squared out at Khan Academy and googling around; I settled on choosing three of (many) standard ways to measure relevance of information retrieval; As listed below, lets look into these in some detail:

nDCG (Normalized Discounted Cumulative Gain) – nDCG or Normalized Discounted Cumulative Gain is a way to measure the quality of a group of search results. It is based on the following hypotheses:

OK, Time for some Math ! Lets consider a hypothetical set of 10 search results. Going from top to bottom and ranking each as follows:

(0 – Not Relevant, 1- Somewhat relevant, 2- Relevant, 3- Very Relevant) – Fair enough?

So according to the mathematical definition of cumulative gain, we need to sum up the relevancy values for the 10 results as follows:

So say hypothetically say if your relevance score for the top 10 in order of results top to bottom were (3, 2, 3, 0, 0, 1, 2, 2, 3, 0), you would end up with a CG of 16. But as you may have noticed that the second last result here has a Relevance score of 3 (Very Relevant) whereas the 4th is 0 (not relevant) … CG(cumulative gain) does not factor in the position of results which is important as we discussed in our hypotheses above. We need to penalize these highly relevant documents appearing lower in the search results. So effectively gain is accumulated starting at the top of the ranking and may be reduced, or discounted, at lower ranks. A typical way to discount is to reduce the graded relevancy logarithmically. Typical discount is 1/log (rank).With base 2, the discount at rank 4 is 1/2, and at rank 8 it is 1/3 etc..

So Discounted Cumulative Gain(DCG) is the is the total gain accumulated at a particular rank p.

Following our sample data above:

Now although DCG is getting is closer to where we want to get , there is still our 3rd conjecture, “Not all queries are equal!” which means that some are just harder than the others and will produce lower scores  than the easier queries. To mitigate this fact we need to instrument some form of normalization. To achieve this we need to scale the results based on the best seen result, so essentially calculate whats called iDCG or Ideal DCG.  Essentially done by sorting documents of a result list by relevance, producing the maximum possible DCG till position p, also called Ideal DCG (IDCG) till that position; Which makes the implementation a bit harder as we need to calculate the  iDCG before computing the nDCG for any given result.

The nDCG values for all queries can then be averaged to obtain a measure of the performance of a search engine’s ranking.So going back to our example above the iDCG = 3, subsequently making the nDCG values as : (1, 0.66, 0.63, 0, 0, 0.13, 0.24, 0.22, 0.32, 0)

Thus nDCG is then relative values on the interval 0.0 to 1.0 and can be scaled across queries to determine relevance.

Taking this for a test drive with a few sample queries across Google and Bing, was a good exercise in validating the equations above, and needless to mention which one was more relevant  🙂 . I am quite anxious to try this out in the Enterprise Search world where the dynamics are a bit different. (something for the next post). In the next post we will also look at other functions such as F-measure and MRR (Mean Reciprocal Rank) to measure search relevance.

There is a quote I like “Natural selection is anything but random.“; Defining and validating fitness functions that enable us to measure and account for relevance is critical factor in measuring and optimizing for survival.|||

Natural selection is a mechanism for generating an exceedingly high degree of improbability. When I came across the quote above by R. A. Fisher while reading a paper on probability theory, I couldn't help but think about the vast corpus of documents that contend for survival (relevance) for a search query. To add to that the ever increasing bar…