Inside Google, identifying risk tolerance is getting better. A centralized dashboard showing system reliability thresholds has been very useful. This has the added benefit that it’s a “living document” — it shows the reliability goals of systems, as well as their current behavior!

Easier said than done: Such an understatement. How do you reconcile business wanting 100% reliability with the realities of day to day failure. That’s what this is about.

If the idea of employees doing something that’s not their job, sounds familiar, that’s because it’s rife through our whole industry. Backend engineers fixing javascript bugs, QA engineers doing requirements gathering, salespeople ending up being scrum-masters.

These aren’t necessarily dysfunctions (not in the way we discuss earlier in the book about development/devops/SRE dysfunctions), but it does depend on the skill-set of the individuals.

Don’t be afraid to approach this procedure just because your job title is wrong: You can be objective and work out what reliability requirements are necessary. Make sure you document your rationale.

Each of these points will be addressed below:

There are two opposite ends of a spectrum. One end is “The system is to be always up, 100% reliability” — which we know is unworkable, and if we see this attitude we need to attempt to adjust it.

At the other end is complacency: A refusal to even measure how available the system is. This is worse than having a large error budget: it’s having an unknowable error budget.

So measuring the right thing is the first step, then working out on that scale what our users expect is step two.

This raises an interesting issue: and in this article and in future ones I will be using these acronyms consistently:

I define an SLA, or “Service Level Agreement” to be an “external availability target with a contract that stipulates penalties if we fail to deliver to the external target”

And an SLO or “Service Level Objective” to be “internal availability target”, with no contractual obligations.

Neither of these can exist without an SLI, or “Service Level Indicator”, which is the metric of how to define success or failure.

And now YouTube is such a massive product while we probably don’t have very interesting SLAs on YouTube uptime: We would certainly have reasonably strict SLOs internally to make sure we’re pushing videos (and Ads!) to our users reliably.

I used to run Ads Frontend! That was my first SRE team. I joined when it was at the tail end of when we had these planned outages. Let me flesh this out a bit:

The maintenance being conducted here was for moving where the database master replica was running from, either so we could prove that it could be moved, or so that datacenter maintenance could be done.

Note: this was a MySQL database, with 1 authoritative master, with geographically distributed read-only replicas.

As of late 2011, the master failover procedure had gotten so fast, that the outage instead of being an hour and conducted as a planned outage (i.e. disable the system and replace it with a ‘sorry’ page, move the master, bring the system back), we could just move the location of the master and suffer elevated error rates and latency for 5 or so minutes, after which everything went back to normal.

Most of the errors were on ‘write’ operations because reads were always done with the database replicas, not the current write-master.

Once we moved to the newer procedure, we started counting all failures and latency during these events against our error budget. Our users were happier because in aggregate the system was up for longer, and web systems can be quite resilient to transient back-end errors.

So simple. Translate reliability into numbers, compare those numbers. So complex to get right, except with the simplest systems.

To assist with the thought exercise: 99.9% is 8 hours downtime a year, 40 minutes a month, or 1 in every 1000 user action being an error.

If you think you can spend $900 and bring a non-trivial user facing system from an expected 40 minutes of downtime a month to 4 minutes a month, I am very interested, please leave a comment. And your résumé.

I don’t think this chapter has adequately explained and defined what ‘Availability’ means. But I think that’s fine, considering it takes me months of meetings to get a room full of people to agree on what it means.

You could likely write entire books on the topic of defining Service Level indicators.

When you think about the fact that AdWords was backed by a MySQL database for over a decade, you have to stop and think about the engineering effort that went into this.

Clearly serving AdWords ads directly out of the MySQL database is not a thing that will scale to the number of users that Google Search has.

Not to say that AdSense is slow: hundreds of milliseconds is actually very fast for page asset loading. And these latency goals were originally set when the 56k modem was still in widespread use.

This is just a useful comparison for where a user might still have a ‘good’ experience with one system being much slower than the other. Both in target and actual performance.

When users are happy with slower responses/delayed responses or transient-invisible errors, you can make a good argument for much looser error budgets.

As I personally didn’t have a great deal to say here about the Risk Tolerance of Infrastructure Services, I’ve elided that section, please click through to the SRE Book site to read the section about estimating risk for a service like Bigtable.|||

I am a Site Reliability Engineer at Google, annotating the SRE book in a series of posts. The opinions stated here are my own, not those of my company. This is commentary on the second part of…