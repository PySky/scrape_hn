I recently gave a talk at Google on the problem of aligning smarter-than-human AI with operators‚Äô goals:

The talk was inspired by ‚ÄúAI Alignment: Why It‚Äôs Hard, and Where to Start,‚Äù and serves as an introduction to the subfield of alignment research in AI. A modified transcript follows.

I‚Äôm the executive director of the Machine Intelligence Research Institute. Very roughly speaking, we‚Äôre a group that‚Äôs thinking in the long term about artificial intelligence and working to make sure that by the time we have advanced AI systems, we also know how to point them in useful directions.

Across history, science and technology have been the largest drivers of change in human and animal welfare, for better and for worse. If we can automate scientific and technological innovation, that has the potential to change the world on a scale not seen since the Industrial Revolution. When I talk about ‚Äúadvanced AI,‚Äù it‚Äôs this potential for automating innovation that I have in mind.

AI systems that exceed humans in this capacity aren‚Äôt coming next year, but many smart people are working on it, and I‚Äôm not one to bet against human ingenuity. I think it‚Äôs likely that we‚Äôll be able to build something like an automated scientist in our lifetimes, which suggests that this is something we need to take seriously.

When people talk about the social implications of general AI, they often fall prey to anthropomorphism. They conflate artificial intelligence with artificial consciousness, or assume that if AI systems are ‚Äúintelligent,‚Äù they must be intelligent in the same way a human is intelligent. A lot of journalists express a concern that when AI systems pass a certain capability level, they‚Äôll spontaneously develop ‚Äúnatural‚Äù desires like a human hunger for power; or they‚Äôll reflect on their programmed goals, find them foolish, and ‚Äúrebel,‚Äù refusing to obey their programmed instructions.

These are misplaced concerns. The human brain is a complicated product of natural selection. We shouldn‚Äôt expect machines that exceed human performance in scientific innovation to closely resemble humans, any more than early rockets, airplanes, or hot air balloons closely resembled birds.

The notion of AI systems ‚Äúbreaking free‚Äù of the shackles of their source code or spontaneously developing human-like desires is just confused. The AI system is its source code, and its actions will only ever follow from the execution of the instructions that we initiate. The CPU just keeps on executing the next instruction in the program register. We could write a program that manipulates its own code, including coded objectives. Even then, though, the manipulations that it makes are made as a result of executing the original code that we wrote; they do not stem from some kind of ghost in the machine.

The serious question with smarter-than-human AI is how we can ensure that the objectives we‚Äôve specified are correct, and how we can minimize costly accidents and unintended consequences in cases of misspecification. As Stuart Russell (co-author of Artificial Intelligence: A Modern Approach) puts it:

These kinds of concerns deserve a lot more attention than the more anthropomorphic risks that are generally depicted in Hollywood blockbusters.

Many people, when they start talking about concerns with smarter-than-human AI, will throw up a picture of the Terminator. I was once quoted in a news article making fun of people who put up Terminator pictures in all their articles about AI, next to a Terminator picture. I learned something about the media that day.

I think this is a much better picture:

This is Mickey Mouse in the movie Fantasia, who has very cleverly enchanted a broom to fill a cauldron on his behalf.

How might Mickey do this? We can imagine that Mickey writes a computer program and has the broom execute the program. Mickey starts by writing down a scoring function or objective function:

 $$\mathcal{U}_{broom} =

 \begin{cases}

 1 &\text{ if cauldron full} \\

 0 &\text{ if cauldron empty}

 \end{cases}$$Given some set ùê¥ of available actions, Mickey then writes a program that can take one of these actions ùëé as input and calculate how high the score is expected to be if the broom takes that action. Then Mickey can write a function that spends some time looking through actions and predicting which ones lead to high scores, and outputs an action that leads to a relatively high score:

 $$\underset{a\in A}{\mathrm{sorta\mbox{-}argmax}} \ \mathbb{E}\left[\mathcal{U}_{broom}\mid a\right]$$The reason this is ‚Äúsorta-argmax‚Äù is that there may not be time to evaluate every action in ùê¥. For realistic action sets, agents should only need to find actions that make the scoring function as large as they can given resource constraints, even if this isn‚Äôt the maximal action.

This program may look simple, but of course, the devil‚Äôs in the details: writing an algorithm that does accurate prediction and smart search through action space is basically the whole problem of AI. Conceptually, however, it‚Äôs pretty simple: We can describe in broad strokes the kinds of operations the broom must carry out, and their plausible consequences at different performance levels.

When Mickey runs this program, everything goes smoothly at first. Then:

I claim that as fictional depictions of AI go, this is pretty realistic.

Why would we expect a generally intelligent system executing the above program to start overflowing the cauldron, or otherwise to go to extreme lengths to ensure the cauldron is full?

The first difficulty is that the objective function that Mickey gave his broom left out a bunch of other terms Mickey cares about:

$$\mathcal{U}_{human} =

 \begin{cases}

 1&\text{ if cauldron full} \\

 0&\text{ if cauldron empty} \\

 -10&\text{ if workshop flooded} \\

 +0.2&\text{ if it‚Äôs funny} \\

 -1000000&\text{ if someone gets killed} \\

 &\text{‚Ä¶ and a whole lot more} \\

 \end{cases}$$

The second difficulty is that Mickey programmed the broom to make the expectation of its score as large as it could. ‚ÄúJust fill one cauldron with water‚Äù looks like a modest, limited-scope goal, but when we translate this goal into a probabilistic context, we find that optimizing it means driving up the probability of success to absurd heights. If the broom assigns a 99.9% probability to ‚Äúthe cauldron is full,‚Äù and it has extra resources lying around, then it will always try to find ways to use those resources to drive the probability even a little bit higher.

Contrast this with the limited ‚Äútask-like‚Äù goal we presumably had in mind. We wanted the cauldron full, but in some intuitive sense we wanted the system to ‚Äúnot try too hard‚Äù even if it has lots of available cognitive and physical resources to devote to the problem. We wanted it to exercise creativity and resourcefulness within some intuitive limits, but we didn‚Äôt want it to pursue ‚Äúabsurd‚Äù strategies, especially ones with large unanticipated consequences.

In this example, the original objective function looked pretty task-like. It was bounded and quite simple. There was no way to get ever-larger amounts of utility. It‚Äôs not like the system got one point for every bucket of water it poured in ‚Äî then there would clearly be an incentive to overfill the cauldron. The problem was hidden in the fact that we‚Äôre maximizing expected utility. This makes the goal open-ended, meaning that even small errors in the system‚Äôs objective function will blow up.

There are a number of different ways that a goal that looks task-like can turn out to be open-ended. Another example: a larger system that has an overarching task-like goal may have subprocesses that are themselves trying to maximize a variety of different objective functions, such as optimizing the system‚Äôs memory usage. If you don‚Äôt understand your system well enough to track whether any of its subprocesses are themselves acting like resourceful open-ended optimizers, then it may not matter how safe the top-level objective is.

So the broom keeps grabbing more pails of water ‚Äî say, on the off chance that the cauldron has a leak in it, or that ‚Äúfullness‚Äù requires the water to be slightly above the level of the brim. And, of course, at no point does the broom ‚Äúrebel against‚Äù Mickey‚Äôs code. If anything, the broom pursued the objectives it was programmed with too effectively.

A common response to this problem is: ‚ÄúOK, there may be some unintended consequences of the objective function, but we can always pull the plug, right?‚Äù

Mickey tries this, and it doesn‚Äôt work:

And I claim that this is realistic too, for systems that are sufficiently good at modeling their environment. If the system is trying to drive up the expectation of its scoring function and is smart enough to recognize that its being shut down will result in lower-scoring outcomes, then the system‚Äôs incentive is to subvert shutdown attempts. The more capable the system is, the likelier it is to find creative ways to achieve that subgoal ‚Äî e.g., by copying itself to the Internet, or by tricking the programmers into thinking it‚Äôs safer.

This isn‚Äôt to say that it‚Äôs impossible to shut down sufficiently capable AI systems; it‚Äôs just that we need to put in the extra work of deliberately designing systems to avoid searching for ways to avoid being shut down. If you‚Äôve written code that would try to resist attempts to shut it down, then you shouldn‚Äôt run the code even if you‚Äôre convinced it would fail ‚Äî there are better ways to spend those compute cycles than on adversarial policy searches.

Relevant safety measures that don‚Äôt assume we can always outthink and outmaneuver the system will need to modify the system‚Äôs objective function in some relevant way. We need to figure out how to formally specify objective functions that don‚Äôt automatically place the AI system into an adversarial context with the operators; or we need to figure out some way to have the system achieve goals without optimizing some objective function in the traditional sense. E.g., we can look for ways to limit the system‚Äôs capabilities to a small set of ‚Äúwhitelisted‚Äù domains that don‚Äôt include thinking about whether the system will be shut down.

One obvious solution is to try changing the score function to make reference to a suspend button ùêÅ:

 $$\mathcal{U}^\prime_{broom} =

 \begin{cases}

 1 \text{ if cauldron full } & \text{& } \textbf{B}\text{=OFF} \\

 0 \text{ if cauldron empty } & \text{& } \textbf{B}\text{=OFF} \\

 1 \text{ if robot suspended } & \text{& } \textbf{B}\text{=ON} \\

 0 \text{ otherwise}

 \end{cases}$$This says that when the button is on, the system‚Äôs goal changes from ‚Äúfill the cauldron‚Äù to ‚Äúbe suspended.‚Äù

It‚Äôs not clear how we could actually specify this goal ‚Äî if there are any loopholes in the way we‚Äôve specified ‚Äúsuspension,‚Äù then the system might try to exploit them to hedge its bets. E.g., it could fulfill the technical definition of ‚Äúsuspend operations‚Äù while tasking a child agent it built or subprocess it started with filling the cauldron. This would allow is to achieve both ‚Äúcauldron full‚Äù and ‚Äúrobot suspended‚Äù at once, just in case it‚Äôs wrong about whether the button has been pushed.

Let‚Äôs assume, however, that we‚Äôve found an airtight way to technically specify suspension. The problem we run into now is that we‚Äôve given the system an incentive to care a lot about whether the button is pushed. If it has any way to influence its operators‚Äô decision to push the button, or any other way to exercise more control, then it will jump on the opportunity and try to ensure the button is pushed, or ensure that it‚Äôs not pushed ‚Äî depending on whether it looks easier to suspend or to fill the cauldron.

And, surprisingly, the state of the art on this problem isn‚Äôt much more advanced than that. There‚Äôs a paper co-authored by me, Fallenstein, Yudkowsky, and Armstrong (‚ÄúCorrigibility‚Äú), and a related result by Orseau and Armstrong (‚ÄúSafely Interruptible Agents‚Äú), but not a lot more.

What we want is a way to combine two objective functions ‚Äî a default function for normal operation, and a suspend function for when we want to suspend the system to disk.$$\mathcal{U}_{normal}(act) =

 \begin{cases}

 1\text{ if cauldron full} \\

 0\text{ if cauldron empty}

 \end{cases}$$

$$\mathcal{U}_{suspend}(act) =

 \begin{cases}

 1\text{ if suspended} \hspace{0.8cm}\\

 0\text{ otherwise}

 \end{cases}$$We want our method for combining the functions to satisfy three conditions: an operator should be able to switch between the functions (say, by pushing a button); the system shouldn‚Äôt have any incentives to control which function is active; and if it‚Äôs plausible that the system‚Äôs normal operations could inadvertently compromise our ability to switch between the functions, then the system should be incentivized to keep that from happening.

So far, we haven‚Äôt found any way to achieve all three goals at once. As an example, we can try to remove any incentive on the system‚Äôs part to control whether its suspend button is pushed by giving the system a switching objective function that always assigns the same expected utility to the button being on or off:

 $$\mathcal{U}_{switch}(act) =

 \begin{cases}

 \mathcal{U}_{normal}(act) &\text{ if button=OFF} \\

 \mathcal{U}_{suspend}(act) + \theta &\text{ if button=ON}

 \end{cases}$$$$\theta = \max_{act} \mathbb{E}\left[\mathcal{U}_{normal}\mid act\right] ‚Äì \max_{act} \mathbb{E}\left[\mathcal{U}_{suspend}\mid act\right]$$Unfortunately, this approach makes it impossible to satisfy the third condition. It turns out that the above objective function is equivalent to setting the probability of the button being pressed to 0. The system won‚Äôt deliberately tamper with the button, but it will happily approve updates that delete any reference to the button as dead code, or build subagents that ignore the button.

Meeting these intuitively simple constraints turns out to be a nontrivial problem. This is a pattern that runs through a lot of questions in this space: traditional tools and concepts run into immediate safety problems that don‚Äôt turn up in conventional capabilities research.

Let‚Äôs take a step back and talk about what‚Äôs needed overall in order to align highly capable AI systems with our interests.

Here‚Äôs a dramatically simplified pipeline: You have some humans who come up with some task or goal or preference set that serves as their intended value function ùòù. Since our values are complicated and context-sensitive, in practice we‚Äôll need to build systems to learn our values over time, rather than coding them by hand. We‚Äôll call the goal the AI system ends up with (which may or may not be identical to ùòù) ùó®.

When the press covers this topic, they often focus on one of two problems: ‚ÄúWhat if the wrong group of humans develops smarter-than-human AI first?‚Äù, and ‚ÄúWhat if AI‚Äôs natural desires cause ùó® to diverge from ùòù?‚Äù

In my view, the ‚Äúwrong humans‚Äù issue shouldn‚Äôt be the thing we focus on until we have reason to think we could get good outcomes with the right group of humans. We‚Äôre very much in a situation where well-intentioned people couldn‚Äôt leverage a general AI system to do good things even if they tried. As a simple example, if you handed me a box that was an extraordinarily powerful function optimizer ‚Äî I could put in a description of any mathematical function, and it would give me an input that makes the output extremely large ‚Äî then I don‚Äôt know how I could use that box to develop a new technology or advance a scientific frontier without causing any catastrophes.

There‚Äôs a lot we don‚Äôt understand about AI capabilities, but we‚Äôre in a position where we at least have a general sense of what progress looks like. We have a number of good frameworks, techniques, and metrics, and we‚Äôve put a great deal of thought and effort into successfully chipping away at the problem from various angles. At the same time, we have a very weak grasp on the problem of how to align highly capable systems with any particular goal. We can list out some intuitive desiderata, but the field hasn‚Äôt really developed its first formal frameworks, techniques, or metrics.

I believe that there‚Äôs a lot of low-hanging fruit in this area, and also that a fair amount of the work does need to be done early (e.g., to help inform capabilities research directions ‚Äî some directions may produce systems that are much easier to align than others). If we don‚Äôt solve these problems, developers with arbitrarily good or bad intentions will end up producing equally bad outcomes. From an academic or scientific standpoint, our first objective in that kind of situation should be to remedy this state of affairs and at least make good outcomes technologically possible.

Many people quickly recognize that ‚Äúnatural desires‚Äù are a fiction, but infer from this that we instead need to focus on the other issues the media tends to emphasize ‚Äî ‚ÄúWhat if bad actors get their hands on smarter-than-human AI?‚Äù, ‚ÄúHow will this kind of AI impact employment and the distribution of wealth?‚Äù, etc. These are important questions, but they‚Äôll only end up actually being relevant if we figure out how to bring general AI systems up to a minimum level of reliability and safety.

Another common thread is ‚ÄúWhy not just tell the AI system to (insert intuitive moral precept here)?‚Äù On this way of thinking about the problem, often (perhaps unfairly) associated with Isaac Asimov‚Äôs writing, ensuring a positive impact from AI systems is largely about coming up with natural-language instructions that are vague enough to subsume a lot of human ethical reasoning:

In contrast, precision is a virtue in real-world safety-critical software systems. Driving down accident risk requires that we begin with limited-scope goals rather than trying to ‚Äúsolve‚Äù all of morality at the outset.

My view is that the critical work is mostly in designing an effective value learning process, and in ensuring that the sorta-argmax process is correctly hooked up to the resultant objective function ùó®:

The better your value learning framework is, the less explicit and precise you need to be in pinpointing your value function ùòù, and the more you can offload the problem of figuring out what you want to the AI system itself. Value learning, however, raises a number of basic difficulties that don‚Äôt crop up in ordinary machine learning tasks.

Classic capabilities research is concentrated in the sorta-argmax and Expectation parts of the diagram, but sorta-argmax also contains what I currently view as the most neglected, tractable, and important safety problems. The easiest way to see why ‚Äúhooking up the value learning process correctly to the system‚Äôs capabilities‚Äù is likely to be an important and difficult challenge in its own right is to consider the case of our own biological history.

Natural selection is the only ‚Äúengineering‚Äù process we know of that has ever led to a generally intelligent artifact: the human brain. Since natural selection relies on a fairly unintelligent hill-climbing approach, one lesson we can take away from this is that it‚Äôs possible to reach general intelligence with a hill-climbing approach and enough brute force ‚Äî though we can presumably do better with our human creativity and foresight.

Another key take-away is that natural selection was maximally strict about only optimizing brains for a single very simple goal: genetic fitness. In spite of this, the internal objectives that humans represent as their goals are not genetic fitness. We have innumerable goals ‚Äî love, justice, beauty, mercy, fun, esteem, good food, good health, ‚Ä¶ ‚Äî that correlated with good survival and reproduction strategies in the ancestral savanna. However, we ended up valuing these correlates directly, rather than valuing propagation of our genes as an end in itself ‚Äî as demonstrated every time we employ birth control.

This is a case where the external optimization pressure on an artifact resulted in a general intelligence with internal objectives that didn‚Äôt match the external selection pressure. And just as this caused humans‚Äô actions to diverge from natural selection‚Äôs pseudo-goal once we gained new capabilities, we can expect AI systems‚Äô actions to diverge from humans‚Äô if we treat their inner workings as black boxes.

If we apply gradient descent to a black box, trying to get it to be very good at maximizing some objective, then with enough ingenuity and patience, we may be able to produce a powerful optimization process of some kind. By default, we should expect an artifact like that to have a goal ùó® that strongly correlates with our objective ùòù in the training environment, but sharply diverges from ùòù in some new environments or when a much wider option set becomes available.

On my view, the most important part of the alignment problem is ensuring that the value learning framework and overall system design we implement allow us to crack open the hood and confirm when the internal targets the system is optimizing for match (or don‚Äôt match) the targets we‚Äôre externally selecting through the learning process.

We expect this to be technically difficult, and if we can‚Äôt get it right, then it doesn‚Äôt matter who‚Äôs standing closest to the AI system when it‚Äôs developed. Good intentions aren‚Äôt sneezed into computer programs by kind-hearted programmers, and coming up with plausible goals for advanced AI systems doesn‚Äôt help if we can‚Äôt align the system‚Äôs cognitive labor with a given goal.

Taking another step back: I‚Äôve given some examples of open problems in this area (suspend buttons, value learning, limited task-based AI, etc.), and I‚Äôve outlined what I consider to be the major problem categories. But my initial characterization of why I consider this an important area ‚Äî ‚ÄúAI could automate general-purpose scientific reasoning, and general-purpose scientific reasoning is a big deal‚Äù ‚Äî was fairly vague. What are the core reasons to prioritize this work?

First, goals and capabilities are orthogonal. That is, knowing an AI system‚Äôs objective function doesn‚Äôt tell you how good it is at optimizing that function, and knowing that something is a powerful optimizer doesn‚Äôt tell you what it‚Äôs optimizing.

I think most programmers intuitively understand this. Some people will insist that when a machine tasked with filling a cauldron gets smart enough, it will abandon cauldron-filling as a goal unworthy of its intelligence. From a computer science perspective, the obvious response is that you could go out of your way to build a system that exhibits that conditional behavior, but you could also build a system that doesn‚Äôt exhibit that conditional behavior. It can just keeps searching for actions that have a higher score on the ‚Äúfill a cauldron‚Äù metric. You and I might get bored if someone told us to just keep searching for better actions, but it‚Äôs entirely possible to write a program that executes a search and never gets bored.

Second, sufficiently optimized objectives tend to converge on adversarial instrumental strategies. Most objectives a smarter-than-human AI system could possess would be furthered by subgoals like ‚Äúacquire resources‚Äù and ‚Äúremain operational‚Äù (along with ‚Äúlearn more about the environment,‚Äù etc.).

This was the problem suspend buttons ran into: even if you don‚Äôt explicitly include ‚Äúremain operational‚Äù in your goal specification, whatever goal you did load into the system is likely to be better achieved if the system remains online. Software systems‚Äô capabilities and (terminal) goals are orthogonal, but they‚Äôll often exhibit similar behaviors if a certain class of actions is useful for a wide variety of possible goals.

To use an example due to Stuart Russell: If you build a robot and program it to go to the supermarket to fetch some milk, and the robot‚Äôs model says that one of the paths is much safer than the other, then the robot, in optimizing for the probability that it returns with milk, will automatically take the safer path. It‚Äôs not that the system fears death, but that it can‚Äôt fetch the milk if it‚Äôs dead.

Third, general-purpose AI systems are likely to show large and rapid capability gains. The human brain isn‚Äôt anywhere near the upper limits for hardware performance (or, one assumes, software performance), and there are a number of other reasons to expect large capability advantages and rapid capability gain from advanced AI systems.

As a simple example, Google can buy a promising AI startup and throw huge numbers of GPUs at them, resulting in a quick jump from ‚Äúthese problems look maybe relevant a decade from now‚Äù to ‚Äúwe need to solve all of these problems in the next year‚Äù √† la DeepMind‚Äôs progress in Go. Or performance may suddenly improve when a system is first given large-scale Internet access, when there‚Äôs a conceptual breakthrough in algorithm design, or when the system itself is able to propose improvements to its hardware and software.

Fourth, aligning advanced AI systems with our interests looks difficult. I‚Äôll say more about why I think this presently.

Roughly speaking, the first proposition says that AI systems won‚Äôt naturally end up sharing our objectives. The second says that by default, systems with substantially different objectives are likely to end up adversarially competing for control of limited resources. The third suggests that adversarial general-purpose AI systems are likely to have a strong advantage over humans. And the fourth says that this problem is hard to solve ‚Äî for example, that it‚Äôs hard to transmit our values to AI systems (addressing orthogonality) or avert adversarial incentives (addressing convergent instrumental strategies).

These four propositions don‚Äôt mean that we‚Äôre screwed, but they mean that this problem is critically important. General-purpose AI has the potential to bring enormous benefits if we solve this problem, but we do need to make finding solutions a priority for the field.

Why do I think that AI alignment looks fairly difficult? The main reason is just that this has been my experience from actually working on these problems. I encourage you to look at some of the problems yourself and try to solve them in toy settings; we could use more eyes here. I‚Äôll also make note of a few structural reasons to expect these problems to be hard:

First, aligning advanced AI systems with our interests looks difficult for the same reason rocket engineering is more difficult than airplane engineering.

Before looking at the details, it‚Äôs natural to think ‚Äúit‚Äôs all just AI‚Äù and assume that the kinds of safety work relevant to current systems are the same as the kinds you need when systems surpass human performance. On that view, it‚Äôs not obvious that we should work on these issues now, given that they might all be worked out in the course of narrow AI research (e.g., making sure that self-driving cars don‚Äôt crash).

Similarly, at a glance someone might say, ‚ÄúWhy would rocket engineering be fundamentally harder than airplane engineering? It‚Äôs all just material science and aerodynamics in the end, isn‚Äôt it?‚Äù In spite of this, empirically, the proportion of rockets that explode is far higher than the proportion of airplanes that crash. The reason for this is that a rocket is put under much greater stress and pressure than an airplane, and small failures are much more likely to be highly destructive.

Analogously, even though general AI and narrow AI are ‚Äújust AI‚Äù in some sense, we can expect that the more general AI systems are likely to experience a wider range of stressors, and possess more dangerous failure modes.

For example, once an AI system begins modeling the fact that (i) your actions affect its ability to achieve its objectives, (ii) your actions depend on your model of the world, and (iii) your model of the world is affected by its actions, the degree to which minor inaccuracies can lead to harmful behavior increases, and the potential harmfulness of its behavior (which can now include, e.g., deception) also increases. In the case of AI, as with rockets, greater capability makes it easier for small defects to cause big problems.

Second, alignment looks difficult for the same reason it‚Äôs harder to build a good space probe than to write a good app.

You can find a number of interesting engineering practices at NASA. They do things like take three independent teams, give each of them the same engineering spec, and tell them to design the same software system; and then they choose between implementations by majority vote. The system that they actually deploy consults all three systems when making a choice, and if the three systems disagree, the choice is made by majority vote. The idea is that any one implementation will have bugs, but it‚Äôs unlikely all three implementations will have a bug in the same place.

This is significantly more caution than goes into the deployment of, say, the new WhatsApp. One big reason for the difference is that it‚Äôs hard to roll back a space probe. You can send version updates to a space probe and correct software bugs, but only if the probe‚Äôs antenna and receiver work, and if all the code required to apply the patch is working. If your system for applying patches is itself failing, then there‚Äôs nothing to be done.

In that respect, smarter-than-human AI is more like a space probe than like an ordinary software project. If you‚Äôre trying to build something smarter than yourself, there are parts of the system that have to work perfectly on the first real deployment. We can do all the test runs we want, but once the system is out there, we can only make online improvements if the code that makes the system allow those improvements is working correctly.

If nothing yet has struck fear into your heart, I suggest meditating on the fact that the future of our civilization may well depend on our ability to write code that works correctly on the first deploy.

Lastly, alignment looks difficult for the same reason computer security is difficult: systems need to be robust to intelligent searches for loopholes.

Suppose you have a dozen different vulnerabilities in your code, none of which is itself fatal or even really problematic in ordinary settings. Security is difficult because you need to account for intelligent attackers who might find all twelve vulnerabilities and chain them together in a novel way to break into (or just break) your system. Failure modes that would never arise by accident can be sought out and exploited; weird and extreme contexts can be instantiated by an attacker to cause your code to follow some crazy code path that you never considered.

A similar sort of problem arises with AI. The problem I‚Äôm highlighting here is not that AI systems might act adversarially: AI alignment as a research program is all about finding ways to prevent adversarial behavior before it can crop up. We don‚Äôt want to be in the business of trying to outsmart arbitrarily intelligent adversaries. That‚Äôs a losing game.

The parallel to cryptography is that in AI alignment we deal with systems that perform intelligent searches through a very large search space, and which can produce weird contexts that force the code down unexpected paths. This is because the weird edge cases are places of extremes, and places of extremes are often the place where a given objective function is optimized. Like computer security professionals, AI alignment researchers need to be very good at thinking about edge cases.

It‚Äôs much easier to make code that works well on the path that you were visualizing than to make code that works on all the paths that you weren‚Äôt visualizing. AI alignment needs to work on all the paths you weren‚Äôt visualizing.

Summing up, we should approach a problem like this with the same level of rigor and caution we‚Äôd use for a security-critical rocket-launched space probe, and do the legwork as early as possible. At this early stage, a key part of the work is just to formalize basic concepts and ideas so that others can critique them and build on them. It‚Äôs one thing to have a philosophical debate about what kinds of suspend buttons people intuit ought to work, and another thing to translate your intuition into an equation so that others can fully evaluate your reasoning.

This is a crucial project, and I encourage all of you who are interested in these problems to get involved and try your hand at them. There are ample resources online for learning more about the open technical problems. Some good places to start include MIRI‚Äôs research agendas and a great paper from researchers at Google Brain, OpenAI, and Stanford called ‚ÄúConcrete Problems in AI Safety.‚Äù|||

