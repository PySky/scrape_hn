BenchmarkDotNet for benchmarks; Intel VTune Amplifier for low-level optimizations; mistakes and lessons from performance optimizations: from BCL API usage to advanced data structures, from bit hacks to making code CPU-friendlier.

This post is based on a real-world feature that is used under high-load scenarios. The feature identifies and filters unwanted bot traffic. It‚Äôs backed by the Aho‚ÄìCorasick algorithm, a string searching algorithm that matches many keywords simultaneously.

In this post we explore the domain area, the used algorithm and its original implementation. The post walks through a series of various performance optimization steps: from BCL API usage to advanced data structures, from bit twiddling hacks to addressing CPU cache misses. It also covers tools I usually use to analyze code.

If you find it interesting you can continue reading or jump to any of the sections:

I work for an advertising technology company. The comics shows the lowdown:

It‚Äôs a bit exaggerated but‚Ä¶ (sigh) yes, this is all about banners at the end, I‚Äôm sorry for this üòû

All websites receive bot traffic! Not a surprise, right? There were quite a few studies from all sides of the advertising business. For instance, the one from Incapsula shows that websites receive 50% of bot traffic in average. Another one from Solve Media shows that bots drive 16% of Internet traffic in the US, this number reaches 56% in Singapore. In general, commercials tend to reduce the numbers, for obvious reasons - a banner impression equals money. Academics and not so involved parties, in their turn, increase the numbers and spread panic, that‚Äôs the goal of a research after all. I believe truth is somewhere in the middle.

But, surprisingly, not all bots are bad, and some of them are even vital for the Internet. The classification could look like this:

We won‚Äôt cover black bots because it is a huge topic with sophisticated analysis and Machine learning algorithms. We will focus on the white and grey bots that identify themselves as such.

There‚Äôs no reason to show a banner for a bot, right? It‚Äôs pointless, waste of resources and money. What‚Äôs the most important is that clients don‚Äôt want to pay for that and our goal is to filter all bots out. There are several ways to identify bot traffic, e.g., IP addresses, behavior analysis, honeypots, etc. One of the ways that became a standard in the industry is to use a defined list of User Agent strings. Advertising companies are required to have it to be compliant to standards. Let‚Äôs take a look at an example.

My browser‚Äôs user agent string looks like this at the moment: Yes, I use Chrome on Windows 10.

One of the Google‚Äôs crawlers has the following user agent: As you can see it has keyword and a link with the information about itself. Google shares information on their bots and how to identify them.

There are various bot user agent lists available on the Internet for free. But‚Ä¶ There‚Äôs The Interactive Advertising Bureau (IAB) which ‚Äúis an advertising business organization that develops industry standards, conducts research, and provides legal support for the online advertising industry.‚Äù They maintain their own ‚ÄúInternational Spiders and Bots List‚Äù (which costs‚Ä¶ wait WHAT? $14000 for non-members??? Holy moly!) The list ‚Äúis required for compliance to the IAB‚Äôs Client Side Counting (CSC) Measurement Guidelines‚Äù. Oh, this is what ‚Äúdevelops industry standards‚Äù means; everything fell into place. It seems that we don‚Äôt have much choice here üòÄ

The bot list contains a list of string tokens that we can find in user agent strings. There are hundreds of those tokens. The simplified version looks like this.

All we need is to find any of those tokens in a user agent and, if there‚Äôs a match, filter the request out as it comes from a bot.

The feature is used in few high-load systems like Real-time bidding, Ad serving and some others. Yes, this is all about banners (sigh).

That‚Äôs basically it. Measurement is vital! It‚Äôs difficult to add anything to that.

Measurement is hard! Variety of versions, libraries, languages, OSes, hardware, tools only aggravate the situation.

Essentially you are interested in two levels, let‚Äôs call them macro and micro. On macro level, metrics and macro-benchmarks help you understand how your code works in production on real data and show the real impact of changes. On micro level, microbenchmarks are crucial. They provide fast feedback and increase confidence. They are like unit tests where performance is a feature. Microbenchmarking is hard!

Your development pipeline looks like the following:

Before any performance optimization (How fast?) we think about efficiency (How much work do we need to do?). Our goal is to check whether a user agent string contains any of the given tokens. We have several hundred tokens. We perform the check once per network request. We don‚Äôt need to find all patterns or to know which of them matched; we need to answer: yes or no. Basically, omitting all unnecessary details, our problem comes down to the multiple string matching problem.

Multiple string/ pattern matching problem is an important problem in many areas of computer science. For example, spam detection, filtering spam based on the content of an email is a very popular technique. Another application is plagiarism detection, using pattern matching algorithms we can compare texts and detect similarities between them. An important usage appears in biology, matching of nucleotide sequences in DNA is an important application of multiple pattern matching algorithms There‚Äôs application in network intrusion detection systems and anti-virus software, such systems should check network traffic and disks content against large amount of malicious patterns. Aaaaaaand we have banners‚Ä¶

There are several string searching algorithms and few of them work with a finite set of patterns. The most suitable for our needs is Aho‚ÄìCorasick algorithm. It was invented by Alfred V. Aho and Margaret J. Corasick back in 1975.

‚ÄúThe complexity of the algorithm is linear in the length of the strings plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches‚Äù We aren‚Äôt interested in the output and can stop when any match is found; the complexity is much better for us.

The algorithm was used in the utility (an early version of ). You can play with an animated version of the algorithm here.

BenchmarkDotNet is a powerful FOSS .NET library for benchmarking. It is like NUnit for unit tests, it provides fast feedback for code changes. I believe that it‚Äôs a must to have it in your solution even if you don‚Äôt write high-performance code.

‚ÄúBenchmarking is really hard (especially microbenchmarking), you can easily make a mistake during performance measurements. BenchmarkDotNet will protect you from the common pitfalls‚Ä¶‚Äù It supports Full .NET Framework, .NET Core, Mono, x86, x64, LegacyJit and RuyJIT, and works on Windows, Linux, MacOS. It has some useful diagnosers based on ETW events like GC and Memory allocation, JIT Inlining and even some hardware counters.

You can find documentation and how to use it on its website; review, give a star or even contribute on github.

PerfView is a general purpose performance analysis tool for .NET. It‚Äôs like a Swiss army knife and can do many things, from CPU and Memory profiling to heap dump analysis, from capturing ETW events to hardware counters like CPU cache misses, branch mispredictions, etc. It has an ugly interface but after few weeks you will find it functional. That‚Äôs what it‚Äôs called - ‚Äúfunctional‚Äù. I believe that PerfView is a great tool to have in your tool belt. It‚Äôs FOSS with the sources hosted on github.

Intel VTune Amplifier is a commercial application for software performance analysis. It supports many programming languages including C#. In my opinion, it‚Äôs the best tool for the low-level performance analysis on the market. It shows not only what code CPU executes but how it does that. It answers not only how long CPU executes something but why it takes that much time. It exposes hundreds of hardware! counters and registers. It has low overhead hence. You can read about it on the Intel website BTW, VTune Amplifier has pretty good documentation and explanation for all major metrics.

‚ÄúILSpy is the open-source .NET assembly browser and decompiler.‚Äù It is a great tool, simple and easy to use; very useful when you want to understand how C# compiler compiles your code.

To be fair, there are other free and great tools like dotPeek or JustDecompile. I prefer ILSpy because it‚Äôs FOSS (the sources on github).

WinDbg - the great and powerful! It is a powerful debugging and exploring tool for Windows. It can be used to debug user applications, device drivers, and the operating system itself. I use it to understand CLR internals, get assembly code, analyze process dumps or debug an ugly problem.

There are few extensions to help us with that:

I find the ‚ÄúDebugging .NET with WinDbg‚Äù document by Sebastian Solnica concise and good as an intro and a reference book.

To be fair, the feature and algorithm were implemented by another developer. My interest in this case lies mostly in the performance field. You can find the original algorithm code in this gist.

Let‚Äôs quickly walk through the code and review the hot path. We have the class that contains logic on how to build itself and traverse/ search for patterns. The hot path starts from the method. There‚Äôs an awkward nesting of methods. The keyword always makes me worry. Here‚Äôs the excerpt code:

The tree class consists of nodes. The class backed by for character keys and nested nodes, it stores its results in .

Basically this would be enough to start with.

Following the main principle, we want to have a reliable way to measure the performance and further code changes. BenchmarkDotNet will help us with that, it is as simple as installing the library via NuGet and creating a test method with a attribute.

A simple benchmark for a common user agent string could looks like this:

This means that we need only 6 microsecond to check a common user agent string against several hundreds of patterns. We can do ~150K calls per second on one CPU Core which is pretty fast and good enough. But can we do better?

During the code review, the attentive reader may have noticed that there‚Äôs the following code in the hot path:

We have two calls to the dictionary: one to check whether the dictionary has the key or not, and then we get the next node. But we all know that there is a single method that can do both at once: . Let‚Äôs fix that:

This was easy, almost 5% improvement just using proper API methods. Lesson learnt: know APIs of libraries you use. Let‚Äôs move to profiling.

Let‚Äôs start from the high-level analysis and try to understand how the code performs. PerfView is the best tool for the high-level general purpose analysis. What we need is to create an isolated console application that executes the code in a loop with close to production usage. Let‚Äôs launch PerfView and profile the application using its functional UI.

PerfView shows a lot of useful .NET related (and system wide) information. For example JIT and GC stats. For instance, we can take a look at the activity of the GC in the ‚ÄúGCStats‚Äù view under the ‚ÄúMemory Group‚Äù folder. If we open the view for our application it shows us that the GC is pretty busy allocating and cleaning garbage up:

Hmmm‚Ä¶ That‚Äôs not what I would expect. We iterate over a string and traverse a prebuilt Trie. Why would we ever need to allocate anything just to traverse the tree?? Luckily PerfView is able to trace allocation object stack traces. Let‚Äôs enable the ‚Äú.NET SampleAlloc‚Äù option and switch to the ‚ÄúGC Heap Net Mem stacks‚Äù view. If we take a look at the allocation stacktrace:

We find that we allocate an instance of class, which is in our case, in the method. Wait a second?! We all know that List‚Äôs enumerator is a . How is that possible to have a struct on the heap? Let‚Äôs go up the stack and find that out. The IEnumerable extension implementation:

Here we call the method to get an enumerator. The List‚Äôs implementation:

We create an instance of the Enumerator struct, cast it to the interface and return it as an interface. To make it clearer we need to dig deeper. ILSpy will help us here. Let‚Äôs launch ILSpy and review the IL code:

Indeed we clearly see the ing operation. The reason for the boxing is that a call to an interface method happens via a Virtual Method Table. The compiler doesn‚Äôt know the type behind the interface. A value type doesn‚Äôt have a virtual method table by nature; to obtain one it has to become a reference type with all its consequences like a header, method table, heap allocation.

Knowing this fact, the fix is quite easy, let‚Äôs get rid of the interface for the sake of the exact type and check for instead of .

Wow, that‚Äôs 2 times faster! We achieved that just joggling .NET internals. Lesson learnt: It‚Äôs important to understand how CLR works.

Now it‚Äôs time to find the bottleneck of the code. Let‚Äôs launch PerfView again and profile the application. At this time we are interested in the ‚ÄúCPU Stacks‚Äù view:

PerfView shows that the bottleneck is in the BCL data structure. This will stop most developers from further work. Dictionary (a hash table) is an awesome data structure. It‚Äôs generic, it‚Äôs fast, it‚Äôs efficient memory-wise. It was bestowed upon us from the above! üòá

But, out of curiosity, let‚Äôs take a look at how it works under the hood. The method, identified as the bottleneck, calls the method under the hood which looks like this:

The first thing is the call where is an implementation. That makes the call a virtual interface call which cannot be inlined. All the same with the interface call. The call stack of the hot path looks like the following in our case:

That‚Äôs intelligible. is a generic general purpose data structure, it must handle any type and any scale equally well. But we know all our types, hence we don‚Äôt need that generic solution.

Let‚Äôs just re-implement a classic hash table for the type and inline it into the class. All we need is an array for buckets which points to an array of values. Basically we removed all unnecessary code and flatten the call stack. The code in that case could look like this:

Yeah, that‚Äôs 1.6 time faster than the previous version. So far so good. Lesson learnt: BCL has general purpose code; it may be useful to re-implement some code for your needs.

Now we came to the point when it‚Äôs important to understand how CPU works to perform analyses and optimizations. Here‚Äôs a necessary picture to show how complex CPUs are:

Yes, a modern CPU is a complex beast and I‚Äôm not in a position to explain you how it works, especially within a blog post. I just want to give a starting point, from where you can start the journey. One of the best starting points is the ‚ÄúCentral processing unit‚Äù wikipedia page.

In few words, CPU is a message passing system with multiple cache layers. Accessing next cache layer is much slower than the previous one. CPU cores exchange messages to keep cache coherent. For example, cache layers and latency for my Intel i7-4770 (Haswell) are the following:

Essentially, CPU can be divided into the Front-end and the Back-end. The Front-end is where instructions are fetched and decoded. The Back-end is where the computation performed. That concept drives all CPU optimizations. For example the Front-end can rearrange the order in which instructions are executed; it can try to predict branches and speculatively push more instructions, even if they may not be needed. The Back-end in its turn has several parallel execution units, e.g., arithmetic logic unit (ALU), floating-point unit (FPU), load-store unit (LSU); It‚Äôs capable of executing few instruction per cycle.

There‚Äôs a question on Stack Overflow, a developer asks a pretty serious and interesting question: ‚ÄúHow to achieve the theoretical maximum number of operations per CPU cycle?‚Äù

But the answer is rather entertaining, another developer has achieved that but to measure CPU temperature üòÜ

His code looks like this:

And many more lines like that. That‚Äôs basically assembly code written in C++ that works directly with CPU registers and instructions. It is amazing how much power and control C++ gives you. The author warns you: ‚ÄúIf you decide to compile and run this, pay attention to your CPU temperatures!!! ‚Ä¶ I take no responsibility for whatever damage that may result from running this code.‚Äù

Further reading: ‚ÄúIntel 64 and IA-32 Architectures Software Developer Manuals‚Äù and ‚ÄúIntel 64 and IA-32 Architectures Optimization Reference Manual‚Äù are the most thorough manuals I‚Äôve seen. I haven‚Äôt read them all though and use them as a reference mostly.

At this point PerfView won‚Äôt show us any useful insight. It‚Äôs time for the heavy artillery - Intel VTune Amplifier! It is a very powerful instrument for low-level profiling. VTune has several predefined analysis types.

The Advanced Hotspots analysis is the best place to start from. Event-based sampling analysis that monitors all the software on your system including the OS. As the name says it‚Äôs useful to identify bottlenecks. We already identified the bottleneck using PerfView. Let‚Äôs continue.

The General Exploration analysis helps identify hardware issues affecting the performance. It collects a comprehensive list of CPU hardware registers available for analysis. It‚Äôs a good starting point when you do hardware-level analysis. It can help you understand how efficiently your code is executing. It provides a neat summary view, from where you can start analyzing issues. For our latest version it produced the following summary:

Memory Access analysis helps identify memory-related issues, like CPU cache misses, NUMA problems and bandwidth-limited accesses. It uses hardware event-based sampling to collect data for memory-related metrics: loads and stores, LLC Misses, L1/L2/L3/DRAM bound metrics, etc. The summary overview for our code looks like this:

As we can see from the both analyses, our code is memory bounded. There are a lot of CPU cache misses. Let‚Äôs try to understand why we have them in the first place, and then let‚Äôs think how to address it. I put comments in the code to explain the CPU cache misses:

Just to extend all that a bit further, .NET is a safety-first platform. This means .NET ensures that your code won‚Äôt access memory not intended for you. For instance in it must ensure that you won‚Äôt access memory outside of the array. To achieve this, .NET adds a range check that looks like . This means that, even if you access an element in the middle, it needs to load the beginning of the array where the is stored. It leads to more loads (and cache misses).

What can we do about all that? Hmm‚Ä¶ we can have just one array, can‚Äôt we? We can put our values into the bucket array. What about hash collision resolution then?

It turns out, there‚Äôs another method of collision resolution in hash tables called ‚ÄúOpen addressing.‚Äù With this method a hash collision is resolved by probing, or searching through alternate locations in the array. There are few probing approaches, e.g, Linear, Quadratic, Double hashing, etc. The most CPU cache-friendly method is the Linear one that just puts an element into the next available bucket.

Open addressing is not a silver bullet, of course, and has drawbacks. It is a poor choice for large size elements because they pollute cache. Its performance dramatically degrades when the load factor grows.

It‚Äôs all about trade offs. And it seems that Open addressing if the perfect choice for us: it‚Äôs cache-friendly, our value record is small (just a pointer), we can control the load factor. Let‚Äôs implement it and take a look at the benchmark results:

Yeah! We fell into nanoseconds zone. That‚Äôs more than 2 times faster! Good job! VTune also confirms that we aren‚Äôt memory bound anymore.

Performance optimization is an iterative process. Let‚Äôs take a look at the General Exploration analysis of the latest optimized version again:

We can spot that VTune Amplifier highlighted the Divider unit, it shows us that almost 50% of execution time spent there. Some arithmetic operations like division and square root take considerably longer than addition or multiplication. They performed by the DIV unit. Indeed, we have a modulo operation in the following code that calculates an index into an array of entries:

WinDbg will help us understand what it compiles to. The code above compiles to the following assembly code by RuyJIT:

instruction consumes considerably more CPU cycles than or for example. It can be from 20 to 100 cycles depending on CPU and register size.

VTune Amplifier gives the clue: ‚ÄúThe DIV unit is active for a significant portion of execution time. Locate the hot long-latency operation(s) and try to eliminate them. For example, if dividing by a constant, consider replacing the divide by a product of the inverse of the constant. If dividing an integer, see whether it is possible to right-shift instead.‚Äù Let‚Äôs replace our modulo operation with a well known bit hack; the modulo of powers of 2 can be replaced by a bitwise operation. Our code becomes like this:

So that we got rid of the operation and the benchmark results show almost 2 times improvement! Awesome! Interesting enough that VTune was quite precise about 50% time spent in the DIV unit.

You can find more interesting bit twiddling hacks on the Stanford university website.

I just realized that I made a huge mistake üòû I benchmarked and profiled a code in the tight loop like this:

The data structure, we are optimizing, is small and less than 32Kb that perfectly fits into L1 CPU cache. In a tight loop all the data resides in the L1 cache that makes it almost free to access (just few cycles). This hides all memory related issues and expose wrong bottlenecks. The code has completely different load profile in production. We access the data structure only once per network request. There is a bunch of other business logic around it, where we read, write and allocate a lot of data. All this means that both the L1 and the L2 CPU caches don‚Äôt have the data readily available. Most probably, even the L3 CPU has just a part of it, causing the CPU to stall while waiting for data.

Having said that, we saw a skewed picture and analyzed incorrect bottlenecks. For instance, the recent division optimization won‚Äôt be so useful in the wild, the CPU stalls and waits for the data to be transferred from RAM to L3, then to L2, then to L1 and finally to CPU registers.

Let‚Äôs take a moment and look at what we have. We built a tree; the tree consists of nodes. A node is a class, it‚Äôs stored somewhere on the heap. A node (hash table) contains an array of keys and value, that array is also stored on the heap. All those node classed and arrays scattered around the heap without a clear access pattern. Accessing them CPU needs to request the memory which could lead to a number of expensive cache misses.

What can we do here? We can make the CPU help us! Modern CPUs perform the data prefetching optimization to improve execution performance. CPU can load instructions or data to a cache before it is actually needed. We need to make the memory access pattern easy to reason about for the CPU. The easiest and usually the fastest pattern is the sequential access. Every tree can be put into array, right? Why don‚Äôt we put the whole tree into one array?

As a side note, there‚Äôs the Software based prefetching too. The compiler can issue the prefetch instructions for the CPU. Unfortunately RyuJIT doesn‚Äôt support that yet; but it‚Äôs OSS and, who knows, may be you are the one who will bring the support in.

All that justify us to go unsafe!

Since we are going unsafe, there is another interesting observation - the user agent string traversal code. The ‚Äúsafe‚Äù version of it looks like this:

The loop compiles to the following code:

What do we see here? Each instruction depends on the previous one, hence CPU cannot execute them in parallel. There‚Äôs a memory access with complex address calculation. JIT could definitely do a better job, right?

Here we have a simple memory access with no calculations. Some of the instructions can be pipelined and executed in parallel.

It gives us 2 times faster traversal in an isolated benchmark:

Having said all the above, without going into details, we flatten tree into an array. We can also compact the array in a way we couldn‚Äôt do in the managed environment i.e. store a flag/ data instead a reference to a node.

The code looks ugly and I‚Äôm not proud of it, but performance you know.

We‚Äôve made some improvement again. But‚Ä¶ how to measure these changes? Unfortunately we are stuck in the situation where microbenchmarking doesn‚Äôt show us the real picture and became useless. Also it‚Äôs quite difficult to measure and profile changes and their impact in the wild. The only way is to employ CPU hardware counters. We already identified the bottleneck as LLC (last-level cache) misses. We are going to monitor only this counter via VTune Amplifier Custom analysis. Intel VTune Amplifier allow us

As simple as selecting interesting hardware events out of hundreds of them:

As for the real-world load profile simulation, let‚Äôs do a trick: create an array that is larger than the L3 cache and traverse it before each iteration. This simple trick will clear the CPU cache. Yes, this additional code consumes almost all CPU time, so we need to measure counters for considerable amount of time to get reliable results.

Shows that we encountered more than 7 million LLC misses during our run.

Shows only 1.5 million LLC misses. We managed to reduce number of LLC misses by more than 4 times! Which is amazing! Number of CPU clocks spent in the code also dropped by 2 times. I think it‚Äôs safe to say that we improved performance in the wild by 2 times using the latest optimization.

To summarize the journey, we improved the performance by probably more than 20 times at the end. We used various techniques: from a simple API change to re-implementing the Dictionary data structure, from the Open addressing Hash table to the modulo bit hack, from admitting our own mistakes to making the algorithm more CPU-friendly.

What can we do next? Obviously, compacting the data structure even more will improve the performance because we would simply load less data. We can find the perfect hash function to have less hash collisions. We can do it programmatically as we know our data beforehand. If we study the data we find out that it‚Äôs ASCII only and most of the patterns consist of English characters with few exceptions. Therefore we can create bit masks for them and use bitwise operations. To evolve it further we could use SIMD (Single instruction, multiple data) instructions and check few input characters at once. But all that is another story.|||

