In this series we’ll get down and dirty with the foundation of all Internet traffic, the HTTP protocol, more specifically the latest version of it. We’ll start with the very basics of what a protocol is, reasons it needed to be updated, the origins of HTTP/2, its inner workings and finally how designers and developers will have to change the way they build applications in order to get the best out of the improvements that HTTP/2 offers.

This is going to be a 3 part series and in this part we will take a look at a brief history of it’s predecessor HTTP/1.1 and it's drawbacks. We will also see how HTTP/2 came or rather, is coming to be.

But let’s brush up on the very basics first.

A protocol is a set of rules governing how computers communicate with each other over a network. HTTP is one such protocol that has it’s own methods of how to format data before it’s sent and what to do with it once it’s received. But this is just over simplification since HTTP is much more than that.

For the first time since 1999, we have a new version of HTTP and it promises a faster and better Internet experience for everyone. HTTP/2 builds on the success of the SPDY protocol created by a couple of smart-ass Google engineers, which addressed many of the drawbacks of HTTP/1.1.

HTTP/2 can load entire webpages for you before you can say HTTP, provided a developer leverages it right. This is because you would have to probably undo many of the previous workarounds you used for HTTP/1.1 to get a page to load faster and more efficiently. It opens up a whole new world of opportunities to optimize our applications and improve performance drastically.

HTTP/1.1 is like the oldest child of the family although technically not, the one who’s parents put all their hopes and dreams upon, the one who’s more like an experiment. And he does good, he succeeds in life but he wasn’t ambitious enough. He didn’t take into account all the possibilities in life. If only if he had the freedom right from the start.

HTTP/1.1 developed at a time when the Internet was made up a handful of text documents with a limited number of extra resources. An average website today serves up resources of around 2 MB with over 100 individual resources required to display a page - a resource being anything from images to CSS or JavaScript. This trend will only continue to increase as websites and web applications get fancier and designers continue to unleash their creativity.

There’s no point in getting that new 1Gbps connection if there are still high levels of latency. HTTP Pipelining is an attempt to solve this problem which basically allows multiple HTTP requests to be sent in parallel over a single TCP connection instead of the standard single HTTP request per TCP connection.

But a lot of the browsers don’t support it out of the box or have withdrawn support for it due to problems such as Head of Line blocking. Consider a queue at a fast food joint. The person in front of you might be the guy who knows what he wants and places his order without a hitch or he could be that mom with annoying kids, each who wants something different and takes forever to decide - Head of Line blocking.

Sure you can send multiple requests at the same time but due to the limitations of HTTP/1.1, the order of responses received must be in the same order that the requests came. This poses a problem as some responses might take longer time than others causing the others to be blocked until the first request is served.

Since HTTP Pipelining didn’t work out too well and most browsers are limited to serving a single request over a TCP connection, browsers are forced to use multiple TCP connections instead. But using multiple TCP connections occupy a greater share of network resources, thus downgrading performance for other users on the same network. Today most browsers can support up to 6-8 TCP connections per domain.

In order to overcome the latency problems, web designers came up with various hacks that could reduce latency. These workarounds have been pretty effective and are accepted standards in the current web development scenario. But keep following them when HTTP/2 takes over and some of these hacks will be counter-productive.

Websites nowadays need to look visually appealing and an important resource for that purpose is images - tons of them. Each image request individually will mount to many requests which will lead to latency issues. So developers came up with a technique called spriting that combines all the images into a single image and then using CSS or JavaScript, extract an image and use it individually.

But this technique is pointless when a page on the website needs only one or two images out of the sprited image. Plus when cache eviction occurs, the entire sprited image will go away instead of letting the most commonly used images remain.

Again taking the concept of combining multiple files into a single one, most front-end developer tools nowadays allow you to combine multiple CSS and JS files into a single file. And like the previous case of Spriting, not all the data is always required for every page. Developers also find it an inconvenience as it just adds extra steps to getting a production ready build.

HTTP requests in the world of HTTP/2 are far cheaper and it’ll pay better to organize assets according to the pages they will be used on.

Since a single domain will support only upto 6-8 TCP connections in most browsers, using multiple domains to load assets can increase the number of open TCP connections that are possible. This technique is called Domain Sharding and can lead to better load times.

Certain static resources like images, CSS and JS files don’t require cookies so developers send these resources from cookie-less domains to save bandwidth and time.

We’ve gone over all the first world problems faced when using HTTP/1.1, like latency sensitivity, too many TCP connections, HTTP pipelining and head of line blocking. Now let’s take a look at how HTTP/2 came to be.

The Internet Engineering Taskforce (IETF) is the organization comprised of an open international committee of network designers, operators and researchers responsible for developing and promoting Internet standards and protocols are one of the things they look after.

They are largely responsible for the development of the series of memos called Request for Comments (RFC) which document everything Internet related from protocols like TCP and HTTP to various other best practices. During the summer of 2007, a task group was organised to create an update for the HTTP/1.1 specification. But the real discussions started late in 2012. The updating work was completed early 2014 and resulted in the RFC 7230 series.

Around the same time, a couple of Google engineers were working on their own version of an improvement over the HTTP/1.1 protocol.

In the year 2009, the first major step in creating the next version of the protocol was started by, who else but Google. SPDY (pronounced speedy) can be considered the experimental elder cousin of HTTP/2 with it’s primary goal being to reduce latency between pages. Specifically the project goals were,

SPDY also required an encrypted connection (HTTPS) between the client and server. SPDY didn’t replace HTTP, rather was a tunnel for the protocol that modified how existing requests and responses were sent. HTTP/2 builds on the success of SPDY which was a working proof of concept for what the IETF hoped for.

The SPDY/3 draft was used as the starting point for developing HTTP/2 and with little tweaks here and there it was made into the HTTP/2 draft. Both the protocols continued to evolve in parallel with SPDY acting as an experimental branch that was used to test new features and proposals before including it in the final HTTP/2 standards.

SPDY has now been deprecated but it will remain a martyr as it paved the way for the future of our internet as performance starts to matter more and more.

In the next part we will dive deep into the inner workings of the HTTP/2 protocol.|||

