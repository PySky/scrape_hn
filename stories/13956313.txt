So in my first post on chalk, I mentioned that unification and normalization of associated types were interesting topics. I’m going to write a two-part blog post series covering that. This first part begins with an overview of how ordinary type unification works during compilation. The next post will add in associated types and we can see what kinds of mischief they bring with them.

Let’s start with a brief overview of what unification is. When you are doing type-checking or trait-checking, it often happens that you wind up with types that you don’t know yet. For example, the user might write – you know that this has type , but you don’t know what that type is. To handle this, the compiler will create a type variable. This basically represents an unknown, to-be-determined type. To denote this, I’ll write , where the leading question mark indicates a variable.

The idea then is that as we go about type-checking we will later find out some constraints that tell us what has to be. For example, imagine that we know that must implement , and we have a trait that is implemented only for :

In order for this impl to apply, it must be the case that the self types are equal, i.e., the same type. (Note that trait matching never considers subtyping.) We write this as a constraint:

Now you can probably see where this is going. Eventually, we’re going to figure out that must be . But it’s not immediately obvious – all we see right now is that two types have to be equal. In particular, we don’t yet have a simple constraint like . To arrive at that, we have to do unification.

So, to restate the previous section in mildly more formal terms, the idea with unification is that we have:

We would like to process these unification constraints and get to one of two outcomes:

Let’s start out with a really simple type system where we only have two kinds of types (in particular, we don’t yet have associated types):

The first kind of type is type variables, as we’ve seen. The second kind of type I am calling “applicative” types, which is really not a great name, but that’s what I called it in chalk for whatever reason. Anyway they correspond to types like , , and even types like . Here the name is the name of the type (i.e., , , ) and the type parameters represent the type parameters of the type. Note that there may be zero of them (as is the case for , which is kind of “shorthand” for ).

So the idea for unification then is that we start out with an empty substitution and we have this list of unification constraints . We want to pop off the first constraint ( ) and figure out what to do based on what category it falls into. At each step, we may update our substitution (i.e., we may figure out the value of a variable). In that case, we’ll replace the variable with its value for all the later steps. Other times, we’ll create new, simpler unification problems.

OK, let’s try to apply those rules to our example. Remember that we had one variable ( ) and one unification problem ( ). We start an initial state like this:

The head constraint consists of two applicative types with the same name ( ), so we can convert that into a simpler equation, reaching this state:

Now the next constraint is of the kind , so we can update our substitution. In this case, there are no more constraints, but if there were, we would replace any uses of in those constraints with `String:

Since there are no more constraints left, we’re done! We found a solution.

Let’s do another example. This one is a bit more interesting. Imagine that we had two variables ( and ) and this initial state:

The first constraint is unifying two tuples – you can think of a tuple as an applicative type, so is kind of like . Hence, we will simplify the first equation into two smaller ones:

To process the next equation , we just update the substitution. We also replace in the remaining problems with , leaving us with this state:

We can do the same for :

Now we, as humans, see that this problem is going to wind up with an error, but the compiler isn’t that smart yet. It has to first break down the remaining unification problem by one more step:

And now we get an error, because we have two applicative types with different names ( vs ).

When describing the unification procedure, I left out one little bit, but it is kind of important. When we have a unification constraint like for some type , we can’t just immediately add to our substitution. We have to first check and make sure that does not appear in ; if it does, that’s also an error. In other words, we would consider a unification constraint like this to be illegal:

The problem here is that this results in an infinitely big type. And I don’t mean a type that occupies an infinite amount of RAM on your computer (although that may be true). I mean a type that I can’t even write down. Like if I tried to write down a type that satisfies this inequality, it would look like:

We don’t want types like that, they cause all manner of mischief (think non-terminating compilations). We already know that no such type arises from our input program (because it has finite size, and it contains all the types in textual form). But they can arise through inference if we’re not careful. So we prevent them by saying that whenever we unify a variable with some value , then cannot occur in (hence the name “occurs check”).

Here is an example Rust program where this could arise:

And indeed if you try this example on the playpen, you will get “cyclic type of infinite size” as an error.

In terms of how this algorithm is typically implemented, it’s quite a bit different than how I presented it here. For example, the “substitution” is usually implemented through a mutable unification table, which uses Tarjan’s Union-Find algorithm (there are a number of implementations available on crates.io); the set of unification constraints is not necessarily created as an explicit vector, but just through recursive calls to a procedure. The relevant code in chalk, if you are curious, can be found here.

The main procedure is , which unifies two types. It begins by normalizing them, which corresponds to applying the substitution that we have built up so far. It then analyzes the various cases in roughly the way we’ve described (ignoring the cases we haven’t talked about yet, like higher-ranked types or associated types):

This post describes how basic unification works. The unification algorithm roughly as I presented it was first introduced by Robinson, I believe, and it forms the heart of Hindley-Milner type inference (used in ML, Haskell, and Rust as well) – as such, I’m sure there are tons of other blog posts covering the same material better, but oh well.

In the next post, I’ll talk about how I chose to extend this basic system to cover associated types. Other interesting topics I would like to cover include:

Post any comments or questions in this internals thread.|||

So in my first post on chalk, I mentioned that unification and normalization of associated types were interesting topics. I’m going to write a two-part blog ...