Easy OpenCL Multiple Device Load Balancing and Pipelining For C#: Cekirdekler API Multi-device OpenCL load balancer and pipeliner for C# in few lines of code. Cekirdekler API is open-source C# OpenCL wrapper that makes load-balancing between multiple opencl-capable devices and adds pipelining to get more performance and lets users apply their genuine C99 codes on all GPUs, CPUs and even FPGAs in their system. The project was pushed to github a few weeks ago:  Simple usage cases in Unity Game Engine(computing on Vector3 arrays and primitive arrays with R7-240 GPU and CPU):  Generally for all thin wrappers of OpenCL, users are needed to implement all buffer copies and event handling themselves. This API takes care of that and users need only select what is to be done with simple API commands. Single line to declare a device or all devices or a sub-group of devices depending on their vendors or compute units ormemory sizes. Single line to declare number cruncher that holds OpenCL kernel that is written in C99 language and passed as a simple multi-line string. Single line to declare an array backed by a buffer in C++(optionally) or just gather user's C# array and enhance it. Single line to compute. The compressed files given in the beginning are for the lazy developers. They are built on a Celeron N3060 so don't expect miracles.  I advise you to visit github address I've given and download whole project and build on your computer, it's open-source after all. Thats the best way for performance and security. Just in case the compressed files are used: Cekirdekler.dll: add this as reference in your C# project. Then add usings like "using Cekirdekler;" and some of its sub-namespaces in code files you use it.   Cekirdekler.XML: this helps intellisense to tell you about methods and classes. KutuphaneCL.dll: needs to be in same folder with Cekirdekler.dll as it uses this with dllimport attributes. System.Threading.dll: you can download this from Microsoft's site too. This makes it able to run in .Net 2.0. Add this as reference too. Cekirdekler.Hardware: explicit device selection instead of selecting all of them Cekirdekler.Arrays: contains array wrappers as ClArray{generic} and ClFloatArray and similar. Let's assume developer needs to add value of PI to all elements of an array then he/she needs to write this:  this makes the addition happen using all GPUs and all CPUs in the system. By running the C99 codes in the string on the array elements. The parameter "1" in the compute method is compute-id which means the next time compute method with same compute id is reached, load balancer will trade some workitems between all devices to minimize the overhead of compute method.  The parameter "1024" in the compute method is the number of total workitems distributed to devices. If there are two GPUs in system, both devices start with 512 workitems each, then converge to a time-minimizing point later with more repeatations of compute. Default value for the workgroup(OpenCL's definition of memory-sharing smallest group of threads that run in same compute unit) size is 256. If one needs it be 64, When more than one buffers are needed in kernel code: they can be added from host-side like this: such that, the arrays f,g and h must be in the same order as in the kernel parameters with memory specifier. Then f is linked to a, g is linked to b and h is linked to c. If developer needs more performance, pipelining can be enabled: parameter with zero is the offset where compute begins so each thread gets a global id shifted by this amount. True value enables pipelining. It is false by default. When enabled, API partitions each device's workload into 4 smaller works and pushes them in an even-driven pipelined manner so these parts hide each others latencies. By default, pipeline type is event-driven. There is also a driver-controlled version which uses 16-command-queues without event synchronizations but with complete blobs(with their own read+compute+write combined) to hide much higher latencies between command queues. Driver-controlled pipelining is enabled by adding another true value: this needs more CPU threads to control all blobs' uploading and downloading datas. Some systems are faster with event-based pipelining with separated reads hiding separating computes or separated writes, some systems are faster with driver-controlled version. Number of blobs for the pipelining must be a minimum of 4 and has to be multiple of 4 and is 4 by default. This value can be changed with adding it after pipeline type: Sometimes even pipelining is not enough because of unnecessary copies between C# arrays and C++ OpenCL buffers, then one can adjust a field of array wrapper to make it zero-copy access: this immediately creates a C++ array inside, copies values of C# array to this C++ array and uses it for all compute methods it calls. If it will be used more than once, it will decrease GPU to Host access timings greatly. If developer needs to start with C++ rightaway, this creates and uses C++ arrays by default. There is also ClFloatArray that can be passed to this as initialization. API has these types for use arrays: float, double,int,uint,char,byte,long. ClNumberCruncher class automatically handles CPUs and iGPUs and any other RAM-sharing devices as a streaming-processor to gain advantage of zero-copy read/writes within kernel. This is useful especially with low compute-to-data scenarios as in this "adding PI" example in the beginning. Streaming option is also enabled for all devices by default so devices may not use dedicated memories.   When developer needs to disable this feature for discrete GPUs, a parameter needs to be given value of false: Developers can also choose devices for GPGPU in a more explicit way: the upper example selects all Intel platforms, then selects all devices in them that share system RAM, effectively selecting CPU and its iGPU, better for streaming data. There are a lot of different methods to pick devices depending on their specialities such as memory size, number of compute units and benchmarks(this in future versions). When performance is not satisfactory, buffer copies needs to be optimized carefully. Cekirdekler API by default behavior, copies all arrays to all device buffers, computes, reads partial results back from all devices. This duplicates some unused same data to all devices. When devices are needed to read only their own part from array, a field needs to be set: then, if device-1 computes %50 of array, then it reads only %50 of the array, device-2 reads the rest of it. Then after compute, both write results on array. There are several flags to inform the API about how buffers will be handled: "read" instructs the API that array will be read as a whole(unless partial is set) before compute. "write" instructs the API that array will be written on it but partially by eah device, opposite of partialRead. If there is only single device and an array needs to be computed many times without copying to/from host, all three fields are needed to be set to false.  Another important part of buffer handling is, "array elements per workitem" value. The API interprets this value equal for all workitems. For example, it there are 1024 workitems and each workitem loads, computes, writes only float4 type variables, it is developer's responsibility to choose "4" for the elementsPerWorkItem value while using float array on host side. the upper sample enables 4x number of elements to be copied to devices. If a device runs 400 workitems, that device now gets 1600 float elements from array. First workitem works on elements 0-3, second workitem works on elements 4-7 and so on. Getting some helpful info from console(will be file in future versions) is also easy: You can find more detailed info on wiki page of github repository. Important info: If total work is salt or sand, then load balancer is trading grains between devices. Grain size is equal to local workgroup size multiplied by pipeline blobs. If pipeline is disabled, then grain size is just local workgroup size. If grainsize is very small compared to global size, then it load balancing becomes finer grained. When there are M number of GPUs in system, global size must be a minimum of M * Grain size. Each device has to have a minimum of 1 grain(for example,256 threads). Devices can't totally sell all grains. Increasing pipelining increases grain size so makes it harder to load balance. Similar for buffers, now one needs to keep in mind: multiply everything with "numberOfElementsPerWorkItem"  of array to know how much data it copies. Example: 2k workitems shared to 2 devices, 1.5k and 512. If pipelining is activated with 4 blobs and workgroup size is 128, then these two devices can trade only 512 workitems and have minimum of 512 workitems. Very bad example of load balancing, grains are not fine. Now assuming kernel uses float16 but host side is given a byte-array(such as it came from TCP-IP directly), then developer needs to set "number OfElementsPerWorkItem" value of byte array to 16*4 because each float is 4 bytes and each workitem is using 16-floats structs. Kernels can be repeated or different kernels can be run consecutively by names, each separated by space or comma or semicolon or newline char "

" or minus char. (these repeats don't change load partitioning, profiles as one operation)(seems useful for only single device usage)  Edit-2: Example of O(N²) algorithm with high data re-use ratio but low compute-to-data ratio This opencl kernel does 4096*4*(1 subtraction + 1 addition + 1 multiplication) per workitem per compute.  Host side executes 4096 workitems so each compute method is doing 201M floating point operations and 537MB RAM access. When load balancer converges, iGPU completes most of the work as quick as 5ms which means 40.2 GFLOPs (%35 of max theoretical value) because compute-to-data ratio is low in the innermost loop. CPU cannot get even closer because CPU is also serving as scheduler for opencl devices.   CPU has 2 cores but 1 core is selected = 4 arithmetic logic units cores are chosen compute(equals to device partition cores).                                                        --------- Selected devices: #0: Intel(R) Celeron(R) CPU  N3060  @ .60GHz(Intel(R) Corporation)     number of compute units:      type:CPU      memory: .83GB #1: Intel(R) HD Graphics (Intel(R) Corporation)                      number of compute units:  12    type:GPU      memory: .52GB --------- Compute-ID:  ----- Load Distributions:  [50.0%] - [50.0%] ----------------------------------------------------- Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: , .48ms, workitems: , Device (stream): Intel(R) HD Graphics         ||| time: .15ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [35.9%] - [64.1%] ----------------------------------------------------- Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .94ms, workitems: , Device (stream): Intel(R) HD Graphics         ||| time: .16ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [25.0%] - [75.0%] ----------------------------------------------------- Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .38ms, workitems: , Device (stream): Intel(R) HD Graphics         ||| time: .43ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [17.2%] - [82.8%] ----------------------------------------------------- Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .98ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .07ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [12.5%] - [87.5%] ----------------------------------------------------- Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .71ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .75ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [9.4%] - [90.6%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .23ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .56ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [6.3%] - [93.8%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .08ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .73ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [4.7%] - [95.3%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .93ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .82ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .73ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .49ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [4.7%] - [95.3%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .55ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .33ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .61ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .24ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .28ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .82ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .85ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .15ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .78ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .82ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .12ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .98ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .66ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .47ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .75ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .97ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .52ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .36ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .79ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .36ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .02ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .07ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .05ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .79ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .47ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .99ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [3.1%] - [96.9%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .18ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .88ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [1.6%] - [98.4%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .74ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .58ms, workitems: , ----------------------------------------------------------------------------------------------------------------- Compute-ID:  ----- Load Distributions:  [1.6%] - [98.4%] ------------------------------------------------------ Device (stream): Intel(R) Celeron(R) CPU  N3060   ||| time: .93ms, workitems: Device (stream): Intel(R) HD Graphics         ||| time: .67ms, workitems: , ----------------------------------------------------------------------------------------------------------------- now same program with an FX8150 + R7-240(much stronger) system:  a stronger GPU beats a stronger CPU. This is probably caused by CPU implementation not having enough registers for all threads, also having less threads in-flight and having slower memory(also same memory used for API buffer copies). Example for streaming data with same host codes but different kernel and 4M workitems with 16M array elements: even a single CPU-core has comparable streaming performance to its iGPU If out of bounds, returns immediately after the console message and early-quits in future compute methods. Hidden the non-useful ClDevices constructor, device selection starts from ClPlatforms.all() and ends in devices____ named methods which return ClDevices when kernel parameters interpret different typed host arrays, element alignment conditions must be met   No need to put -1 to both number-of-cores and number-of-gpus parameters in ClNumberCruncher constructor when choosing devices explicitly(these parameters are only exist for implicit device selection now) Now array of user-defined structs can be wrapped by a ClArray of type byte, example: in the upper example, vertices is an array of Vector3(this is from a working Unity example). This automatically sets the "numberOfElementsPerWorkItem" property accordingly with the bytes per struct so no need to set it but can get it to see. Beware! OpenCL treats float3(kernel-side struct) differently for each vendor. So use Vector3 and similar 3D elements as pure floats and multiply the indexer by 3 and add 1 for y, 2 for z (x is already  + zero). Device to device pipelining feature added. This lets develoeprs use multiple GPUs concurrently on different kernels that are to be run consecutively. Double buffering is handled automatically to overlap all pipeline stages compute and data movement operations to hide their latencies. Here is its demonstration: https://www.youtube.com/watch?v=pNIBzQvc4F8 and here is the wiki page about it: https://github.com/tugrul512bit/Cekirdekler/wiki/Pipelining:-Device-to-Device Removed "Copy Memory" dependency, Now its even more adaptable to Unity Game Engine (still Windows). Added `normalizedGlobalRangesOfDevices(int id)`  and `normalizedComputePowersOfDevices()` to ClNumberCruncher to query some performance info from client code directly(without needing to set performanceReport flag) Here is a gif showing how the pipeline works: binding two stages together and creating the pipeline: pushing data to pipeline, getting result to an array: (pipeline.pushData( object[] { arrayToGetData }, object[] { arrayToReceiveResult })) { Console.WriteLine( ); Console.WriteLine( Extra client arrays for both inputs and outputs of pipeline" ); Console.WriteLine( ); } (pipeline.pushData()) { Console.WriteLine( ); Console.WriteLine( input of first stage and output of last stage are directly accessed" ); } now it computes (x+1)*5 for each element of array and uses two gpus concurrently, one per stage and moving data at the same time with help of double buffering. Each device has to have a minimum of 1 grain. Then, if any grains left, those are placed by load-balancer. Increasing local range(workgroup size or number of workitems per local-memory-sharing-group) also increases grain size Trading grains between devices are fast for the first 10 iterations, then smoothing is enabled so sudden spikes of performances of individual devices can't corrupt it. Global offset parameter doesn't affect number of workitems or any workitem based bounds check but it affects array out of bounds and developer/user needs to check it before running(probably in cluster) Note: there are some classes that have "cluster" in their names, those are in prealpha stage and works unoptimized way and not translated to english(yet). The global offset parameter was being used by those classes.  Note2: number cruncher object allocates 16 queues, which may not be appropriate for some devices and may give CL_OUT_OF_RESOURCES or CL_OUT_OF_HOST_MEMORY even if RAM is not full. Works for AMD and INTEL, didn't try with NVIDIA. I didn't even get close to any FPGA, I'd like to. I heard their opencl compile times are hours!  Note3: ClNumberCruncher doesn't check if compiler is available, so, ClNumberCruncher will be added that control logic in future so it will be more thread-safe. For now, build all ClNumberCruncher instances serially, with locks. Compute methods of different ClNumberCruncher instances are also thread-safe but same instance can't be used in different threads for compute method. Note4: All devices, platforms, everything releases their C++(unmanaged) objects upon destruction so user may not need to dispose() them ever(unless some tight memory control is needed) For latest version, please visit github repository and feel free to add an "issue" if you have a problem related to Cekirdekler API. If you have written a complete image-resizer with this, you will have instant-speedup whenever you put another GPU into case, whether it is same vendor or not, same tier or not. Even overclocking one of the cards will have positive effect on performance, if the image is big enough to have a finer-grained load balancing. Will keep adding more here after each new feature added in github. This article, along with any associated source code and files, is licensed under The GNU General Public License (GPLv3) 

 You may also be interested in... You must Sign In to use this message board. github updated to v1.1.7, will update this article when it gets to v1.1.8 Because it puts too much work on reviewers just to update a download file and 1-2 extra feature tutorial.

 Warning: when I update article, I delete old download files, which causes them be unable to be downloaded So you must wait until review is accomplished by admins or moderators or reviewers.

 

 Next time, I won't delete old download files until it has updated at least two times already.

 Don't hesitate to criticize(or complain). I'll make my best to answer and file the issue on github and solve there Also thank you for your precious time.

 Use Ctrl+Left/Right to switch messages, Ctrl+Up/Down to switch threads, Ctrl+Shift+Left/Right to switch pages.|||

