When developing and deploying web services, apps or sites the following questions come up: "How will it perform?", "How many concurrent users will it support?", "If I tweak this setting, will it be faster?", "Do these new features effect performance?". The list could go on and on and on. Performance questions are common, solid answers are not.

Performance testing can take many different shapes, from dead-simple one-liners to complex setups, tests, tear-downs and analysis. While this article focuses on quick, easy and straightforward testing, future articles will address more advanced topics.

There are some great easy tools to get first ball-park answers to performance questions getting at the number of concurrent users as well as how the response time changes as load increases. Here I'll give a short intro to .

Let us begin with setting up some basic terminology, first let's refer to our machine under test as the , this can be any kind of http-accessible server you have. Second we will want an machine to drive our tests from. 

 When performance testing, it is key to limit the number of possible variables which could distort our results. Ideally your is a separate dedicated machine and as close as possible (network distance wise) to you system in order to minimize the amount of networking you test. This is especially relevant when you are testing applications hosted in a shared environment (ie cloud). The performance impact of noisy neighbors can be surprising, but that is a topic we will explore in detail in the future.

ApacheBench is a command line tool ( ) which allows for simple load driving against HTTP hosts. It's great at producing large numbers of requests, capable of producing thousands of requests per second. Generally I find ApacheBench most useful for getting a rough idea of how many requests an application can handle. It's extremely simple to use and therefore a great tool while debugging configurations.

The above section should be plenty to get you started, but lets look at a quick example of testing caching. Below I've setup a simple server example, which on request calculated a random Fibonacci number between and .

Now let's see how we do performance wise. We set the concurrency to 1 using and specify the number of requests to 500 by setting . Note that we are using the simple flask dev-server, which is single threaded.

In the above example, we see the average request time was , the median was and we had (requests per second). What happens if instead of a single connection we have 10 concurrent connections (setting )?

While the RPS remains very similar to before at , our response times have gone through the roof (mean of and median of )! Here we have a situation, where multiple parallel connections get serialized and processed one at a time, while the overall rate remains unchanged, the quality of service delivered to each client degrades by a factor roughly similar to the number of concurrent connections.

Let's see if we can do better. Since we repeatedly calculate the same 30 Fibonacci numbers, let's add some caching into the mix. Generally, if you have long-running requests that will always return an unchanging value, it is a good idea to cache these. With the caching in place, the first few requests will still have the same slow response time, but all of the following requests will benefit from the cache and therefore be as fast as our cache lookup. See the modified code below:

Now let's run our single connection, 500 request benchmark again:

The results are quite impressive: the mean is down to , the median down to and the request rate is at ! That is 10x faster. Once the cache is initialized and our benchmark no longer includes the cache seeding time, we get even higher performance, which at this point is likely limited only by the cache-lookups. My local testing gets me up to with median and mean both less than . While this is a simple example for demonstration, it is important to note that part of what we are seeing is a misleading flaw in how most load driving tools generate requests and record latencies, this is known as the coordinated-omission problem, but that is a topic for another day.

This concludes our short introduction into performance testing, but soon to follow will be more articles addressing more complex setups, benchmarking methods and types, metrics to be evaluating and considerations for repeatability.|||

Introduction to performance testing, using ApacheBench to load test a simple Flask server and optimizing it's performance.