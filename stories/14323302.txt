An experimentation framework for Reinforcement Learning using OpenAI Gym, Tensorflow, and Keras.

OpenAI Lab is created to do Reinforcement Learning (RL) like science - theorize, experiment. It provides an easy interface to OpenAI Gym and Keras, with an automated experimentation and evaluation framework.

With OpenAI Lab, we could focus on researching the essential elements of reinforcement learning such as the algorithm, policy, memory, and parameter tuning. It allows us to build agents efficiently using existing components with the implementations from research ideas. We could then test the research hypotheses systematically by running experiments.

Read more about the research problems the Lab addresses in Motivations. Ultimately, the Lab is a generalized framework for doing reinforcement learning, agnostic of OpenAI Gym and Keras. E.g. Pytorch-based implementations are on the roadmap.

To see their scores against OpenAI gym environments, go to Fitness Matrix.

Next, see Installation and jump to Quickstart.

If you plan to commit code, fork this repo then clone it instead.

Run the following commands to install:

For quick repeated setup on remote servers, instead of these commands, run the equivalent setup script:

Run . This will create the config files from template, needed for lab usage:

The Lab comes with experiments with the best found solutions. Run your first below.

Run the single best trial for an experiment using lab command:

Alternatively, the plain python command invoked above is:

Then check your folder for graphs and data files.

The grunt command is recommended as it’s easier to schedule and run multiple experiments with. It sources from , which should now have ; more can be added.

This trial is the best found solution agent of solving . You should see the Lab running like so:

Next step is to run a small experiment that searches for the best trial solutions.

This is under under in . The experiment studies the effect of varying learning rate and the DQN neural net architecture . If you like, change the to try more values.

Then check your folder for graphs and data files.

The experiment will take about 15 minutes (depending on your machine). It will produce experiment data from the trials. Refer to Analysis on how to interpret them.

We find it extremely useful to have data file-sync when running the lab on a remote server. This allows us to have a live view of the experiment graphs and data on our Dropbox app, on a computer or a smartphone.

For auto-syncing lab we use Grunt file watcher for automatically copying data files to Dropbox. In your dropbox, set up a shared folder and sync to desktop.

Experiments take a while to run, and we find it useful also to be notified automatically on completion. We use noti, which is also installed with .

Set up a Slack, create a new channel , and get a Slack bot token.

Notifications from the lab running on our remote server beast.

For setting up your own hardware, especially with a GPU, googling will help more than we could. Also, setup is usually non-trivial since there’re so many moving parts. Here’s the recommended references:

To understand the Lab’s Framework and Demo, skip to the next section.

The general flow for running a production lab is:

Grunt will read off the JSON file in , which looks like:

We use Grunt to run the lab - set up experiments, pause/resume lab, run analyses, sync data, notify on completion. Internally runs the command (harder to use), logged to stdout as

The useful grunt commands are:

See below for the full Grunt Command Reference or the Python Command Reference.

If you’re using a remote server, run the commands inside a . That is, log in via ssh, start a screen, run, then detach screen.

Since a remote server is away, you should check the system status occasionally to ensure no overrunning processes (memory growth, large processes, overheating). Use (already installed in ) to monitor your expensive machines.

Experiments take a long time to complete, and if your process gets terminated, resuming the lab is trivial with a flag: . This will use the :

The is created in the last run that maps s to s, and resume any incomplete experiments based on that . You can manually tweak the file to set the resume target of course.

By default the command (no task or flag) runs the lab in mode using .

The Python command is invoked inside under the function. Change it if you need to.

The experimental framework design and terminology should be familiar, since they’re borrowed from experimental science. The Lab runs experiments and produces data for analysis.

An experiment runs separate trials by varying parameters. Each trial runs multiple sessions for averaging the results.

An experiment is specified by an in .

How are organized and ran.

When the Lab runs an experiment with (e.g. ):

Given the framework explained above, here’s a quick demo.

Suppose we aim to solve the CartPole-v0 problem with the plain DQN agent. Suppose again for this experiment, we implement a new agent component, namely a policy, and try to find the best parameter sets for this new agent.

The example below is fully specified in under :

Specifically of interests, we have specified the variables:

Given , this experiment will try all the discrete combinations of the , which makes for trials. Each trial will run a max of 5 sessions (terminate on 2 if fail to solve). Overall, this experiments will run at most sessions, then produce and the analytics.

The example workflow to setup this experiment is as follow:

Now that you can produce the experiment data and graphs, see how to analyze them.

Once the Lab is running experiments, it will produce data. This section details how to analyze and understand the data, before we can contribute to the Solutions.

An experiment produces 3 types of data files in the folder :

We will illustrate with an example experiment from the dqn solution PR.

When an experiment is running, the lab will plot the session graphs live, one for each session.

The analysis graph is the primary graph used to judge the overall experiment - how all the trials perform. It is a pair-plot of the measurement metrics on the y-axis, and the experiment variables on the x-axis.

Each data point represents a trial, with the data averaged over its sessions. The points are colored (see legend) with the hue:

The granularity of the depends on the number of sessions ran per trial. From experience, we settle on 5 sessions per trial as it’s the best tradeoff between granularity and computation time.

Multiple sessions allow us to observe the consistency of an agent. As we have noticed across the parameter space, there is a spectrum of solvability: agents who cannot solve at all, can solve occasionally, and can always solve. The agents that solves occasionally can be valuable when developing an new algorithm, and most people will throw them away - this is bad when a strong agent is hard to find in the early stage.

Every subplot in the graph shows the distribution of all the trial points in the pair of y vs x variables, with the other x’ dimensions flattened. For each, observe the population distribution, y-positions, and trend across the x-axis.

Note that these will use swarmplot which allows us to see the distribution of points by spreading them horizontally to prevent overlap. However, when the x-axis has too many values (.e.g continuous x-values in random search), it will switch to scatter plot instead.

Population distribution: more darker points implies that the many trials could solve the environment consistently. Higher ratio of dark points also means the environment is easier for the agent. If the points are closer and the distribution has smaller vertical gaps, then the x is a stabler value for the y value even when other x’ dimensions vary. In a scatterplot, clustering of points in a random search also shows the convergence of the search.

trend across y-values: the fitter trial will show up higher in the y-axes (except for ). Generally good solutions are scarce and they show up at higher , whereas the non-solutions get clustered in the lower region. Notice how the plots can clearly distinguish the good solutions (darker points), whereas in the and plots it is hard to tell apart. We will discuss how the custom-formulated function achieves this in the metrics section.

trend across x-values: to find a stable and good x-value, observe the vertical gaps in distribution, the clustering of darker points. Usually there’s one maxima with a steady trend towards it. Recall that the plots flatten the other x’ values, but the dependence on x value is usually very consistent across x’ that there will still be a flattened trend.

The correlation graph reveals pairwise x-value correlations that is flattened in the analysis graph. This is a pair-plot between the orderable parameter variables.

The diagonals simply shows the population distribution for that x-value; the off-diagonal plots show the fitness score heatmap that tells how to best combine separate parameter values. Note that the heatmap color scheme does not represent absolute fitness, but points are colored by which they fall into.

The points are semi-transparent, so if they overlap, their colors will stack instead of hiding the points behind.

After glancing through the graphs, it will be easier to understand the data and find the targets.

The will show the data for each trial, sorted by the highest first. The left columns are the measured output values; then they’re separated by the ; the right columns are the parameter values for the trial.

The will tell us which to check for even more details on the best trials. Usually we can also spot some trend in the right parameter columns.

The best will show us directly what is its , and more stats about the trial. When submitting a solution PR, retrieve the to update the default , and get the from here too.

, top 5 trials, from the dqn-2017_03_19_004714 experiment. We can see that among the dominating parameter values are gamma=0.999, hidden_layers=[64], lr=[0.02]. The best trial json below.

This concludes the analysis. See the solution PR here. The best trial is , with , and the variables:

Now that you know how to analyze the data,

Agents and best solutions by OpenAI Lab users. We want people to start from working solutions instead of stumbling their ways there.

If you invent a new algorithm/combination that beats the best solutions, please submit a Pull Request to OpenAI Lab. Refer to the PR template for the submission guideline. See examples from the accepted solution PRs.

To learn how to analyze experiment data, refer to Analysis.

A matrix of the best of Agents v.s. Environments, sourced from the accepted solution PRs. See Metric for the design of fitness score and generalized metrics.

A projection of the Fitness Matrix along the Agents axis. This shows overall status of the Agents in OpenAI Lab. Feel free to invent new ones! For more details, see Algorithms and Families of RL Algorithms.

Pending: we have a generic formalization to cross-evaluate Agents using heatmap statistics; see Metrics. This is on the roadmap.

A projection of the Fitness Matrix along the Environments axis. This shows the best solutions for the environments. The list of accepted solutions can be seen in the solution PRs.

The Lab setup allows us to run experiments at scale; the standardized framework also allows us to reliably compare multiple agents (algorithms) and environments (problems). These are shown with the Fitness Matrix, which also necessitates a higher level evaluation metric.

With the Lab, we are breeding multiple agents across many environments and selecting the best ones. Naturally, this selection metric is called the . Some evolutionary search algorithm for the is on our roadmap.

The Fitness Matrix is a projection from the parameter space of each agent-environment pair, where each matrix cell is the highest fitness score the agent could achieve in the environment.

To understand the bigger picture, the domain for the fitness function for each matrix cell is the parameter space of the agent conditioned on the environment. Inside the parameter space, each point gets mapped to a fitness score.

To analogize, see fitness score as temperature, then we have a heatmap inside the parameter space, and we are searching for the hottest point and recording that in a cell of the Fitness Matrix.

In this section, we will formalize these ideas.

The fitness function is the base function behind the Fitness Matrix and the fitness heatmap. It computes the fitness score for each point (trial) in a parameter space.

The fitness function’s design is motivated by the need to have a richer evaluation of an agent, and the ability of the Lab to provide such data. We felt that the metric used in the OpenAI gym evaluation is insufficient, but it is included in our design under strength.

The fitness score of a trial is as follows:

or renaming variables by what the terms represent:

The fitness score is designed to capture the following:

The higher (more positive) the an agent gets, the stronger it is. The mean is taken over the past 100 episodes, as common to OpenAI gym.

Given two same , the agent that achieves it in less episodes is faster, with . This yields the notion of . Use the sessions-mean of a trial, i.e. = mean of multiple values.

for a session, with range from unstable to stable. Measures session-level stability. Use the sessions-mean of a trial, i.e. mean of multiple values.

The reasoning for the definition is simple: regardless if the problem is solved or not, we want to see if the agent could stability its performance. Once it solves (for solvable problems) or achieves a sufficiently high score (unsolved problems), its every episode should not drop too much, even when accounting for random fluctuations. We can define a minimal threshold (10% less than the ideal) that the subsequent episodes should surpass. Then, the stability is simple the ratio of the number of episodes that pass the threshold after the first solution.

for a trial, with range from inconsistent to consistent. This is the trial-level measurement of stability, as it measures how consistently the agent can solve an environment given multiple repeated sessions.

always for unsolved problems (unbounded rewards) since solution is undefined.

When or the multiplier will drop to zero, regardless if there is any partial solutions. This will make training harder as the agent will not learn from partial solutions if these are treated as non-solutions. So, restore the granularity that preserves partial solution simply by adding 1, i.e. and .

To separate solutions from noise, amplify the good ones and separate them out, while diminish and cluster the worse ones together. Amplify by session-level stability linearly with since it’s of the same order as . Amplify by trial-level stability quadratically with since trial stability is of the next order. Amplifier is always positive.

Always amplify to make better solutions have more positive fitness score. If is negative, amplify toward the 0 axis, i.e. divide by amplifier. If is positive, amplify away from the 0 axis, i.e. multiply the amplifier. Essentially, .

With the fitness function defined above, we can evaluate a single agent in a single environment. In fact, this is a single experiment with multiple trials, and we compute a fitness score per trial, using the trial’s parameter values. We then pick the max fitness score for the Fitness Matrix.

Given that the Lab can run multiple agents across environments in its standardized framework, naturally we ask:

This section formalizes the generalized metrics for these, and shows that the Fitness Matrix is just a max-function projection of some higher dimensional space.

We include the formalization of evaluation metrics across the Agent space and Environment space, and present a generalization, which produces the Fitness Matrix.

We’re using LaTeX for better symbolic rendering.

The currently implemented algorithms combine deep neural networks with a number of classic reinforcement learning algorithms. These are just a starting point. Please invent your own!

Reinforcement learning (RL) is learning from interaction with an environment, from the consequences of action, rather than from explicit teaching. RL become popular in the 1990s within machine learning and artificial intelligence, but also within operations research and with offshoots in psychology and neuroscience.

Most RL research is conducted within the mathematical framework of Markov decision processes (MDPs). MDPs involve a decision-making agent interacting with its environment so as to maximize the cumulative reward it receives over time. The agent perceives aspects of the environment’s state and selects actions. The agent may estimate a value function and use it to construct better and better decision-making policies over time.

RL algorithms are methods for solving this kind of problem, that is, problems involving sequences of decisions in which each decision affects what opportunities are available later, in which the effects need not be deterministic, and in which there are long-term goals. RL methods are intended to address the kind of learning and decision making problems that people and animals face in their normal, everyday lives.

For further reading on reinforcement learning see David Silver’s lectures and the book, Reinforcement Learning: An Introduction, by Sutton and Barto.

RL problems are characterized by incomplete information. The transition probabilities from one state to another given the action taken, for all states and actions are not known. Nor is the distribution of the rewards given a state and action. So in order to solve problems, RL algorithms involve approximating one or more unknown, typically complex, non linear functions. Deep neural networks make good candidates for these function approximators since they excel at approximating complex functions, particularly if the states are characterized by pixel level features.

For further reading on neural networks see Neural Networks and Deep Learning

To navigate the different RL algorithms:

See the overall list of agents and their implementation status under the Agents Fitness Matrix. This section briefly explains the theories behind the implemented algorithms/agents.

Q-learning algorithms attempt to estimate the optimal Q function, i.e the value of taking action in state under a specific policy. Q-learning algorithms have an implicit policy, typically -greedy in which the action with the maximum value is selected with probability and a random action is taken with probability . The random actions encourage exploration of the state space and help prevent algorithms from getting stuck in local minima.

Q-learning algorithms are off-policy algorithms in that the policy used to evaluate the value of the action taken is different to the policy used to determine which state-action pairs are visited.

It is also a temporal difference algorithm. Updates to the function are based on existing estimates. The estimate in time t is updated using an estimate from time t+1. This allows Q-Learning algorithms to be online and incremental, so the agent can be trained during an episode. The update to is as follows

For more details, please see chapter 6 of Reinforcement Learning: An Introduction, Sutton and Barto.

Since the policy that is used to evaluate the target is fixed (a greedy policy that selects the action that maximises the Q-value for a particular state) and is different to the policy used to determine which state-action pairs are being visited, it is possible to use experience replay to train an agent.

This is often needed for Agents to act in environments to get experiences. Experiences consist of the state, the action taken, the next state, and the reward, and are denoted as . These experiences are stored in the agents memory. Periodically during an episode the agent is trained. During training n batches of size m are selected from memory and the update step is performed. This is different to Sarsa algorithms which are on-policy and agents are trained after each experience using only the most recent experience.

Q-learning algorithm with two function approximators to address the maximisation bias problem, , and . One function is used to select the action in the next state, , the other is used to evaluate the action in state . Periodically the roles of each function are switched. Online training every experiences.

Deep Q-Learning algorithms tends to be unstable. To address this issue, create two function approximators, one for exploration, , and one for evaluating the target, . The target is a copy of the exploration network with frozen weights which lag the exploration network.

These weights are updated periodically to match the exploration network. Freezing the target network weights help avoids oscillations in the policy, where slight changes to Q-values can lead to significant changes in the policy, and helps break correlations between the Q-network and the target. See David Silver’s lecture slides for more details. Online training every n experiences.

algorithms also attempt to estimate the optimal function. They are on policy algorithms so the policy used to evaluate the target is the same as to the policy used to determine which state-action pairs are being visited.

Like Q-Learning, is a temporal difference algorithm. However, since they are on policy, it is trickier to take advantage of experience replay, requiring storage of the action in state and the Q-value for the state and action selected in in an experience. In the following implementations, updates are made after each action with the exception of off policy expected Sarsa.

This update is made each time the agent acts in an environment and gets an experience

Uses the expected value of the function under the current policy to construct the target instead of the Q-value for the action selected.

Sarsa is typically an on policy algorithm. However, if a different policy is used to evaluate the target than the one used to explore, it becomes and off-policy algorithm. With this set up, Q-Learning can be understood as a specific instance of Off Policy Expected Sarsa, when the policy used to evaluate the target is the greedy policy.

In off policy expected sarsa, actions are selected under the exploration policy, (annealling epsilon greedy for example). Then the value of next state and action pair is calculated as the expected Q value under the target policy, for example epsilon greedy with some fixed value for epsilon.

update and translation to neural network update: Same as with fixed epsilon.

Agents are containers for reinforcement learning algorithms. They consist of a number of different components which are specified in the .

To define an you must specify each of the components. The example below is from the specification for in . The files contains lots of other examples.

Each of the components with the exception of and are uncoupled, so can be freely switched in an out for different types of components. Different combinations of components may work better than others. We leave that up to you to experiment with. For some inspiration, see how the Lab users build the best solutions.

For the currently implemented algorithms, the following can go with the following .

Problems are not part of agent, but they are part of the that gets specified with the agent.

We have not added all the OpenAI gym environments AKA problems. If you get to new environments using the lab, please add them in , and it should be clear from those examples.

Moreover, when adding new problems, consider the dependencies setup too, such as Mujoco. Please add these to the so that other users could run it.

See the overall list of agents and their implementation status under the Agents Fitness Matrix. See algorithms section for an explanation of the currently implemented agents (learning algorithms).

The main class that will house all the other components. This is where the algorithms like are implemented. This class shall contain only the algorithm-specific logic; the generic components are modularized for reuse across different agents.

Despite the multitude of parameters you can see in the contructor of the Agent classes, s always need the following:

The input and output layer sizes are autoamtically inferred from the environment specs.

For HyperOptimizer to search over the network architecture (yes this is possible), use the auto-architecture mode for building the network. The HyperOptimizer will search over the network depth and width (2 parameters is sufficient to construct a whole network). To do so, set the param to true, and specific the and the .

A policy is a decision function for acting in an environment. Policies take as input a description of the state space and output an action for the agent to take.

Depending on the algorithm used, agents may directly approximate the policy (policy based algorithms) or have an indirect policy, that depends on the Q-value function approximation (value based algorithms). Algorithms that approximate both the policy and the Q-value function are known as actor-critic algorithms.

All of the algorithms implemented so far are value-based. The policy for acting at each timestep is often a simple epsilon-greedy policy.

Alternatively, an indirect policy may use the Q-value to output a probability distribution over actions, and sample actions based on this distribution. This is the approach taken by the Boltzmann policies.

A critical component of the policy is how is balances exploration vs. exploitation. To learn how to act well in an environment an agent must explore the state space. The more random the actions an agent takes, the more it explores. However, to do well in an environment, an agent needs to take the best possible action given the state. It must exploit what it has learnt.

Below is a summary of the currently implemented policies. Each takes a slightly different approach to balancing the exploration-exploitation problem.

Parameterized by starting value for epsilon ( ), min value for epsilon ( ), and the number of epsiodes to anneal epsilon over ( ). The value of epsilon is decayed linearly from start to min.

When actions are not random this policy selects actions by summing the outputs from each of the two Q-state approximators before taking the max of the result. Same approach as to decaying epsilon and same params.

Parameterized by the starting value for tau ( ), min value for tau ( ), and the number of epsiodes to anneal epsilon over ( ). At each step this policy selects actions based on the following probability distribution

Tau is decayed linearly over time in the same way as in the .

Same as the Boltzmann policy except that the Q value used for a given action is the sum of the outputs from each of the two Q-state approximators.

Same params as epsilon greedy policy. This policy swtches between active and inactive exploration cycles controlled by partial mean rewards and it distance to the target mean rewards.

Same params as epsilon greedy policy. Epsilon is decayed exponentially.

Same as epsilon greedy policy except at episode 18 epsilon is dropped to the max of 1/3 or its current value or min epsilon.

A policy has to have the following functions. You can create your own by inheriting from Policy or one of its children.

The agent’s memory stores experiences that an agent gains by acting within an environment. An environment is in a particular state. Then the agent acts, and receives a reward from the environment. The agent also receives information about the next state, including a flag indicating whether the next state is the terminal state. Finally, an error measure is stored, indicating how well an agent can estimate the value of this particular transition.

This information about a single step is stored as an experience. Each experience consists of

Crucially, the memory controls how long experiences are stored for, and which experiences are sampled from it to use as input into the learning algorithm of an agent. Below is a summary of the currently implemented memories.

The size of the memory is unbounded and experiences are sampled random uniformly from memory.

Parameterizes by param which bounds the size of the memory. Once memory reaches the max size, the oldest experiences are deleted from the memory to make space for new experiences. Experiences are sampled random uniformly from memory.

Like linear memory with sampling via a left-tail distribution. This has the effect of drawing more from newer experiences.

Experiences are weighted by the error, a measure of how well the learning algorithm currently performs on that experience. Experiences are sampled from memory in proportion to the p value (adjusted error value)

The parameter (if not positive, our implementation will bump it up) is a constant added onto the error to prevent experiences with error 0 never being sampled. controls how spiked the distribution is. The lower the closer to unform the distribution is. corresponds to uniform random sampling.

This has the effect of drawing more from experiences that the learning algorithm doesn’t perform well on, i.e. the experiences from which is has most to learn. The size of the memory is bounded as in LinearMemoryWithForgetting.

Memory sorted by the of each episode. Still experimental.

A memory has to have the following functions. You can create your own by inheriting from Memory or one of its children.

Controls how to optimize the function approximators contained within the agent. For feedforward and convolutional neural networks, we suggest using Adam with the default parameters for everything except the learning rate as this is widely considered to be the best algorithm for optmizing deep neural network based function approximators. For recurrent neural networks we suggest using RMSprop.

Stochastic Gradient Descent. Parameterized by (learning rate), , and . See Keras for more details.

Parameterized by (learning rate), , , , . See Keras for more details.

Parameterized by (learning rate), , , . See Keras for more details.

Controls how to search over the hyperparameter space. We suggest using each of the three hyperoptimizers in the following order when trying to find the optimal parameters for an agent in an environment, to help gradually narrow down the best search space. From rough to fine granularity, .

Varies one parameter per trial while using default values for the rest. This is useful when trying to estimate of what value range should work for a parameter without having to search through the full space blindly.

Given that we have built a proven base of best parameters, this is now seldom used, as we often could know the optimal parameter values to a small range.

Once the search space is narrowed down, it’s time to do a systematic grid search. This takes the cartesian products of the discrete list of options, i.e. every combination, and run trials. This is the most commonly used, and could help us properly see the heatmap when we probe the search space systematically.

After GridSearch, there are usually intermediate values between the grid points that could yield slightly higher fitness score. Narrow down even further to the best search space as shown in the GridSearch heatmap, and do a random search. This is the final fine-tuning. The RandomSearch algorithm is directional and greedy.

These are the future hyperparameter optimization algorithms we’d like to implement standalone in the Lab. The implementations for them currently exists, but they’re too bloated, and their engineering aspects are not ideal for the Lab.

All implemented hyperoptimizers shall extend the base class in and follow its design for compatibility. Below we show this design to be general theoretically and practically. Moreover, do not use bloated dependencies.

Note that the search space is a tensor space product of bounded real spaces and bounded discrete spaces . The search path in must also be well-ordered to ensure resumability.

1. for real variable, specify a distribution (an interval is just a uniformly distributed space) in the . Example:

2. for discrete variable, specify a list of the values to search over (since it is finite anyway) in the . This will automatically be sorted when read into to ensure ordering. Example:

The hyperopt implementation shall be able to take these 2 types of param_range specs and construct its search space.

Note that whether a variable is real or discrete can be up to the user; some variable such as can be sampled from interval or human-specified options . One way may be more efficient than the other depending on the search algorithm.

The experiment will run it as:

Sometimes preprocessing the states before they are received by the agent can help to simplify the problem or make the agent strong. One example is the pixel preprocessing the removes color channels and rescales image size, in order to reduce unnecessary information overload. The other is to concat states from sequential timesteps to present richer, correlated information that is otherwise sparse.

The change in dimensions after preprocessing is handled automatically, so you can use them without any concerns.

The default that does not preprocess, but pass on the states as is.

Concat the current and the previous states. Turns out this boosts agent performance in the environment.

Convert images to greyscale, downsize, crop, then stack 4 most recent states together. Useful for the Atari environments.

For agent-specific development, see Agents. This section details the general, non-agent development guideline.

The design of the code is clean enough to simply infer how things work by existing examples. The fastest way is to develop is to dig into the source code.

Check the latest under the Github Projects

See the full list of contributors here.

Note: we are not affiliated with OpenAI; OpenAI Lab is not tied to any organizations.

OpenAI we hope you find this useful!

This section is more casual, but we thought we’d share the motivations behind the Lab.

We the authors never set out to build OpenAI Lab with any grand vision in mind. We just wanted to test our RL ideas in OpenAI Gym, faced many problems along the way, and their solutions became features. These opened up new adjacent possibles to do new things, and even more problems, and so on. Before we knew it, the critical components fell it place and we had something very similar to a scientific lab.

The problems faced by us are numerous and diverse, but there are several major categories. The first two are nicely described by WildML’s Denny in his post Engineering Is The Bottleneck In (Deep Learning) Research, which resonates strongly with a lot of people.

1. the difficulty of building upon other’s work

If you have tried to implement any algorithms by looking at someone elses code, chances are it’s painful. Sometimes you just want to research a small component like a prioritized memory, but you’d have to write 90% of the unrelated components from scratch. Simply look at the solution source codes submitted to the OpenAI Gym leaderboard; you can’t extend them to build something much bigger.

Of many implementations we saw which solve OpenAI gym environments, many had to rewrite the same basic components instead of just the new components being researched. This is unnecessary and inefficient.

There is no design or engineering standards for reinforcement learning, and that contributes to the major inertia in RL research. A lot of times research ideas are not difficult to come by, but implementing them is hard because there is no reliable foundation to build on.

We patiently built every piece of that foundation because giving up wasn’t an option, so here it is. As the Lab grows, we hope that engineers and researchers can experiment with an idea fast by building on top of our existing components, and of course, contribute back.

2. the lack of rigor in comparisons

Denny describes this already, read his blog.

As the Lab became mature, we became more ambitious and try to solve more environment, with more agents. This naturally begs the question, “how do we compare them, across agents and environments?”

Multiple experiments running in the Lab will produce standardized data analytics and evaluation metrics. This will allow us to compare agents and environments meaningfully, and that is the point of the Lab’s Fitness Matrix. It also inspired a generalization of evaluation metrics, which we have only discovered recently.

When you’re heels down implementing an algorithm and the extra 90% side components from scratch, it’s hard to organize your work from a high level. Having to worry about other irrelevant components also makes you lose focus. The Lab removes that inertia and frees us from that myopic vision.

This freedom means more mental energy and time to focus on the essential components of research. It opens up new adjacent possibles and has us asking new questions.

With those problems above resolved, the Lab opens up the new adjacent possibles and allows us to do more. Below are some:

We think this framework is useful for two types of users, those who are new to RL and RL researchers / advanced RL users.

For users that are new to RL, there is a lot to get your head around before you can get started. Understanding the Open AI gym environments and how to work with them, understanding RL algorithms, and understanding neural networks and their role as function approximators. OpenAI Lab reduces the inertia to begin.

We provide a range of implemented algorithms and components, as well as solutions to OpenAI gym environments that can be used out of the box. We also wanted to give new users the flexibility to change as much as possible in their experiments so we parameterized many of the algorithm variables and exposed them through a simple JSON interface.

This makes it possible for new users to spend their time experimenting with the different components and parameter settings, understanding what they do and how they affect an agent’s performance, instead of going through the time consuming process of having to implement things themselves.

For advanced users or RL researchers, the Lab makes it easier to develop new ideas (a new way of sampling from memory for example) because a user needs to write only that code and can reuse the other components.

It also makes it easier to build on other peoples work. Solutions submitted via the lab’s framework encapsulate all of the code and parameter settings, so that the result can be reproduced in just a few lines of code.

Additionally, the experiment framework and analytics allows better measurement of their ideas, and standardization provides meaningful comparisons among algorithms.

We see the main new contributions/features of the Lab as follows:

1 - 4 have been covered in the sections above. Here we’ll focus on the Fitness matrix

The Fitness Matrix compares algorithms and environments. Tables playing a similar role to this are often seen in RL research papers from DeepMind or OpenAI.

Clearly it would take a lot for an individual to produce something similar - first to implement so many algorithms, then to run them across many environments, which would consume a lot of computational time.

We think the Fitness Matrix could be useful for RL researchers or new users as a benchmark to evaluate their new algorithms. An open source fitness matrix means everyone can use it instead of building one from scratch.

We see this as an extension of OpenAI gym’s Evaluation boards (example: CartPole-v0) in two ways: - First, the score is richer; it takes into account not just the speed to solve a problem and the maximum score, but also the stability and consistency of solutions. See Fitness Score. - Second, it makes comparison of environments and the comparison of agents across environments possible.

Our long term goal is to build a community of users and contributors around this framework. The more contributors, the more advanced the algorithms that are implemented and comparable wth the Lab. If successful, it could serve as the standardized benchmark for a large class of algorithms and a large class of problems. It becomes a public, living record which can be used by researchers who need to evaluate new algorithms and have no access to detailed benchmarks (unless if you’re Google/OpenAI).

With these metrics across multiple environments and agents, we can characterize, say, if an agent is strong (solves many problems); or if a generic component like Prioritized Memory Replay can increase the score of agent across all environments.

We also think the Fitness matrix could be used to further research, e.g. let us study cross-environment evaluation metrics, find patterns, and classify the characteristic of problems and agents. For example, we noticed that some classes of problems require spacial reasoning, or multi-step learning to solve; and there are agents who could or could not do that.

For the formulation of these measurements and more details, see generalized metrics. Such high level, cross-environment / agent analysis seems fairly new, and could prove useful to researchers. We would like to continue our work on this.|||

