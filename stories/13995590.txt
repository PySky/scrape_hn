The Amazon Echo is a breakthrough product. It is the first consumer product with a voice-centric interaction model, and it has really caught on. The number of skills (think apps… more on this below) available in the Alexa Skills store has increased 5X between Q2 and Q4 of 2016, and Amazon Echo sales have more than doubled between 2015 and 2016, from 2.4 million to 5.2 million.

SpinDance recently created an Alexa skill that enables consumers to control Whirlpool and Jenn-Air home appliances with their voices. Along the way we discovered a better way to design skills, resulting in less code and increased flexibility. Read on to learn how you can follow suit.

Alexa Skills are still relatively new, so before we explain the improved design methodology, here’s some background information on Amazon Alxa skills, and the traditional way of designing them.

It might help to think of Alexa skills as mobile apps. Just like mobile apps add features to mobile phones, Alexa skills add features to Alexa-powered devices, like the Echo, Echo Dot, or even a Kindle tablet.

Users can browse for skills in Amazon’s skill store, enable them, and then interact with their device by talking to it. Here are some examples:

These are just a few of the types of skills you can enable on your device. New ones are being added every week.

Now that you know what a skill is, let’s dive into how a developer would design one.

There are four things needed to create a skill:

The first three are defined in Amazon’s developer portal, while the last is done with custom server-side software.

The first thing to define is the intent schema. Working off of our Bob’s Pizza example above, an intent schema might look like this

Each intent schema has a name, and zero or more slots. The OrderPizzaIntent above has three slots. Its basic purpose is to define the events that an Alexa skill can respond to. It has a few components: intent and slots.

It’s also possible to define an intent that has no slots. It would look like this:

Each slot is composed of a unique name and a type. There are several built-in slot types, including AMAZON.TIME in the above example. AMAZON.NUMBER and AMAZON.DURATION are some other common ones. Amazon’s excellent documentation outlines these in detail.

In addition to built-in slot types, developers can, and often do, define custom slots. In our example, SIZES and TOPPINGS are examples of custom slots. Each custom slot is a list of one or more values:

The definitions above give a sense of how we would define the possible values for these custom slots. It’s important to remember that these are just suggestions: Amazon’s voice processing engine uses them to identify user input, but the final value is not guaranteed to match any of the developer-defined values. A user might say say “tiny” for a Size, or “sardines” for a Topping; those words would be passed along to your backend business logic.

Finally, we need to define a set of sample utterances. For our Bob’s Pizza example, our sample utterances would look like this:

When Alexa hears the user say “Alexa, ask Bob’s Pizza to deliver a large pepperoni pizza at 7pm tonight,” Amazon’s backend will parse the expression, and try to match it to one of the sample utterances in the Alexa skill’s definition.

To see how the intent schema, slot values and sample utterances are used, let’s break down how a phrase would be processed. “Alexa, ask Bob’s Pizza deliver a large pepperoni pizza at 7pm tonight.” There are three parts:

Part 1 is called the wake word. This cause the Amazon Echo to start processing to whatever the user says next.

Part 2 is called the skill invocation phrase. It allows the Amazon Echo to look up the proper skill to handle the voice event that is being processed.

Part 3 is the actual user intent. This information will be parsed and sent to the cloud application to perform the appropriate action and return a response to the user.

Here is a breakdown of how part 3 relates to the sample utterance above:

Once the Alexa service matches an intent and fills in the slots, the data is passed off to the backend logic to handle. This could be an AWS Lambda function, or a webhook hosted someplace else. The backend fulfills the user’s request, or returns an error message.

The above example included three sample utterances, but the reality is users will try many different ways to order a pizza. The differences may be subtle, but they will cause issues if the Alexa skill doesn’t have an adequate number of sample utterances. Here are a few examples:

In every case above the user wants the same thing, but as you can see there are many possible ways of expressing this request.

One option is to write out all the sample utterances you can think of. This is the approach we used in the OrderPizzaIntent above.

A lot of sample utterances would be needed to support all the subtle variations. It seems like the variations could go on forever, especially if we were dealing with a more complicated intents or combinations of intents. This is a good exercise, but it will leave the sample utterances very crowded and messy, not to mention difficult to maintain.

Now that you’ve seen a traditional way of building sample utterances, let’s show you a more compact example that will be easier to maintain. This technique will allow your Alexa skill to support exponentially more sample utterances, while reducing the number of lines in your sample utterance list: a great win-win.

The key is to create custom slots for information that will never be used by the backend business logic. In essence, there are two different kinds of custom slots:

Recall the example of ordering a pizza from Bob’s Pizza. We thought of a few examples of what a user might say above, and here they are again:

Look at the examples closely: each sample utterance has subtle variations, but the parts of speech in each sentence are essentially the same. Looking the sample utterances above we see:

We can make the most of this by defining our intent like this:

If we carefully define our custom slots, we can support all of the voice events above with a single sample utterance.

Now we need only one sample utterance:

This sample utterance will support all three expressions outlined above.

Our Alexa skill now supports all three expressions using only one sample utterance. As a bonus, it also supports any combination of the different parts of speech that we extracted from the original three examples.

Additionally, if users are using different words for pizza, it is easy to add support. Say, for example users start calling pizzas “pies.” We could add support by adding more values to the PIZZA_NAMES custom slot definition.

The main benefit of this approach is that it allows you to support exponentially more expressions with less code. The original design supported nine different expression variations (excluding the time slot) with three sample utterances.

After we redesigned it, the skill supported 81 expression variations with just one sample utterance. Adding one new Prefix or Article doubles the number of expressions the skill can understand. It shows how less can really be more when developing great Alexa skills. That’s a big win!

Another benefit is improved maintainability of sample utterances. For example, adding a new pizza name to the PIZZA_NAMES custom slot definition will automatically be applied to any intents that use the PIZZA_NAMES custom slot type.

No approach is perfect, of course. One drawback is that the new sample utterances are typically more abstract, and therefore more difficult to reason about. This can lead to increased complexity when testing the skill. Testing voice interfaces is a big topic, and we’ll be exploring that in future posts.

SpinDance is excited to see where voice interaction goes in IoT, and are looking forward to writing many more voice-enabled skills in the future.

Are you developing voice apps? We’d love to hear from you: drop us an email at hello@spindance.com.|||

SpinDance demonstrates a more flexible way to design and build Alexa Skills.