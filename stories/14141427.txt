This is our ongoing PyTorch implementation for both unpaired and paired image-to-image translation.

The code was written by Jun-Yan Zhu and Taesung Park.

Check out the original CycleGAN Torch and pix2pix Torch code if you would like to reproduce the exact same results as in the papers.

If you use this code for your research, please cite:

The test results will be saved to a html file here: .

The test results will be saved to a html file here: .

More example scripts can be found at directory.

Download the CycleGAN datasets using the following script:

To train a model on your own datasets, you need to create a data folder with two subdirectories and that contain images from domain A and B. You can test your model on your training set by setting in . You can also create subdirectories and if you have test data.

You should not expect our method to work on just any random combination of input and output datasets (e.g. ). From our experiments, we find it works better if two datasets share similar visual content. For example, works much better than . achieves compelling results while completely fails.

Download the pix2pix datasets using the following script:

We provide a python script to generate pix2pix training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene. For example, these might be pairs {label map, photo} or {bw image, color image}. Then we can learn to translate A to B or B to A:

Create folder with subfolders and . and should each have their own subfolders , , , etc. In , put training images in style A. In , put the corresponding images in style B. Repeat same for other data splits ( , , etc).

Corresponding images in a pair {A,B} must be the same size and have the same filename, e.g., is considered to correspond to .

Once the data is formatted this way, call:

This will combine each pair of images (A,B) into a single image file, ready for training.

CycleGAN: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks

 pix2pix: Image-to-image translation with conditional adversarial nets

 iGAN: Interactive Image Generation via Generative Adversarial Networks

If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection:

 [Github] [Webpage]|||

pytorch-CycleGAN-and-pix2pix - Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more)