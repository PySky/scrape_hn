I recently stumbled upon an interesting and straightforward data exploration made by David Robinson from StackOverflow: What programming languages are used late at night?. Among other fun facts about the programming crowd, he discovered that Haskell is different from mainstream language, the SO questions’ frequency grows much stronger in the evenings compared to other languages. It is intriguing to compare these observations with how people actually code on GitHub, and besides, our CEO Eiso Kant is fond of Haskell enough to let me check, he-he. So I decided to use my Open Source Friday to work on this post.

source{d} opens as much code and data as we can. In this particular case, we are going to use two public datasets that we published on data.world originating from our prev-gen data retrieval pipeline:

They are not feature complete, especially the second one, but should be sufficient for our purposes.

Besides the data sets, we need the computing resources to digest the datasets. I am using Google Cloud and will show how to apply Dataproc.

The first thing we shall do is to launch the smallest Dataproc cluster. I have a post about how to setup a working PySpark in Jupyter, it covers the basic stuff. The initialization script evolved since then.

We specify as the initialization script here. It should be accessible by everybody. It normally takes less than 5 minutes to get the cluster running and fully operational. The master and worker nodes will be prepared to our requirements. Particularly, the master node will have:

The next step is to download the datasets. data.world has a size limit which prevents from uploading files bigger than 50MB, so we decided to upload ours to Google Drive for ease of use.

and are Google Drive identifiers of the datasets, 452M commits on GitHub and GitHub repositories - languages distribution. It takes about 8 minutes to pull them (35 gigs).

Now we need to upload the files to either HDFS or Google Cloud Storage. I prefer the latter because it is persistent whereas HDFS disappears when the cluster is deleted (GCP documentation)

We are using the switch here for concurrent upload streams - they speed up the process dramatically. More information about gsutil.

When this command finishes, we are ready for executing a PySpark job to analyze the data.

Open (again, read the previous post how), you should see the list of your buckets thanks to src-d/jgscm Jupyter backend. Enter the one you wish to store the notebooks and start a new “PySpark 3”.

452M commits on GitHub has the following format:

The above is prettified json but in the file it is a single line. We would like to aggregate commits by repository, then by weekday and finally by hour. For example,

The tricky part is, of course, dealing with time. People commit with invalid timezones quite often. Besides, we do not want to mix users living in different countries… and we’ve only got “t”-s like

I am sure you can do better, but I decided to cut corners and analyze on the PST timezone; Silicon Valley and friends ;).

There is still an issue I need to deal with though: PST turns into PDT and back, and we need to keep track of the time offset in each part of the year. Anyway, here is the complete code:

Before running it, you would probably like to dynamically resize your Dataproc cluster.

My favourite Dataproc feature - dynamic cluster resizing using the cheap preemptible nodes.

The job normally takes about 20 minutes on a 32-node 4-core configuration. When it finishes, we must copy results locally to work with them:

For those of you who are lazy or don’t want to spend on the computational resources, I uploaded that file to Google Drive.

Of course, the best way to deal with the languages distribution in commits is to inspect diff-s individually, however, source{d}’s next generation pipeline is in the process of being built. Instead, we will consider all commits to the same repository equal and carry the constant language proportions belonging to that repository. Let’s load the second dataset:

This operation takes a while, around 6 minutes. If you’ve inflated the cluster in the previous section, you can now safely shrink it to the minimal volume - we are no longer using Spark.

The following code parses the file which was generated before and calculates the resulting commit frequency distribution for every language:

We could simply in the snippet above but it is always much slower compared to manual parsing. The variable stores the commit frequencies we desire. They are aggregated by weekday and then by hour. E.g. is the vector which shows how many commits were made in each language on Monday between 12am and 1pm (European locale everywhere).

And now the fun part. We will plot in the form of a circular histogram to reflect the daily cycle.

There are several tricks here that I apply:

We see that the programming languages can be divided into two highly distinguishable groups.

The first is “weekend languages” (on the left), in which we see new, emerging species as well as die-hards like Haskell. That group has nearly the same contribution activity throughout the whole week and no “lunch time” productivity fall. The larger the difference, the more emerging is the language, e.g. Elm and D.

So why do Haskell programmers ask relatively more questions on StackOverflow in the evening? We can see from the coding habit that Haskelistas become very active after 8 pm, almost the same level as on the general “productivity peak” at 3 pm.

The second group is mainstream, well established languages (on the right). We see the clear pitfall at lunch time, between 12 am and 2 pm, which is the evidence that those languages are used for work and not as a hobby. Besides, the weekend activity of those languages has very similar gaussian distribution as in the first group. One interesting observataion is that PHP programmers seem to code a lot in the night, sharp deadlines or a big involvement?

There is a productivity peak between 2 pm and 5 pm for all the languages, when the commit frequency is the highest. This is the industry’s golden time. Managers should never distract coders during this interval.

I would like to thank our infrastructure engineer, Sonia Meruelo, for the valuable ideas.|||

Building the first AI that understands code