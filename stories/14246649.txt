I started with creating Terraform code for AutoScaling Group first. My configuration is straightforward — a single instance in a single Availability Zone with few tags populated on launch. There are two important bits though: “CNAME” tag and “depends_on” setting. First one is necessary to find the right Route53 record to update for the newly created instance. It makes the code reusable for other AutoScaling Groups. “depends_on” guarantees the right order of created resources. I am using “aws_cloudwatch_event_target” so that the Lambda function and CloudWatch Rule were ready before AutoScaling spins up a first instance.

After creating the AutoScaling Group I was able to start working on my Python script that would update Route53. Amazon has a great documentation about how to write Lambda function in Python and what events AutoScaling is sending. Our Lambda function has to accept a JSON data and check if the event type is the one we are looking for. Next it needs to extract instance ID from the event, find its CNAME tag, and update Route53 entry. The code can be found here. Below you can see an example event that you can use to test your Lambda code.

In my opinion, the most difficult part. It took me a while to understand how Lambda function gets its credentials from IAM and what “AssumeRole” action was. Basically, it allows an application or a service to request temporary credentials from IAM for the specified role. It does that by calling Security Token Service (AWS STS) endpoint with “AssumeRole” action and ARN role. Returned credentials have permissions specified in a policy for the requested role.

CloudWatch is the glue that connects all the pieces together. It will receive events from AutoScaling group, filter them out, and trigger Lambda function. In my case, I am only interested in successful launch events from one particular AutoScaling Group so I need to specify these fields in my “event_pattern”.

As I said in the beginning, I wanted Terraform to upload my Lambda code. To do that, I am using simple bash commands called from “null_resource”.

It will install all Python dependencies to a local directory, update config file, zip everything together, and upload it to AWS.

You can checkout my code locally or use it directly from github as a module like below.|||

Recently I was looking at a way to automatically update Route53 records for instances created by an AutoScaling Group. Most people tend to choose Elastic Load Balancer to do that. But to me it felt…