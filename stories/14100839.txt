Mapping strings to things makes sense. You take a meaningful string and you assign it a value. As with most things we do in programming there are many pitfalls to approaching a problem. So, lets explore some of them!

Here’s a situation we’ve all run into at some point.

Say you’re reading records from a database and it has a column called “Type”. Lets say for some reason the designer wanted things to be a little bit human readable so this “Type” column is a . When you read this string in you need to parse it into some internal type within your application.

What method do you use?

and use the index as your enum type?

And indeed you would be right! Of all the STL containers is by far the most used, tested, and fully understood container to ever grace C++. What it isn’t is a Swiss Army Knife. But lets use it like one anyway!

With this you can now lookup your internal type like so:

I understand this is an extremely naïve implementation but the concept is there. We use strings in order to obtain a handle to some internal type.

Don’t worry we’ll get to benchmarks and how we can make this particular implementation faster.

Well that was easier. What does the request to get the type look like?

OK so that looks a little easier to read and if we have a ton of types we won’t exactly do a linear search due to the way stores its entries (using RB trees).

follows the same rules as the only difference is that when you want to request all of the types from it the resulting list is unsorted. This behavior does have some performance implications and tells us a little bit about stores its elements (probably using buckets and separate-chaining to handle collisions).

Right, so you’re probably tired of hearing me ramble about silly implementations of different things string storing techniques. Lets compare some.

For this test I generated random strings all between the lengths of 10 and 100 so we can observe strings outside of SSO (Small String Optimization)

So… That’s less than helpful. At around 1,000,000 elements our unsorted vector takes around 32 minutes to lookup a string.

Let’s fix this. An easy way to get our implementation in-line with the rest of the containers is we can sort it and utilize an algorithm, , in order to speed up our lookup times.

Lets see how that change affects the benchmark:

These are much better numbers. We can actually see basically even with while starts beating out both.

If you think about the problem we’re actually solving here you could actually relate it to another common problem of pattern matching in search engines.

In our problem we have a finite set of things that a string could match to. With that in mind we can short cut a lot of the matching process if we manage to find a string with a certain prefix.

For example, in the list of strings if we have the prefix then we have two potential matches, and , however if we have a prefix of just then we don’t even have to compare the rest of the string to to have a full match, we can just take . This, of course, is all under the assumption you can short circuit like that in your string match. It’s possible you have malformed type strings. Keep this in mind when considering the following solution!

Luckily there is a data structure that will do just this type of prefix matching. A Trie.

Many may know what a Trie is but for those who don’t it is a tree based structure that is optimized for matching string prefixes to words inserted into the tree.

This is the resulting structure after we insert the words and,

An extremely simple implementation of this structure uses a to place a char leading to another node in the tree. Nodes can then be annotated with whether or not they’re a word (since individual branches can also be words in the case of above).

Lets see how this implementation might compare to our existing benchmarks:

OK, so with the naïve implementation we don’t even beat . This is unsurprising because it uses under the covers to maintain the Trie invariant of being sorted. We want to maintain that the Trie is sorted so we can return sorted lists of words, so we won’t bother using to implement the node behavior.

All this said, there is a lot of room for improvement. Mainly in the way we store words that are leaves. If you’ll notice, whenever we created the subtree for we added an extra branch node between where the leaf and the end of the word. This problem is exacerbated when we have very long words with no common prefixes with other words in the tree.

In essence the tree has two different types of nodes: leaf nodes and branch nodes. Leaf nodes are those that only contain the full words and branch nodes have child nodes that are either leaves or more branches. Here, I chose inheritance to do the trick for me:

To determine if I was at a leaf or branch I used the visitor pattern to tell me the information I needed.

Lets see how this implementation stacks up:

This is looking promising! This implementation even beats in terms of lookups. The reason we start to edge out is because this structure is allowed to shortcut longer string comparisons by binary searching and has a decent memory layout when inspecting leaf data.

doesn’t quite have what we need just yet. It can’t actually store values. It only looks up strings. So the final implementation, , will store values and has yet another neat feature.

In branch nodes were annotated via a boolean variable to indicate whether or not it was word. Just like in the naïve implementation, . In , however, we take a different approach. We distinguish branches that carry values with a type of their own:

Notice we also templated each type in order to store the values in leaf and value branches. How does this implementation fare?

Good! We’re still beating out the implementation even while storing values!

Now that we have all the information, lets see how it all looks:

It’s not enough to just benchmark lookup times, insert times are a concern too. Here’s a benchmark of inserting various numbers of elements using the same parameters (words between 10 and 100 characters in length):

Here it’s pretty expected that beats since its insert time only occasionally rehashes the whole data set. Generally the hash is linear in complexity, making the total insert time O(n + k) where n is the string length and k is (potentially your chain size). In our Trie we need to do a prefix match and potentially breakup a leaf node into multiple branches and up to two other leaf nodes. This makes our insert time on the order of O(n lg n).

I encourage you to check out the code where the benchmark code can be found along with the Trie implementations.

A quick shout out to gochart for providing the awesome chart making utility and draw.io for the Trie visualization.|||

