The tragedy of data science is that 79% of an analyst’s time goes to data preparation. Data preparation is not only tedious, it steals time from analysis.

A data package is an abstraction that encapsulates and automates data preparation. More specifically, a data package is a tree of serialized data wrapped in a Python module. Each data package has a unique handle, a revision history, and a web page. Packages are stored in a server-side registry that enforces access control.

Example: Bike for Your Rights

 Suppose you wish to analyze bicycle traffic on Seattle’s Fremont Bridge. You could locate the source data, download it, parse it, index the date column, etc. — as Jake Vanderplas demonstrates — or you could install the data as a package in less than a minute:

Now we can load the data directly into Python:

In contrast to files, data packages require very little data preparation. Package users can jump straight to the analysis.

Less is More

 The Jupyter notebooks shown in Fig. 1 perform the same analysis on the same data. The notebooks differ only in data injection. On the left we see a typical file-based workflow: download files, discover file formats, write scripts to parse, clean, and load the data, run the scripts, and finally begin analysis. On the right we see a package-based workflow: install the data, import the data, and begin the analysis. The key takeaway is that file-based workflows require substantial data preparation (red) prior to analysis (green).

Get the Package Manager

 To run the code samples in this article you’ll need HDF5 1.8 (here’s how to install HDF5) and the Quilt package manger:

Get a Data Package

 Recall how we acquired the Fremont Bridge data:

connects to a remote registry and materializes a package on the calling machine. is similar in spirit to or , but it scales to big data, keeps your source code history clean, and handles serialization.

Work with Package Data

 To simplify dependency injection, Quilt rolls data packages into a Python module so that you can import data like you import code:

Importing large data packages is fast since disk I/O is deferred until the data are referenced in code. At the moment of reference, binary data are copied from disk into main memory. Since there’s no parsing overhead, deserialization is five to twenty times faster than loading data from text files.

We can see that is a group containing two items:

A group contains other groups and, at its leaves, contains data:

Create a Package

 Let’s start with some source data. How do we convert source files into a data package? We’ll need a configuration file, conventionally called . tells how to structure a package. Fortunately, we don’t need to write by hand. creates a build file that mirrors the contents of any directory:

Let’s open the file that we just generated, :

Let’s edit to shorten the Python name for our data. Oh, and let’s index on the “Date” column:

— or any name that we write in its place — is the name that package users will type to access the data extracted from the CSV file. Behind the scenes, and are passed to as keyword arguments.

Now we can build our package:

You’ll notice that takes a few seconds to construct the date index.

The build process has two key advantages: 1) parsing and serialization are automated; 2) packages are built once for the benefit of all users — there’s no repetitive data prep.

Push to the Registry

 We’re ready to push our package to the registry, where it’s stored for anyone who needs it:

The package now resides in the registry and has a landing page populated by . Landing pages look like this.

Packages are private by default, so you’ll see a 404 until and unless you log in to the registry. To publish a package, use :

To share a package with a specific user, replace with their Quilt username.

Package handles, such as , provide a common frame of reference that can be reproduced by any user on any machine. But what happens if the data changes? tracks changes over time:

allows us to install historical snapshots:

The upshot for reproducibility is that we no longer run models on “some data,” but on specific hash versions of specific packages.

Data packages make for fast, reproducible analysis by simplifying data prep, eliminating parsing, and versioning data. In round numbers, data packages speed both I/O and data preparation by a factor of 10.

In future articles we’ll virtualize data packages across Python, Spark, and R.

The Quilt client is open source. Visit our GitHub repository to contribute.

Notes

 We plan to transition to Apache Parquet in the near future.↩|||

The tragedy of data science is that 79% of an analyst’s time goes to data preparation. Data preparation is not only tedious, it steals time from analysis. Data packages make for fast, reproducible analysis by simplifying data prep, eliminating parsing, and versioning data. In round numbers, data packages speed both I/O and data preparation by a factor of 10.