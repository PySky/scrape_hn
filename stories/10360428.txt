This package provides a simple way, yet extensible, to scrape HTML and JSON pages. It uses spiders around the web scheduled at certain configurable intervals to fetch data. It is written in Golang and is MIT licensed.

You can see an example app using this package here: https://github.com/celrenheit/trending-machine

main ( ) It is defined below in the init function spider. () { spider. () Register the spider to be scheduled every 15 seconds scheduler. (schedule. ( *time. ), LionelMessiSpider) This will run every minute of every day scheduler. (schedule. ( ), LionelMessiSpider) scheduler. () Exit 5 seconds later to let time for the request to be done. time. ( * time. ) } () { LionelMessiSpider = spider. ( , (ctx *spider. ) { fmt. (time. ()) , ctx. (); err != { err } , ctx. () err != { err } Get the first paragraph of the wikipedia page htmlparser. ( ). (). () fmt. (summary) }) }

In order, to create your own spiders you have to implement the spider.Spider interface. It has two functions, Setup and Spin.

Setup gets a Context and returns a new Context with an error if something wrong happened. Usually, it is in this function that you create a new http client and http request.

Spin gets a Context do its work and returns an error if necessarry. It is in this function that you do your work (do a request, handle response, parse HTML or JSON, etc...). It should return an error if something didn't happened correctly.

The documentation is hosted on GoDoc.

Contributions are welcome ! Feel free to submit a pull request. You can improve documentation and examples to start. You can also provides spiders and better schedulers.

If you have developed your own spiders or schedulers, I will be pleased to review your code and eventually merge it into the project.

Dkron for the new in memory scheduler (as of 0.3)|||

Scheduler of spiders for scraping and parsing HTML and JSON pages