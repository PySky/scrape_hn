Now that we’ve touched microservices and their main benefits for us, let’s talk about putting them to work.

At the moment, our monolith is still in production as one large service, living on dedicated instances on Amazon Web Services (AWS). In our microservice environment however, we want multiple services to be able to run efficiently on single instances.

This is where Docker comes in — containers provide a way for us to run multiple services on a single server. Since we don’t want to manually decide which service goes where and when, we went shopping for a container orchestration solution.

We did an internal evaluation of three systems by setting up hello world applications, and then made our decision.

We first tried AWS ECS, Amazon’s fully managed container management service. Despite not having a pretty interface, AWS ECS worked like a charm. We had logging, monitoring and horizontal auto scaling both in containers and machines. The “out of the box” machine auto scaling in particular was a killer feature compared to Kubernetes running on AWS. However, because of the lack of documentation and open community, as well as vendor lock-in, we weren’t convinced yet.

Next, we explored Mesosphere Marathon (DC/OS), which is a collection of container-related technologies by Apache. We took a deep dive into the communities and documentation, and concluded that given the alternatives, DC/OS did not feel right for us us. We liked the interface, but the overall feeling was that DC/OS was aimed more at enterprises that also want to efficiently manage their dedicated servers somewhere in a rack. We don’t see us leaving the cloud anytime soon, and we felt that there was too much overhead for our use case.

The final option that we tried and considered was Kubernetes: an open-source container cluster manager originally designed by Google. Kubernetes was born out of Borg, Google’s internal resource allocation system, and is, acccording to Wired, “one of the best-kept secrets of Google’s rapid evolution into the most dominant force on the web”.

What we liked about Kubernetes in particular:

Given our situation, Kubernetes felt like the right choice, so we went for it and moved on to getting it in production.

If the above still doesn’t make any sense, this video will give you a quick intro on container orchestration:

We first tried deploying Kubernetes on AWS, where we’re currently hosting our monolith. To get started, we set it up using the kube-up.sh command. Everything seemed fine until one day our cluster broke — turns out writing logs to small AWS EBS disks will eventually break the system.

Lesson learned: on AWS you really have to manage your Kubernetes cluster, and you’re going to need dedicated DevOps time for that.

Additionally, since Kubernetes isn’t integrated with AWS, we had to resort to CloudWatch/Lambda hacks for automatic scaling, and in-cluster Elasticsearch for pod logging.

Around this time we more formally decided that we wanted to be cloud agnostic. To prove that we were not locked in, we deleted our broken cluster on AWS and started a test cluster on the Google Cloud Platform (GCP).

By doing so, we gained the ability to scale horizontally in terms of servers and our pod logs were automatically scraped and centralized in Stackdriver Logging. Google Container Engine (GKE) gives us a one-click-go cluster, so we’re happy for now.

So are we now locked in at GCP? Not really: automatic horizontal scaling at GCP is a nice bonus, but that doesn’t mean that we’re married to Google. We can still decide to move our cluster away from GCP since Kubernetes can live anywhere. Sure, it might mean a little more effort to keep automatic node and pod scaling, but it’s definitely possible. And with Cluster Federation coming up, it will soon be possible to run Kubernetes over multiple cloud architectures. So while Kubernetes did nudge us in the direction of GCP, we’re more cloud agnostic than ever.|||

The first step in the rebuild of our web platform is a proper foundation: the infrastructure. And our title has already given away the first decision that we’ve made: we’re going for microservices…