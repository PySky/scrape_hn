Serverless architectures with AWS Lambda seemed very promising a while back when the new concept was introduced at re:Invent 2014. We used the service right away and delivered successful applications in production that have been running for over a year now. However, as we continue to support the production workload in Lambda, we have found a few disadvantages that I will share to help people understand the trade-offs of choosing a serverless architecture with AWS Lambda.

It’s about running small pieces of compute on-demand. If your code is not running, you don’t pay for it. It also removes the need for setting up servers, monitoring, scaling and load balancing. So in general it’s a great new concept and there is no doubt the cloud infrastructure will move towards this pattern — small serverless functions that get executed in response to events.

There is a limitation of concurrent lambda executions that you really need to be aware of so you can fully understand the impact it will have on your applications. The documentation states the following:

Update. — New execution default limit has been set to 1,000.

At first, I understood the limit was per-function, not across all functions in the region. So if you have multiple applications deployed in the same account, each one with its own set of lambda functions, and one of them spikes in executions, it will cause all of the functions to get throttled — causing serious downtime.

Now, this is a soft limit of 100 concurrent executions, meaning that you can request an increase to AWS Support, however, you cannot request the removal of this limit. So you have to come up with a desired specific number; in our case, we set it to 3,000 at first in production. But we have multiple teams deploying serverless applications into the same account, so it’s easy to lose track of whether 3,000 is still a good number, which by the way, you can’t see this number anywhere in the AWS Console. So of course, it happened, we hit this limit one day, causing downtime of all the applications running on Lambda just because one function was incorrectly being invoked uncontrollably.

I am not against having a limit of concurrent executions. Every system has limits, even EC2 has a soft limit of how many instances you can run within a region — set to 20 by default. My problem is that a single lambda function can throttle all functions in the same account. It defeats all our efforts of isolating teams into separate microservices so that one team can screw up their application without affecting the others.

Exposing lambda functions as HTTP endpoints isn’t a good idea if your user-interface depends on them. Your users will experience inconsistent loading times — especially when you face cold starts. The following chart shows response times of 1,000 consecutive requests to a lambda function written in Java.

As you can see in the chart, we have a lot of requests taking more than 2 seconds to respond: In the world of modern cloud applications, this does not provide an acceptable user experience.

Even discarding those spikes of times over 2 seconds, the average response time is around 500 ms, which isn’t great either. Given that the code is not doing any kind of heavy compute.

The following chart represents the same Java code running as a docker container deployed in Marathon in our Mesos cluster.

Much better. Not only faster but also consistent; not too many spikes and an average response time of about 250 ms.

When you look at the cost per 100 ms of execution in Lambda, it is $0.000002501 for a 1.5 GB function, which might look incredibly cheap. But if you do the math, having a lambda function running for an entire hour will cost even more than having an m4.4xlarge instance running the same hour on-demand. So a few invocations an hour will definitely get you a pretty good deal, but for higher loads, it becomes not so attractive.

It is taking Amazon a long time to have AWS Lambda enabled in all regions, especially in the Asia-Pacific ones. We have the requirement of deploying our applications in China and we haven’t been able to do so because of this limitation. AWS keeps saying they will be available soon but it’s been more than 1 year already and we are still looking forward to it. So be very aware of this.

There are great things about lambdas too! Like the native integration they have developed with multiple AWS services. It works really well to create an event-driven architecture where different processes run in response to events: like S3 uploads, IoT actions, DynamoDB updates or Kinesis Streams.

I haven’t personally used Step Functions yet, but it seems like a great use case for lambdas too. Writing the tasks as lambda functions provides a smooth implementation to build a reliable workflow system. — As long as you are not doing any heavy compute on the workflow tasks/steps.

My recommendation to teams building applications on AWS is to avoid going all-in with serverless using Lambdas. It is not ready yet. It will get better in the future for sure but stay away from it for critical applications, and perhaps experiment with a few proof of concepts or small projects.

If you are considering going serverless because of cost-optimization, there are alternatives like running docker containers in a shared cluster using Amazon ECS, for example. It results in good cost optimization if you have multiple applications to share the cluster with.

I am not saying to avoid using lambdas completely. I still think they are a great choice for certain use-cases like responding to events of other AWS services. I am just sharing our experience so you can consider the pros and cons of serverless architectures. I hope this helps.|||

This article describes my experience with serverless architectures using AWS Lambda and why do I think you should be careful when going all-in serverless.