It was almost 11PM. My distributed web crawler had been running for a few hours when I discovered a very weird thing. One of its log files had stopped being updated for about 2 hours. This was not a good sign, to say the least.

Was the process hung somewhere? Or, had it crashed? I was determined the find the root cause of the problem.

In this post, you'll find out how I was able to debug a Python crash in my Linux environment.

So I found that some log file - generated by one of my crawler's child processes - had stopped being updated.

As you can see, the PID was appended at the end of the filenames. And process 3395 had stopped updating its log file at 20:47. So I peeked into it using vim and I scrolled through to the end.

The log file had stopped being updated while performing a HTTP GET request against www.careertechz.com.

Was the process simply hung, or had it crashed? Let's see if the process was still running:

Indeed, there was no Python process with PID 3395 running on my system! So clearly, it had died. But why, exactly?

This is when I thought of looking at the global system message log. This would probably give me a better idea of the problem, especially since my crawler was running as a systemd service. I could thus expect systemd to log relevant information to /var/log/messages.

So I opened it using vim1 and I searched for the PID in question. And this is what I saw.

So process 3395 had been terminated by oom-killer, as evidenced by lines 3 and 25. oom-killer is a program responsible for automatically killing some processes whenever the system runs out of memory. Process 3395 had apparently been taking too much memory - ~320MB of virtual memory, as seen on line 22 - so it had been killed to free up some memory.

But... why had it used so much memory?

I figured out that maybe the last URL crawled by process 3395 was the culprit, namely http://www.careertechz.com/. So I loaded it in Google Chrome and it confirmed what had been floating in my mind.

The web page was actually a never-ending PHP error page, displaying a super huge stack trace. Thus, it had probably caused my crawler to consume an ever-growing amount of memory. Unless some timeout had occurred, right?

Here's the class that my crawler used to download web pages:

This class uses the requests library to perform HTTP requests. So just to validate its behavior, I created a small unit test that downloads the problematic URL.

In the above test, a timeout of 10 seconds is used. I was expecting requests to time out after that, causing an exception of type HttpError to be raised.

But surprisingly, it never timed out when I ran it. And the memory use of the Python process was growing without bounds. Aha! So I had clearly identified the source of the problem!

But now, why wasn't it timing out?

As discussed in one of my previous posts, HTTP libraries such as requests typically use two forms of timeout:

But neither form of timeout had occurred; on the contrary, requests was receiving data from the server, nonstop.

Thus, if I wanted to force requests to time out during long downloads, I needed some kind of "global timeout". Unfortunately, I found that requests doesn't currently have that feature. So I needed an alternative.

After googling around a little bit, I found an interesting discussion about global timeouts.

People first suggested that one could use eventlet, a networking library that allows non-blocking I/O operations in Python. In theory, I could use it to force the HTTP GET request to time out whenever a download is too long.

So I changed my class as follows:

There's no magic in the above code, really:

This was cool on paper, but not in practice, though. After carefully testing my class, I found that there were some side effects. For example, this would hang the unit tests suite for my crawler, and the exact reasons weren't clear. "What else could it break?", I wondered. So I just dropped eventlet, out of desperation.

People also suggested that Python signals could be used. The problem was that I needed the SIGALRM signal, as explained in this discussion. And that signal cannot be used under Windows - my development platform - as explained in the documentation:

So I realized that implementing a global timeout in requests was much harder than I thought.

This is when I had an epiphany - or sort of. Why was I trying to implement a global timeout mechanism, exactly?

Because I wasn't trying to abort long downloads; instead, I was trying to abort large ones. So why not download each web page by chunks, and then re-calculate the total download size after each new chunk?

Then I could simply use some threshold to limit downloads up to a certain size. This was a super promising - and much cleaner - avenue to explore.

So here's the modified class that I came up with.

Whenever a new HTTP GET request is sent, the above code downloads the response by chunks of 20KB, and it checks whether the total download size has exceeded a predetermined threshold of 1MB. If it does, then it raises an exception of type core.http.exceptions.HttpError.

I tested the code out and guess what? Once cumulative chunks reached a total size of 1MB, the code successfully aborted the download; it raised an exception of type core.http.exceptions.HttpError, as expected.

In this post, we've seen how to resolve a specific type of Python crash. I determined that one of my crawler's processes was taking way too much memory, thus causing it to be killed.

The diagnosis was rather straightforward thanks to the availability of critical information in logs; I was quite lucky in this sense. However, with other types of crashes, I'd probably have to generate a Linux core dump and analyze it using the gdb debugger. I've never had to do it, so it would certainly make a super interesting blog post.

Now, I'd be curious to know about you! Have you ever had to debug a Python crash? If so, what tools did you use? And were you successful? Please leave a comment in the comment section below!

Update (17/03/2017): this post was featured in Lobsters. It was also featured in the Python Weekly and Pycoders Weekly newsletters. If you get a chance to subscribe to any one of them, you won't be disappointed! Thanks to everyone for your support and your great feedback!|||

In this post, you'll find out how I was able to debug a Python crash in my Linux environment.