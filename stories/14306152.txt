Big O notation is the language we use for articulating how long an algorithm takes to run. It's how we compare the efficiency of different approaches to a problem.

With big O notation we express the runtime in terms of—brace yourself—how quickly it grows relative to the input, as the input gets arbitrarily large.

Big O notation is like math except it's an awesome, not-boring kind of math where you get to wave your hands through the details and just focus on what's basically happening.

If this seems abstract so far, that's because it is. Let's look at some examples.|||

Finally, a simple explanation of big O notation. I'll show you everything you need to crush your technical interviews, or ace your CS exam.