If you’ve ever wondered how music visualizers like MilkDrop are made, this post is for you. We’ll start with simple visualizations using the Canvas API and move on to more sophisticated visualizations with WebGL shaders.

The first thing you need to make an audio visualizer is some audio. Today we have two options: a saw sweep from A3 to A6 and a song I made (a reconstruction of the track “Zero Centre” by Pye Corner Audio).

The second thing all audio visualizers need is a way to access the audio data. The Web Audio API provides the for this purpose. In addition to providing the raw waveform (aka time domain) data, it provides methods for accessing the audio spectrum (aka frequency domain) data. Using the is simple: create a of length and then call the method to populate the array with the current waveform data.

At this point, the array will contain values from -1 to 1 corresponding to the audio waveform playing through the node. This is just a snapshot of whatever’s currently playing. In order to be useful, we need to update the array periodically. It’s a good idea to update the array in a callback.

The array will now be updated 60 times per second, which brings us to the final ingredient: some drawing code. In this example, we simply plot the waveform on the y-axis like an oscilloscope.

Try clicking the “Saw Sweep” button multiple times to see how the waveform responds.

The AnalyserNode also provides data on the frequencies currently present in the audio. It runs an FFT on the waveform data and exposes these values as an array. In this case we’ll request the data as a because values in the range 0-255 are exactly what we need when performing Canvas pixel manipulation.

Similar to the array, the array will now be updated 60 times per second with the current audio spectrum. The values correspond to the volume of a given slice of the spectrum, in order from low frequencies to high frequencies. Let’s see how to use this data to create a visualization known as a spectrogram.

I’ve found the spectrogram to be one of the most useful tools for analyzing audio, for instance to find out what chord is being played or to debug a synth patch that doesn’t sound right. Spectrograms are also good for finding easter eggs!

My favorite computer graphics technique is fullscreen pixel shaders with WebGL. Normally several pixel shaders are used in combination with 3D geometry to render a scene, but today we’re going to skip the geometry and render the entire scene using a single pixel shader (aka fragment shader). There’s a bit more boilerplate compared to the Canvas API, but the end result is well worth it.

To start, we need to draw a rectangle (aka quad) covering the entire screen. This is the surface upon which the fragment shader will be drawn.

Now that we have the fullscreen quad (technically it’s two half-screen triangles), we need a shader program. Here’s a function that takes a vertex shader and a fragment shader, and returns a compiled shader program.

The vertex shader for this visualization is extremely simple. It just passes through the vertex position without modifying it.

The fragment shader is a lot more interesting. We’ll start with this shader by Danguafer and make a few strategic modifications so it responds to the audio.

The key is multiplying the output color with the spectrum intensity. The other difference is that we scale by 0.2 because most of the audio is in the first 20% of the spectrum texture.

What is the spectrum texture, exactly? It’s the array from before, copied into a 1024x1 image. Here’s how to accomplish that (the same technique could be used for the waveform data):

With all that out of the way, we’re finally ready to draw the visualization. First, we initialize the canvas and compile the shader.

Next, we initialize the shader variables: , , , and the one we’re most interested in, .

Now that the variables are set up, we initialize the fullscreen quad and start the render loop. On every frame, we update the variable and the texture, and render the quad.

As you can see, fullscreen fragment shaders are quite powerful. For more ideas, spend some time exploring Shadertoy and The Book of Shaders. Making a shader react to audio is a great way to breathe more life into it, and as we’ve seen, the Web Audio API makes it easy to do. If you end up making a cool music visualization, share it in the comments!

If you enjoyed this post, subscribe to the newsletter or follow Noisehack on Twitter.|||

Basics of music visualization using the Web Audio API.