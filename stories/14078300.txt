Please open an issue if you have questions or problems with the dataset.

The 2017 competition, sponsored by Google, is part of the FGVC^4 workshop at CVPR.

There are a total of 5,089 categories in the dataset, with 579,184 training images and 95,986 validation images. For the training set, the distribution of images per category follows the observation frequency of that category by the iNaturalist community. Therefore, there is a non-uniform distribution of images per category. Example images, along with their unique GBIF ID numbers (where available), can be viewed here.

We follow a similar metric to the classification tasks of the ILSVRC. For each image , an algorithm will produce 5 labels , . We allow 5 labels because some categories are disambiguated with additional data provided by the observer, such as latitude, longitude and date. It might also be the case that multiple categories occur in an image (e.g. a photo of a bee on a flower). For this competition each image has one ground truth label , and the error for that image is:

The overall error score for an algorithm is the average error over all test images:

Participants are restricted to train their algorithms on iNaturalist 2017 train and validation sets. Pretrained models may be used to construct the algorithms (e.g. ImageNet pretrained models) as long as participants do not actively collect additional data for the target categories of the iNaturalist 2017 competition. Please specify any and all external data used for training when uploading results.

The general rule is that we want participants to use only the provided training and validation images to train a model to classify the test images. We do not want participants crawling the web in search of additional data for the target categories. Participants should be in the mindset that this is the only data available for those categories.

Participants are allowed to collect additional annotations (e.g. bounding boxes) on the provided training and validation sets. Teams should specify that they collected additional annotations when submitting results.

We closely follow the annotation format of the COCO dataset. The annotations are stored in the JSON format and are organized as follows:

TBD The submission format is still being determined. It may look similar to:

By downloading this dataset you agree to the following terms:|||

inat_comp - iNaturalist competition details