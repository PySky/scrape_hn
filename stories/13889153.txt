In my last blog post, we looked at the very basics of proof refinement systems. I noted at the end that there was a big drawback of the systems as shown, namely that information flows only in one direction: from the root of a proof tree to the leaves. In this post, we’ll see how to get information to flow in the opposite direction as well.

The code for this blog post can be found on GitHub here.

The addition judgment which we defined last time was a three-argument judgment. The judgment was a claim that is the sum of and . But we might want to instead think about specifying only some of the arguments to this judgment. In a language like Prolog, this is implemented by the use of metavariables which get valued in the course of computation. In the proof refinement setting, however, we take a different approach. Instead, we make use of a notion called bidirectionality, where we specify that a judgment’s arguments can come in two modes: input and output. This is somewhat related to viewing thing from a functional perspective, but in a more non-deterministic or relational fashion.

Let’s start by deciding that the mode of the first two arguments to is that of input, and the third is output. This way we’ll think about actually computing the sum of two numbers. Instead of three inputs, we’ll instead use only two. We’ll use a judgment name that reflects which two:

The behavior of the decomposer for will have to be slightly different now, though, because there are only two arguments. But additionally, we want to not only expand a goal into subgoals, but also somehow synthesize a result from the results of the subgoals. So where before we had only to map from a goal to some new goals, now we must also provide something that maps from some subresults to a result.

Rather than giving two separate functions, we’ll give a single function that returns both the new subgoals, and the function that composes a value from the values synthesized for those subgoals.

The function is somewhat redundant in this example, but the shape of the problem is preserved by having this redundancy. The same is true of the in the types. We can always decompose now, so the option is never used, but I’m keeping it here to maintain parallelism with the general framework.

Building a proof tree in this setting proceeds very similarly to how it did before, except now we need to make use of the synthesis functions in parallel with building a proof tree:

Now when we run this on some inputs, we get back not only a proof tree, but also the resulting value, which is the that would have been the third argument of in the previous version of the system.

Let’s now add another judgment, , which will synthesize the second argument of the original judgment from the other two. Therefore, the judgment means subtracted from . We’ll add this judgment to the existing declarations.

We can now define a decomposition function for this judgment. Notice that this one is partial, in that, for some inputs, there’s no decomposition because the first argument is larger than the second. We’ll also extend the function appropriately.

If we now try to find a proof for , we get back , as expected, because 1 subtracted from 3 is 2. Similarly, if we try to find a proof for , we find that we get no proof, because we can’t subtract a number from something smaller than it.

We’ll aim to do the same thing with the type checker as we did with addition. We’ll split the judgment into two judgments, and . The judgment will be used precisely in those cases where a type must be provided for the proof to go through, while the judgment will be used for cases where the structure of the program is enough to tell us its type.

Additionally, because of these changes in information flow, we’ll remove some type annotations from the various program forms, and instead introduce a general type annotation form that will be used to shift between judgments explicitly.

The last change that we’ll make will be to change the notion of synthesis a little bit from what we had before. In the addition section, we said merely that we needed a function that synthesized a new value from some subvalues. More generally, though, we need that process to be able to fail, because we wish to place constraints on the synthesized values. To capture this, we’ll wrap the return type in .

Finally, the function has to be augmented slightly to deal with the new in the return type of the synthesizing function:

If we now try to find proofs for some typical synthesis and checking examples, we find what we expect:

The bidirectional style in this post has some interesting limitations and drawbacks. Consider the case of the rule for : it has to synthesize the type of both the function and the argument and then compare them appropriately. This means that certain programs require additional type annotations to be written. But this is somewhat unnecessary, because once we synthesize the type of the function, we can use that information to check the argument. In the next blog post, I’ll try to explain how to do this in a more elegant way, the combines both the upward and downward flows of information.

If you have comments or questions, get in touch. I’m @psygnisfive on Twitter, augur on freenode (in #haskell).|||

Cascading disruption