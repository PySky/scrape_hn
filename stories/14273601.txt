At Shazam we have been using GPUs to do music recognitions since 2012. By putting a few GPU cards like the NVIDIA Tesla K10 or the K80 on a single server, we have been able to maximise recognition throughput as these fast GPUs, designed for intensive calculations, allow us to run more recognitions on a single bare metal server. These GPUs work well because when you Shazam a song, what you actually send us is a fingerprint of the audio that your device recorded. We use that fingerprint to search for matching songs across all of our catalog, which is a very intensive operation.

In a nutshell, we create fingerprints of all the songs we have in our catalog, compile them into custom databases that then we load onto each of our GPU’s memory, and then perform searches on them. This is an operation that GPUs can do much faster than CPUs.

When we started using GPUs, we leased bare metal servers from a service provider, and so we ended up with our own managed datacenter where we would run our fleet of GPU-enabled servers. We normally have a very predictable usage pattern where we see the peak of our traffic during the evenings and weekends. This is understandable, as this is the time when most people are relaxing, maybe listening to music or watching TV at home, or maybe out enjoying themselves with friends whilst music is being played in the background.

We also have a number of events during the year that send our traffic usage through the roof. Think the Super Bowl or Eurovision. It’s during these events that we get huge amounts of traffic, and we need to be able to serve quick responses for all of it. Provisioning a new bare metal server takes time, and once you have it you normally have to commit to a contract to use it. For this reason, we have provisioned for peak load so that we are able to serve traffic for these events. We work around this by utilising the server for other purposes, like running experiments or running different software stacks on them. Basically we had a world where most of our servers run in a datacenter, statically defined for long periods of time where changes to the infrastructure would be rare (i.e. increasing or decreasing capacity by adding or removing servers).

In order to keep our searchable music catalog up to date we run a process that updates the data loaded on our GPUs on a regular interval. To not lose capacity or data during these reloads on any of our clusters, we have a number of spare servers that we use to reload our catalogs. These also work as a spare servers we can use when our GPUs fail (unfortunately, GPU cards, as well as other hardware components, do fail eventually).

If only there was a way to instantly grow your server fleet…

About 6 months ago we decided to start looking at GPUs in the cloud. We evaluated several cloud providers, but eventually landed on Google because of the great service and amazing tooling that surrounds their cloud offerings. Google was just piloting GPUs in the cloud, and were kind enough to let us into their Early Access Program. We started experimenting with them and decided that this was the right moment to try and move a part of our very static infrastructure to a dynamic environment like the cloud.

Our typical datacenter would be made of a number of recognition clusters, each of them having a fixed amount of servers with eight GPU dies in each of them. These would be hit by our frontend servers when Shazaming a song. These recognition clusters would be used to search different datasets of music, so we’d have clusters that would have the most popular music at the very front of our searches, and other clusters, holding more rare and obscure music, being at the very back of our searches. Naturally, the number of requests that fall through to each cluster gradually decreases as the music becomes more and more obscure. These clusters farther down in the search plan are provisioned accordingly to match the funnel pattern.

Our monitoring infrastructure is mostly based on Nagios and Graphite. As we have evolved as a company on bare metal infrastructure, we have been using Nagios almost everywhere, with an extensive library of custom written NRPE checks for our GPU infrastructure. This, coupled with Graphite for metrics and some other custom solutions, allowed us to detect problems in our infrastructure.

We have a number of problems with this approach. To start with, our static bare metal infrastructure.

And then moving to our auxiliary systems:

When we started looking at GPU in the cloud we wanted to overcome all these problems we had with our old infrastructure, but at the same time be able to play with it along side our legacy systems. Migrations of live infrastructure are always difficult and delicate, so we needed to plan a way to do this whilst keeping our current infrastructure serving live traffic.

The vision we had for this was:

Our recognition infrastructure is mostly comprised of nodes running custom software that utilises each of the GPU cards in a node. These nodes build custom databases of our songs’ fingerprints and put them into the GPU memory where we perform our searches. Up until now we were using several GPUs on each of our servers. All of them would be monitored by a number of tools and scripts that would integrate with Graphite, so our monitoring was a mix of custom host checks and metrics checks that would allow us to know when something was not working as expected.

On moving to Google Cloud Platform, we made the following changes:

This, obviously, required extensive changes to our software.

To start with, we needed to write our central orchestration system from the ground up to work with Google’s API. This piece would be able to interact with the Google API to create new VMs every time we needed to deploy a new version of our software or when one of our VMs had failed. It would also present us with a nice UI that we could use to increase the capacity (or reduce it) as well as an API in case we wanted to query it or automate some tasks, etc.

As per monitoring, we’d need to make every single gear in our complex machinery support Prometheus by exporting metrics. Furthermore, we were going to make Prometheus try to self heal the system, so we needed to connect it to our central brain by notifying it (before us) every time a GPU VM was producing errors, not behaving properly, or offline.

With some of our bare metal contracts coming to an end, it was important for us to get to a Google solution as quickly as possible so that we weren’t paying for the same infrastructure twice. Since GPUs in Google were still very new, we started implementing and testing a solution before the offering was fully productionised. This also meant that our timeline was very compressed from when we decided to move to Google and when our contracts would expire.

Our development teams, with close involvement from the SRE team, scoped out the ideal solution and began iteratively developing it. We did weekly show and tells, and this was driven completely by the project team, which consisted of SREs and developers. We aimed for a minimum viable product (MVP) that we could use to get up and running in conjunction with a tight timeline with some of our bare metal contracts expiring.

As we moved closer to a point where we had our system in a working state, we started testing it with real traffic. We first selected a small dataset with less popular tracks in it and connected it to one of our existing datacenters. We then started sending a small percentage of our traffic to it. As our confidence grew, we sent up to a third of our global traffic. Then we created copies of bigger and more important datasets in GCP and started sending traffic to them. We can switch the traffic between the new and old infrastructure in a matter of seconds. This way, we could play with the traffic and increase it gradually until we were confident that the system could behave properly.

In the end we were able to deliver a solution that we consider to be far and above what one would expect from an MVP delivery. This is a testament to how easy the Google ecosystem is to learn, and how good our engineering teams are!. The tooling is second to none.

Migration to the cloud has been an exciting challenge for our development and SRE teams. Now we have a much more dynamic setup where we can add and reduce capacity much more quickly, software deploys and data reloads are faster and we don’t have to spend our time dealing with hardware failures.

We’re very excited about the possibilities which our migration has opened, and look forward to continuing to work with Google in the future.|||

At Shazam we have been using GPUs to do music recognitions since 2012. By putting a few GPU cards like the NVIDIA Tesla K10 or the K80 on a single server, we have been able to maximise recognition…