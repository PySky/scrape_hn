Often, however, it's likely to be less a case of human-to-human communication than corporation to consumer. Corporations are not always known for putting their customers' rights and well-being ahead of their profit margins, particularly industries which are infamous for basing their marketing strategy around making consumers feel bad about themselves such as the fashion and beauty industry.

Say 15 years from now a particular brand of weight loss supplements obtains a particular girl's information and locks on. When she scrolls through her Facebook, she sees pictures of rail-thin celebrities, carefully calibrated to capture her attention. When she turns on the TV, it automatically starts on an episode of "The Biggest Loser," tracking her facial expressions to find the optimal moment for a supplement commercial. When she sets her music on shuffle, it "randomly" plays through a selection of the songs which make her sad. This goes on for weeks.

Now let's add another layer. This girl is 14, and struggling with depression. She's being bullied in school. Having become the target of a deliberate and persistent campaign by her technology to undermine her body image and sense of self-worth, she's at risk of making some drastic choices.

This scenario is a long time from being reality, but it raises issues which are worth thinking about now. The question of who bears responsibility for safeguarding the wellbeing of those targeted by marketing using affective computing is a significant one. Should software developers build in pre-set "trip-wires" to stop the program if a high level of emotional distress is detected? Should there be limitations on affective marketing targeted at children, or those likely to be more emotionally vulnerable?

Affective computing has the potential to intimately affect the inner workings of society and shape individual lives. Access, an international digital rights organization, emphasizes the need for informed consent, and the right for users to choose not to have their data collected. "All users should be fully informed about what information a company seeks to collect," says Drew Mitnick, Policy Counsel with Access, "The invasive nature of emotion analysis means that users should have as much information as possible before being asked to subject [themselves] to it."

Affectiva makes a point of asking for users' consent to use their webcams and record their data, but those who buy Affectiva's software development kit are not necessarily bound by the same code of conduct. 'In our license agreement we make it very clear that we want this technology to be used with the opt-in of the person who is being recorded," says Zijderveld, but as it is not a legal obligation there is little that can be done to enforce this.

Corporations and media companies aren't the only ones with an interest in affective computing. Emotion is a powerful factor in the democratic process, influencing not only how people vote in elections but also which issues capture public attention and make it onto the political agenda. Emotion analytics companies are already investigating the possible political applications of affective computing. A 2013 study by Affectiva and MIT's Media Lab used participants' webcams to analyze their facial expressions online as they watched clips of the 2012 US presidential debates.

Using this data they developed a method for predicting independent voter preferences based on facial expressions, with an accuracy of 73%. During the first Republican Primary debate in 2015, Emotient used its software to analyze the expressions of a live audience, finding that Donald Trump and Scott Walker evoked the strongest responses while Ted Cruz failed to make a significant emotional impact with the audience.

The use of affective computing in politics has implications for the democratic process. On one hand, it could be argued that enabling political leaders to be better attuned to the emotional state and preferences of their constituents will lead to better, more democratic policy making. On the other, it could also contribute to an increase in short-term policy making in order to win votes, placing the focus on snappy one-liners which draw a smile rather than sound long-term policy. The use of affective computing in lobbying could lead to important issues receiving public attention. It could also become one more tool in the arsenal of powerful interest groups whose agendas may not necessarily line up with the public interest.

Affective computing may also be intriguing to political leaders of a different, and less democratic stripe. Digital surveillance software has been used to target activists and human rights defenders in authoritarian states all over the world. Despite sanctions and sales regulations, once a technology is widely available it's extremely difficult to keep it out of the wrong hands. The same program used to analyze responses to advertising could easily be used to monitor people's faces during propaganda films, and identify potential dissenters. Surveillance using emotion analytics could be a reality in democratic and non-democratic states alike in the near future, and Affectiva say they have already rejected approaches by governments and security services for their products.

Access' Drew Mitnick observes that mass government surveillance runs counter to the principles of human rights. "Databases of emotion information would be particularly revealing, and [the] collection of this data should be highly protected under strict standards, including a restriction for the collection to occur on a targeted basis and a high burden of proof [that it is necessary]."

The potential for affective computing and emotion analytics to fundamentally change the way in which we relate to our technology, and to impact our personal, political and financial decisions needs to be taken seriously. Rather than falling into Collingridge's dilemma, and remaining ignorant of the implications until it’s too late to change, there should be an early and meaningful public dialogue about affective computing and digital rights, which draws in the perspectives of industry, rights activists, advertisers, political leaders and civil society.

Technology should not be feared, but it should be respected for the powerful social tool that it is. When it comes to new technologies—especially those which touch upon our emotions, the very things which make us most human—we need to think carefully not just about the kind of technology we want to have, but about the kind of society we want to live in.|||

By the time we'll need reform, it might be too late.—Hopes&Fears