“When non-authoritative information ranks too high in our search results, we develop scalable, automated approaches to fix the problems, rather than manually removing these one-by-one,” a Google spokesperson tells TechCrunch via email. “We are working on improvements to our algorithm that will help surface more high-quality, credible content on the web, and we’ll continue to improve our algorithms over time in order to tackle these challenges.”

In addition to its search results and suggestions, Google’s photo algorithms and the ads it serves have also been problematic. At one point, Google’s photo algorithm mistakenly labeled black people as gorillas.

Google launched Photos in May 2015 to relatively good reception. But after developer Jacky Alciné pointed out the flaw, Bradley Horowitz, who led Google Photos at the time, said his inbox was on fire.

“That day was one of the worst days of my professional life, maybe my life,” Horowitz said in December.

“People were typing in gorilla and African-American people were being returned in the search results,” Horowitz said. How that happened, he said, was that there was garbage going in and garbage going out — a saying he said is common in computer science. “To the degree that the data is sexist or racist, you’re going to have the algorithm imitating those behaviors.”

Horowitz added that Google’s employee base isn’t representative of the users it serves. He admitted that if Google had a more diverse team, the company would have noticed the problems earlier in the development process.

Another time, Google featured mugshots at the top of search results for people with “black-sounding” names. Latanya Sweeney, a black professor in government and technology at Harvard University and founder of the Data Privacy Lab, brought this to the public’s attention in 2013 when she published her study of Google AdWords. She found that when people search Google for names that traditionally belong to black people, the ads shown are of arrest records and mugshots.

What’s driving mistakes like this is the idea that the natural world and natural processes are just like the social world and social processes of people, says Pasquale.

“And it’s this assumption that if we can develop an algorithm that picks out all of the rocks as rocks correctly, we can have one that classifies people correctly or in a useful way or something like that,” Pasquale says. “I think that’s the fundamental problem. They are taking a lot of natural science methods and throwing them into social situations and they’re not trying to tailor the intervention to reflect human values.”

When an algorithm produces less-than-ideal results, it could be that the data set was bad to begin with, the algorithm wasn’t flexible enough, the team behind the product didn’t fully think through the use cases, humans interacted with the algorithm enough to manipulate it, or even all of the above. But no longer are the days where tech companies can just say, ‘Oh, well it’s just an app” or “Oh, we didn’t have the data,” Patil says. “We have a different level of responsibility when you’re designing a product that really impacts people’s lives in the way that it can.”

While algorithms also have vast potential to change our world, Google’s aforementioned fails are indicative of a larger issue: the algorithm’s role in either sustaining or perpetuating historic models of discrimination and bias or spreading false information.

“There is both the harm data can do and the incredible opportunity it has to help,” Patil says. “We often focus on the harm side and people talk about the way math is — we should be scared of it and why we should be so afraid of it.

“We have to remember these algorithms and these techniques are going to be the way we’re going to solve cancer. This is how we’re going to cure the next form of diseases. This is how we’re going to battle crises like Ebola and Zika. Big data is the solution.”|||

When Netflix recommends you watch “Grace and Frankie” after you’ve finished “Love,” an algorithm decided that would be the next logical thing for..