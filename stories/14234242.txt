What's not Actionable & Business Critical Shouldn't Ring: Building the Right Alerting System

Last year, my colleague Guillaume and myself spent two terrible months. In only 60 days, only the 2 of us being on-call, we spent more than 300 hours working off hours. It was 4 times our regular monthly on-call hours and twice our busiest month since we joined the company.

There are reasons why we had so many incidents, but about half of them did not require an immediate intervention. Alerts were sent anyway and we had to wake up or stop our weekend stuff to fix things. With all these alerts being sent, we didn’t know if something could be acknowledged or needed immediate fixing anymore.

My team spent 2 years improving our infrastructure. We replaced outdated software and hardware, switched on a 2 data center model, and ensured everything was redundant. We improved monitoring too, adding more than 150.000 metrics and 30.000 triggers, more than 5,000 of them raising an alert in Pagerduty.

We had a great infrastructure and a great monitoring. But we also had an alerting problem.

We designed our monitoring and alerting system for the infrastructure we took over 2 years sooner, where losing a server was not an option. It was not adapted to the infrastructure we had built anymore and needed a complete redesign.

During those 2 years, we added many server related probes, but also functional probes. Lots of them. They are great probes. Trust me.

On a lovely Monday morning, I told Guillaume we would switch from an infrastructure oriented monitoring to a business oriented monitoring.

The idea was promising, but we didn't know if it was possible considering the how we designed our existing monitoring. It turns out it was easy. We started to review every alert we had in the past months with one thing in mind:

We started small, but lowering the alerting threshold of our Elasticsearch clusters. These clusters have more than 40 servers each, which means we can easily lose hardware without production impact. Also, these servers had been under heavy pressure during the past 2 months and responsible for most of our alerts. We only kept cluster related alerts, and waited for a week to see how things went.

After a week, we did the same on the whole platform. For every component, we asked ourselves a simple question.

When answer was "yes", alert thresholds were lowered. A "no" often meant we lack functional probes, so we added them.

It worked like a charm. We reduced the number of triggers actually raising an alert in Pagerduty from 5000 to 250. The first month after doing this, we divided the number of off work hours by 4. 3 months later, it was divided by 7.5 compared to our busiest months. We had done the job right.

There's one question left opened though. Could we have done it sooner?

That's a difficult question, and there's no real answer. Part of the infrastructure was ready for switching from a server oriented monitoring to a business oriented one. But we had too many things that were top priority then, and reducing alerting wasn't one. Or, to be honest, we had so many things to cover first that alerting was a secondary priority.

After a few months, I can tell reducing our alerting rate should have been a top priority before things got out of hands, for a few reasons.

The next steps are obvious, and we have already the first bricks: having trends analysed so our monitoring system can raise flags before problems arise. Another article to come.|||

Last year, my colleague Guillaume and myself spent two terrible months. In only 60 days, only the 2 of us being on-call, we spent more than 300 hours working off hours. It was 4 times our regular…