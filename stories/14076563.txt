We recently launched a service allowing users to enrich their chat experience with audio and video. A project like this requires developers to make technical choices for managing fluctuating traffic and also deal with challenges in monitoring a service that’s based on ephemeral cloud infrastructure. We wanted to share what we did and what lessons we learned.

As with many global web services, the demand for Kik’s video service fluctuates over time. Unsurprisingly, the demand for the service follows the global daytime, especially during the weekend. To conserve infrastructure costs, we’ve decided to rely on ephemeral infrastructure: servers are spun (and destroyed) according to actual demand. This design choice introduced a whole range of monitoring issues. We’ve solved them by relying on a suite of open-source tools, with Prometheus serving as the core component.

But before we review Prometheus, let’s focus on the basic unit of monitoring: the metric. A metric is simply a data point conveying some quantifiable information. Every metric has a name, a value and a timestamp at which it was taken. For example, the following is a metric gathered at 1487578310 (a Unix timestamp), indicating that the CPU usage is at 5% on the host at which it was taken.

Every cloud-based server we spin generates many metrics. Many of these metrics are concerned with the host: metrics like CPU usage and memory consumption. The other kinds are application-level metrics: information that is specific to the application being run on the host. We’ve been using Telegraf, a go-based open-source project that is both easy to use and to extend. Once installed on a host, Telegraf periodically collects metrics and then provides these as an HTTP page.

Metrics can be extended with additional metadata key-value pairs, sometimes called “dimensions.” These dimensions can later be used to group together metrics from different hosts. For example, a typical metric with dimension would look like:

In the example above, the metric has been enriched with two dimensions: one indicating that this metric belongs to a video-server and another indicating that the host is located in the NYC zone. Dimensions make it possible to group together hosts that perform similar jobs and to monitor them as service – with Prometheus, the metrics store.

Prometheus is a monitoring solution with many attractive features. It’s a time-series database that efficiently stores and processes metrics. It offers a rich querying language for metrics and an alerting system to track them. We found that for most of our needs, the built-in query language was more than sufficient. Of course, it’s also possible to query metrics externally and build more elaborate alert tools if the need arises. In addition, Prometheus integrates exceptionally well with Grafana, an open-source graphs tool, which we use to visualize our ops.

Here’s a simple example in Prometheus’s query language, calculating the current average CPU usage over all the hosts in the NYC zone:

The result is a number: the average CPU usage.

Prometheus can be configured to send an alert whenever this number goes above a certain threshold, grouped by a dimension. As you can see below, it’s not rocket science:

The Boolean expression in the “if” clause is calculated for each cloud_zone. If it evaluates to “true” for 10 consecutive seconds, an alert is sent.

Having now explained how metrics are generated and processed, we can turn our attention to the missing piece: auto-discovery. Prometheus can only monitor hosts it is aware of, but since we’re constantly creating and destroying servers, the list of hosts constantly changes. The solution is an auto-discovery service: Consul.

Consul is a go-based open-source auto-discovery service that plays well with Prometheus. The desired behavior is simple: Once a server is spun, Prometheus should be made aware of its existence and start collecting metrics and monitoring it. Achieving this behavior is somewhat more involved. To begin with, you need to set up a Consul cluster (or multiple Consul clusters if your deployment is global). Every cluster is composed of a minimum of three non-ephemeral servers running the Consul-server application. Consul clients are expected to register at these servers to become discoverable.

Our cloud provider (Digital Ocean) conveniently offers to store pre-fabricated images on its infrastructure, which can then be spun on demand. We bake our video-server software into such an image and also throw in Consul and Telegraf clients. The end result is a video-server that, once spun, immediately registers at its Consul server and is consequently known to Prometheus. The actual implementation involves running a local Ansible play when the server is booted (using a Digital Ocean feature called User Data).

Putting it all together, our monitoring system is composed of:

And in the spirit of sharing, here’s what we learned in setting up our monitoring solution. The advice below can help developers trying to set up production monitoring environments based on the stack described above:|||

We launched a service allowing users to enrich their chat experience with audio and video. We wanted to share what we did and what lessons we learned.