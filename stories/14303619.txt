This is fairseq, a sequence-to-sequence learning toolkit for Torch from Facebook AI Research tailored to Neural Machine Translation (NMT). It implements the convolutional NMT models proposed in Convolutional Sequence to Sequence Learning and A Convolutional Encoder Model for Neural Machine Translation as well as a standard LSTM-based model. It features multi-GPU training on a single machine as well as fast beam search generation on both CPU and GPU. We provide pre-trained models for English to French, English to German and English to Romanian translation.

If you use the code in your paper, then please cite it as:

Install fairseq by cloning the GitHub repository and running

LuaRocks will fetch and build any additional dependencies that may be missing. In order to install the CPU-only version (which is only useful for translating new data with an existing model), do

The LuaRocks installation provides a command-line tool that includes the following functionality:

First, download a pre-trained model along with its vocabularies:

This will unpack vocabulary files and a serialized model for English to French translation to .

Let's use to translate some text. This model uses a Byte Pair Encoding (BPE) vocabulary, so we'll have to apply the encoding to the source text. This can be done with apply_bpe.py using the file in within . is used as a continuation marker and the original text can be easily recovered with e.g. . Prior to BPE, input text needs to be tokenized using from mosesdecoder. Here, we use a beam size of 5:

This generation script produces four types of output: a line prefixed with S shows the supplied source sentence after applying the vocabulary; O is a copy of the original source sentence; H is the hypothesis along with an average log-likelihood and A are attention maxima for each word in the hypothesis (including the end-of-sentence marker which is omitted from the text).

Check below for a full list of pre-trained models available.

The fairseq source distribution contains an example pre-processing script for the IWSLT14 German-English corpus. Pre-process and binarize the data as follows:

This will write binarized data that can be used for model training to data-bin/iwslt14.tokenized.de-en.

Use to train a new model. Here a few example settings that work well for the IWSLT14 dataset:

By default, will use all available GPUs on your machine. Use the CUDA_VISIBLE_DEVICES environment variable to select specific GPUs or to change the number of GPU devices that will be used.

Once your model is trained, you can translate with it using (for binarized data) or (for text). Here, we'll do it for a fully convolutional model:

Use to convert a trained model to use CPU-only operations (this has to be done on a GPU machine):

We provide the following pre-trained fully convolutional sequence-to-sequence models:

In addition, we provide pre-processed and binarized test sets for the models above:

Generation with the binarized test sets can be run in batch mode as follows, e.g. for English-French on a GTX-1080ti:

fairseq is BSD-licensed. The license applies to the pre-trained models as well. We also provide an additional patent grant.|||

fairseq - Facebook AI Research Sequence-to-Sequence Toolkit