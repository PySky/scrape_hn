Nothing can beat a good clickbait title. But this one bears truth in it. In fact I would argue that mentally the migration already started even before that as things got more and more complicated with MySQL.

If you’re a small team, back then we were only three, there are only so much resources and expertise you can invest in your infrastructure.

MySQL has many benefits and we certainly wouldn’t be where we are, without it. But over time its shortcomings factored in until they outweigh the benefits. Due to lack of deeper expertise we always relied on either Percona MySQL or the native version shipped with Ubuntu LTS 14.04 and never worked with external tools because, well, someone has to learn/understand/maintain these workflows.

There were other small or benign issues. Each on their isn’t a problem, but they all factored in to our dissatisfaction:

It was somewhen around 2014 when our former hosting partner 25th floor suggested to use Postgres, but despite the good words this wouldn’t automagically increase our teams’ resources to switch the whole application over.

Nevertheless we followed the advice and took a closer look at Postgres. In our simplemindedness we thought we could get away with a simple driver change. Boy were we wrong. The first PR with the vision to completely switch to Postgres was created in November 2014 and was titled “[POSTGRES] Brave new world”. However, it had too many problems, got abandoned soon and eventually was thrown away half a year later. Famous CTO response on the final comment: “kick it like beckham!”

But we didn’t give up. After some discussion and planning, we decided that we will base the new Swat.io Analytics Engine on Postgres. This turned out to be the perfect opportunity: everything would be done from scratch, so no ties to an existing system. The only downside: our primary data would still be in MySQL. To get it into Postgres, we needed synchronization jobs. Not ideal, but acceptable from a business perspective and thus the way forward for us.

The project itself took quite some months to complete, which, besides having to develop the new backend and UI code, also gave us ample time to learn the new database system:

The Analytics project was a success and we were so fond of Postgres but still remembered the failed attempt to migrate the whole application in one go that we tried another approach:

 Create mirror models within our ORM which were connected to Postgres. On every save of the MySQL model we would trigger the same save to Postgres. While in theory this sounded nice, we always had gaps or certain fields not being properly updated that we had to abandon this approach after a while. Also having to apply such a workflow change across multiple repositories with different frameworks didn’t help exactly, either.

When we hit our Summer camp 2015 we decided to move some logging tables to Postgres. The information herein is volatile by nature and recycled after some TTL anyway. This allowed us to forgo any migration of data and simply switch to the new database and let the old one discontinue and eventually remove it.

Around August 2015 our team hit some more frustration points again with MySQL and another attempt to migrate the application code base was made. This attempt was far more advanced than the first one and one could really “use” almost the complete application.

There was just one “minor” inconvenience: the performance was WAY behind MySQL! This was even easily measurable in our modest developers VM. We did micro-optimizations (e.g. disable SSL connections) and started to overhaul queries. But the main problem as we concluded: our application does (sometimes) hundreds and hundreds of small SQL statements during a page load and each one of them was just a tiny bit slower in Postgres, but it added so much up that the difference wasn’t acceptable.

Mind you that this conclusion was a major factor how any future optimization was done:

Unfortunately, this attempt stalled after a few weeks/months and was almost forgotten.

Still, over time every new internal project pretty soon raised the question:

This wasn’t always easy as we were embracing InnoDB features of foreign keys to keep our data consistent and having parts of it in another database is error-prone. But we absolutely were intrigued by having our main “pain” points with MySQL being basically non-existent:

As we didn’t just expand our data stores but also our code repositories, writing tests reached it’s then-peak around 2016 when we introduced a new backend: our JSONAPI based API. We had great experience with wrapping any kind of DML and DDL statements in transaction for our tests but ultimately had to abandon this approach as we still had our primary data in MySQL which didn’t support this.

The trend to Postgres however continued. Issues with MySQL haunted us more and more,  maintenance windows during the night became more and more problematic as adding a few columns to our biggest tables started to take 2+ hours or sometimes was completely unpredictable and exceeded our announced downtime windows.

After a very productive spring and summer 2016 we made another attempt to migrate the whole application to Postgres. Based on the former year-old PR a new attempt was made. Eventually prioritization with the management became fruitful and the project was greenlit.

Having learned all the things in our attempt in 2016 led to an enrichment of performance and unleashing hidden capabilities of developers bug finding, which was unheard of before in our team. After many months of hard work and much much testing by the team, eventually in February 2017 we were, or so we thought, ready to make the move.

Whilst the migration of the data itself was successful, the final result had performance problems in a critical part of our applications. Unfortunately, we had to roll back, but we were eager determined to find the root causes and don’t give up.

Throughout our road to Postgres we received excellent support from Markus Winand of use-the-index-luke and modern-sql fame whose overall in-depth knowledge of multiple databases continues to amaze us every time. With his help and dedication from our team, we overcame the problems which in the end turned out to be a negative perception of performance due to the database being “cold”. As a final step to our migration we added on selected relations and so enhanced the first-time experience for our most user-facing critical parts (an “issue” which basically vanished after a few hours of operation).

Our second attempt in March 2017 was finally a success. Although the data migration took longer than expected and during the final stages uncertainty spread while we monitored the first steps when we let the traffic hit Postgres, in the end, everything turned out to work awesome.

The mental overhead of the two databases (because ultimately both served the same domain) and the maintenance headaches with MySQL started to have negative effects on our teams morale and Postgres finally put at end to this. It’s hard to find words describing the kind of satisfaction our team felt when we finally were able to push the button.

With all the baggage left behind we’re looking with great prospect into 2017 to improve performance even more, handling our scaling needs better and increase our customers satisfaction.|||

Learn about the top reasons why we were not satisfied with MySQL, how we migrated and what lessons we learned in a 2 year long project.