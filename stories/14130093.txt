Mouna Hammoudi (PhD, University of Nebraska) states that there are five main reasons why recorded automation scripts break easily. In this article, we will look at some solutions as well as critiques to the problems that are brought up. This study has shown that weak locators make up 73.6 percent of automation script failures in record/replay tests.

You can read the academic journal here

What’s nice about this write up is that it puts a percentage to the failures to the breakages. In the case for locators, it is a whopping 73.6%! That means, according to this study, we can fix a majority of recorded/replay tests if we can fix this problem. In detail, this is because locators are automatically generated in recorders.

Let’s start with an example:

And the values that were inputted were “hello world”. This would no longer be valid because the browser would not allow the form to be submitted. I actually don’t think there is a good solution to this problem. The input form has changed to an “email” type. Therefore, it should be so that the automation script behave accordingly. Anything else would produce a non-deterministic value.

I can answer this with a straightfoward technical solution. The problem exists because recorders do not understand the fact that the browser is navigating to a whole different page. The solution would be to listen for when a browser will jump ship and act accordingly. I think Selenium IDE fixes this issue by using “clickAndWait” instead of just “click” when it knows the browser is navigating to a different page.

This issue is directly related to a misuse of implicit waits. Misconfiguring timeouts can be a timesink to debug because it’s dangerous. If your recorded tests do this, it may be time to jump ship to another provider.

Recorded scripts can fail because popup boxes can appear (and not appear). Once again, I think this is a case of not communicating with feature developers to make sure that this problem does not occur. It used to be that resolving popups (accepting them) was difficult. But advancements to WebDriver and browsers implementing better drivers has eased mitigating the technical problem at hand.

In essence, automating a browser has two functions, where you want to act and what you want to do. More often than not, the fact that automation scripts break when web application change in the smallest of ways proves automation less fruitful work.

I would speculate that the root of these technical issues lie in the communication (or lackof) between the developer and tester. The issue lies in part by the tester assuming that the feature was stable. It also lies in the developer going in and changing it because they thought it wouldn’t break the testing suite (or just don’t care). There are two main solutions to this problem:|||

This is a breakdown of the academic journal by Mouna Hammoudi, 'Why do Record/Replay Tests of Web Applications Break'. In this article, we will look at some ...