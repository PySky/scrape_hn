War, like the man said, is hell. A wiki war is no different.

Wiki wars are literally conflicts between individuals or groups fighting over the control of narratives published on the internet, primarily Wikipedia.

“Battles to the death for insanely low stakes” as put by RationalWiki’s founder, David Gerard.

A wiki war occurs across a digital, contextual battlefield. A landscape comprised of WikiMedia platforms, Google search, sub Reddits, and blogs which become weaponized to suppress edits by groups of online users or to intimidate and harass other users on the web.

Wiki Wars are very complex, and in a heated event they can often require 8 – 16 hour full time days in heated arguments, consensus building, research, and three dimensional chess strategies between admins and editors gaming the process.

Wiki wars often begin from one single event, an ‘edit war’ that occurs on Wikipedia between the editors on an article.

The edit war turns to wiki war when working through disagreements on the sly, employing various tactics of editor suppression to remove the dissenting editors from the article.

One aggressive event to remove an editor from an article can trigger hundreds of defensive, and then regressive responses, igniting communities around the web in response, igniting into a much larger event.

On an encyclopedia that ‘anyone can edit’, this can mean just about anyone can find themselves involved in one.

Even if people are not involved in a wiki war, bots have been created to battle other bots on Wikipedia.

Wiki wars are far more nefarious than just a bunch of nerds arguing over oxford commas.

Wiki wars are dark collaboratives, evolved upon the design sensibility of WikiMedia software and playing off of PageRank algorithm for Google search.

Therefore while Wikipedia often is the center, these wiki wars spread outside of Wikipedia, to other WikiMedia platform sites such as RationalWiki, Encyclopedia Dramatica, Metapedia, Conservapedia, KiwiFarms, and a host of other wikis.

All of these platforms, along with Google search, are gamed by ‘dark collaboratives’, employing deceptive and harassing tactics for control.

These tactics can easily be initiated by any interest based group or individual, for any reason, political, polemic, or just in some instances, nothing at all.

The encyclopedic voice of Wikipedia is credibility, notability, and reliability for searchable information.

The winner of the wiki war gets to control the narrative and influence the global audience via prime real estate on Google search.

Wikipedia’s own guidelines encourage us to trust the narrative voice on Wikipedia with it’s emphasis on ‘neutral point of view’ editing, a paradise where only disinterested editors without any bias are tirelessly improving the encyclopedia.

The value of Wikipedia as a powerful online publisher gives any shrewd agenda editor a touch point to disseminate references, context, and in some cases mis-information and mis-direction to influence every one interested in a search topic.

The most valuable real estate on the web.

The winner in the wiki war controls the broadcast signal around a specific narrative.

The losers in a wiki war are banned from editing an article in the least, and can face harsher repercussions, such as doxxing, harassment, reputation destruction, and stalking; as detailed on this study. Naturally, these tactics often fuel more responsive, and in many cases like my own, defensive measures.

Noted; many “losers” in the wiki war on Wikipedia can often rush off to another WikiMedia software platform, and restore their lost narrative on a host of other wikis, each one catering to a specific world view, such as RationalWiki, RightPedia, InfoGalactic, or Conservapedia.

Many other “wikis” on the web actually emerged from wiki wars happening on Wikipedia.

Other than GamerGate, we don’t hear about wiki wars much, really.

The signal with the biggest broadcast capability about anything ‘Wikipedia’ is WikiMedia Foundation.

Because of this, ‘Wiki Wars’ are sometimes reported as ‘cutesy’ little fun things happening on the worlds greatest thing since slice bread or only reserved for a dozen or so highly significant topics which Wikipedia always seems to be able to account for.

GamerGate did get some mainstream attention on them, but the complexity of the problem over clouded the mediums that gave it life.

One of the reasons I believe they are difficult to bring attention to is just the complexity of the software itself.

Why? Because they’re so mind numbingly awfully complex to follow, highlight, and detail.

Buried out in the open.

This complexity cloaks discoverability of what is actually occurring in one, and unless someone has been directly involved, they are almost impossible to bring attention to.

One of the hurdles in producing this site was actually detailing the full arc of events in a wiki war while working through the labyrinth of the platforms software.

“Wikis” in general are widely adopted via WikiMedia’s software, which is the underlying platform housing the majority of most online wikis beyond Wikipedia.

Any flaw, therefore, on Wikipedia is repeated across all wikis running off of Wikimedia’s software.

In a wiki war, a number of online forces appear to converge, sometimes erupting in a ‘perfect storm’ of miscommunication, mistrust, and misinformation. In this sense, a wiki war is a ‘whole system’, with varying dynamics that find their balance inside of one.

Some of these “elementals” are easy to outline, distinctive elements that highlight their activity.

I’ll do the best I can to define each one.

Digital wildfires are one element; the mediums allowing the exchange in the war craft itself.

Digital wildfires represent bad distribution with questionable attribution, a medium with which misinformation can quickly spread and become adopted as true by millions.

According to the Macmillian online dictionary – a digital wildfire is ‘false or suspicious information’ that spreads virally on the internet.  A ‘dark meme’ that is collaboratively constructed by a mob type mind set. A false rumor that uses online social networks to spread at ‘breakneck’ speed.

In 2014, the World Economic Forum declared ‘digital wildfires’ a leading global threat for stability.

And that was published before the 2016 election.

Digital Wildfires is a broader terms for fake news and online misinformation – now a dominant tactic of war between nations, states, shareholders and citizens.

False information emerging as a human reaction to what is believed to be true. While there is plenty of evidence I believe to show that things like PizzaGate were also intentional digital wildfires, manipulated by a small set of deceptive actors – what really gave it adoption were well intentioned people. They really believed they were white knights, defending children against a great evil in the world.

In the belief that we are doing the right thing, we can easily participate in one without realizing the harm we are causing or the repercussions that can follow.

“Digital” is the key word in digital wildfires.

It represents the technical mediums and environments that distributes the information. This medium is pure technology. Digital online environments via mobile phones or desktop computers enable exponential spreading of information, or misinformation, at break neck speed to hundreds of millions.

These mediums also influence the behavior of the users in participation with them.

Digital wildfires ignite quickly, and what I believe accelerates them spreading is deception, misunderstanding, and confusion amplified by digital technology.

Digital wildfires, just like wild fires in nature, can be started intentionally, or unintentionally.

Just like forest fires, digital wildfires only require one online user with the intention of causing disruption to ignite.

I believe the majority of digital wildfires, however, are unintentional. I think it may be easier to understand them and build solutions for them by distinguishing that set from the more nefarious and intentional ones.

Let’s refer to wiki noise as just ‘semantical confusion’ that arises uniquely in a digital environment. Something we just have to work through in an online collaborative to avoid any misunderstanding, a breakdown in “seeing what each other means”.

That is really all ‘wiki noise’ is, common misunderstanding. And often it can be a feature of language or medium itself, not any one’s intention.

Left unattended, wiki noise is like dried tinder, awaiting to be ignited.

Digitally, especially with the limitations within WikiMedia software, this noise can be amplified.

Sci Fi Author Robert Anton Wilson coined this as ‘semantic noise‘ back in the 1980’s. I always loved the stories Bob told about it. Say or write the words, “I love fish” and one group interprets it as a preference for dining, another as a fondness for a home aquarium, without even being aware there was any disagreement at all.

I’m Watzlavic! No you are!

The infamous ‘I’m Watzlavic!’ story told by Wilson and Dr Paul Watzlavic, a communications theorist and psychologist, highlights how a ‘communication jam’ coming from different contexts being perceived in a conversation can even lead to perceptions of insanity or paranoia about others.

Dr. Watzlavic told the story of his own personal account as a new staff member at a hospital. His first day on the job, he reported to the office, where he found a woman sitting at the desk. He assumed it was the director’s secretary and approached her.

Dr. Watzlavic at this point classified her as a schizophrenic patient and was concerned she had wandered into the staff offices. Naturally, he became very careful in ‘dealing with’ her.

However, from the woman’s point of view, Dr. Watzlavick himself had appeared as a schizophrenic patient.

Dealing with these natural and unintentional communication jams can be serious business in online consensus building. It is easy for anyone to mirror their own personal psychology to filter out semantical noise or the confusion inherent in text and meaning.

This suggests that even if all participants in an online collaborative are all well intentioned individuals, the inherent flaws of the communication medium itself can sow the seeds of mistrust, even paranoia, in the consensus.

So a digital wildfire can be informed by nothing more than the inherent flaws of our own communication mediums and language, even without intention.

“Misunderstanding” is of course not the only thing that can ignite a digital wildfire – what can also ignite a digital wildfire is social propaganda, intentional misinformation.

It is also somewhat a ‘anti social’ methodology of raising status or lowering status of different voices and perspectives in any form of online consensus with information or misinformation timed to target, embarrass, threaten or compete with someone.

Social Propaganda is similar to just plain old propaganda, with the exception that it is used towards influencing the attitudes of others in a ‘small or closed social groups’ online to accept or discredit new voices coming into the group while boosting their own.

In online consensus building, any type of activist group or individual, any disgruntled commenter, will use a tactic I refer to in this study as “flag waving”.

Flag waving strategies are methods used in social propaganda campaigns to misdirect or mislead an online community, a mixture of persuasion and deception.

There are various subtle, even petty misinformation strategies that are easily to perform and surprisingly effective in scope.

In wiki and consensus building groups – social propaganda tags such as ‘sock puppet’ and ‘troll’ are used to favor dissent against an opponent, and on Wikipedia my direct experience proved can be used as a tactic for editor suppression.

I’ve witness my own ‘propaganda’ campaign, instigated by the notorious Oliver D. Smith, spread across multiple platforms, attempting to redefine the narrative of who I am, and of course to discredit this website.

Combining social propaganda with wiki noise, we have the ingredients of a digital wildfire that flow from wiki wars and continue to play out on any medium it can find a home.

So just how common are ‘wiki wars’ on Wikipedia?

Currently, there is no way to tell other than by visiting various noticeboards on Wikipedia and trying to figure that out from reams of discussions and accusations flying back and forth. There is no way to have a platform wide accounting of the problem because ultimately, no one is really accountable on Wikipedia.

However, the more time that carries on – the more the faulty architecture of Wikipedia becomes exposed, and what’s becoming exposed is that Wikipedia has no real solution to the problem. So if the problem happens a lot – then the leading source for global public education can only head down a path of discredit for Wikipedia if no solution emerges.

The idealist message of what the internet can offer can often distract from genuine real world problems, such as fraud, harassment, fake news, propaganda, manipulation, slander, libel and tracking that are occurring online.

It’s no different on Wikipedia.

If we are to obtain the utilitarian ideal of what knowledge building, collaborative platforms offer, these darker problems will need to find a resolution.

The conflict however that I see is that historically, WikiMedia has promoted the idealistic message while somewhat sweeping under the rug the very real world type problems that are occurring because of it.

WikiMedia conveniently passes responsibility to a community that will not arrive at a consensus.

Currently, I believe there is no solution to the problem likely to be adopted on Wikipedia. WikiMedia software itself creates too much of a competitive environment, creating a competition while amplifying opportunities to exploit anonymity and twist it into deception and manipulation .

WikiMedia takes a hand off approach, touting idealism. Thumbing their noses at revenue models, or paying editors or admins, they give the responsibility to a community with no tools to arrive at it.

Therefore any changes to Wikipedia’s structure, or the adoption of a new one, would need to find community consensus, which is prevented by the same software that facilitates it.

They tend to sort of stick their heads in the sand, removing themselves legally from any liability for any abuse that occurs on the platform they created, pass of the responsibility to an unmanaged and anonymous online community with plenty at stake with little to no proper oversight.

Free of having to worry about core problems in community editorializing like any responsible publisher would normally have to, all Wikimedia has to worry about is fundraising and spreading the utopian message at TED talks, receive the adulation of the world, a massive PR friendly message by every web search.

It makes one question who is really benefitting from giving all the power and responsibility to a community that is growing toxic.|||

