This is my third article on how to build Machine Learning models in the Cloud. I previously explored Amazon Machine Learning and Azure Machine Learning – relative newcomers in the cloud data market. Google Prediction API, on the other hand, was released all the way back in 2011, and offers a very stable and simple way to train Machine Learning models via a RESTful interface, although it might seem less friendly if you generally prefer browser interfaces.

I am not going to explore the wide range of services offered by Google Cloud Platform, you can easily check the Developers Console out by yourself for free, sign up for the Free Trial offered by Google ($300 in credit to use for 2 months), and check out Cloud Academy’s courses on Google Cloud Platform.

We can define Google’s approach as a “black box”, since you get no control over what happens under the hood: your model configuration is restricted to specifying “Classification” vs. “Regression,” or providing a preprocessing PMML (Predictive Model Markup Language) file and a set of weighting parameters in the case of categorical models. That’s it.

Let me clarify a few basic concepts that will help you specifically with Google Prediction API:

On the other hand, your input features (your columns) can contain any type of data, although certain types are easier to work with (i.e. text analysis is clearly more complex than numerical regression).

The good news is that Google just doesn’t impose any arbitrary constraints on your input data types or require any configuration process. All you need to do is format your dataset the right way. Think of it as a big table, where each row is an input vector and the first column is your target value.

You will need to upload a single CSV file and Google Prediction API will take care of types detection, values normalization, features selection, etc.

The only Google Cloud service we need in order to use Google Prediction API is Cloud Storage, where we will store our dataset. You will not need to enable it on your Console, since it’s automatically enabled on every Google Cloud project.

First of all we will create a new Project. You have to choose a name, an ID and, optionally, the datacenter location.

Just as we did for my previous articles on AmazonML and AzureML, we are going to train a model for HAR (Human Activity Recognition) using an open dataset built by UCI and freely available here.

The dataset is composed of more than 10,000 records, each one defined by 560 input features and one target column, which can take one of the following values: 1, 2, 3, 4, 5 and 6 (walking, walking upstairs, walking downstairs, sitting, standing, laying down).

Every record has been generated on a smartphone, using accelerometer and gyroscope data, and labelled manually based on the performer’s activity.

We are going to build a multi-class model to understand whether a given record of sensor data (generated in real time) can be definitively associated with walking, standing, sitting, laying, etc. Such a model might be useful for things like activity tracking and healthcare monitoring.

I have already gone through the process of manipulating the original dataset to create one single CSV file, since the original dataset has been split into smaller datasets (for training and testing) and input features have been separated from target values. You can find my Python script here.

The next step involves uploading the dataset file to Google Cloud Storage: you can do this from the Developers Console, clicking on “Storage > Cloud Storage > Storage Browser” on the side menu. Here you want to create a new bucket (i.e. a folder), select it, then upload your file. It will take a while (the file contains about 90MB of data). If you’ve got a slow network connection, you might try to upload only a smaller portion of the dataset. The model accuracy should be pretty good with only 20% of our Ground Truth.

Unfortunately, Google Prediction API doesn’t provide any user-friendly Web interface, and almost every step beyond this point will be performed using Python scripts via an API call. If you really can’t stand coding, you might use the official APIs Explorer, but that’s not how you want to build your products, right?

Before making real API calls, we need to enable Google Prediction API on our project. You will find it by clicking on “APIs & auth > APIs”: it will be the last but one item of the Google Cloud APIs list. Enabling an API is quite straightforward and you need to do it only once for each of your projects. A single click on “Enable API” will do the job.

One last step: you need to create a new oAuth2 Client ID. Most Google APIs use oAuth2 for authentication: you can either create a Service account key (for server to server applications) or a Web Application Client. In our case, since we don’t need to work with our users’ data, we are going to use a server to server key.

Eventually, you might use a WebApp Client ID even for a server to server application, but your code will end up being slightly more complicated and you will need to go through the typical oAuth flow (either using your browser or copying and pasting oAuth codes on your terminal).

Let’s proceed. Click on “APIs & auth > Credentials” and “Create new Client ID”. Select “Service account” in the popup and confirm the creation. You will automatically download a JSON file containing the new Client ID data, including ‘client_email’ and ‘private_key’: we will open this file and use these two fields in our code.

We are finally ready to use Google Prediction API. I am going to work with their excellent official Python client – even if the documentation might sometimes be misleading. Also, I am going to show small code segments to focus on each sub-task, but you can find the whole script here.

As you can see from the official documentation, you can either use a Hosted Model or train your own ones. In order to train a new model, we are going to use the insert API method.

Every Google Prediction API method takes your project ID as first parameter. The Trainedmodels.Insert method expects a body parameter, containing a model ID (that you choose), your model type (classification or regression), and your dataset (either a Cloud Storage location or a set of instances).

Optionally, you can specify the following parameters:

Of course the training phase is asynchronous and you will need to check your model status using the Trainedmodels.get method. As long as your model’s trainingStatus property is not “DONE”, you won’t be able to use it.

Now you can start generating new predictions, but you might want to first analyze it to see what kind of accuracy you can expect. You can call the Trainedmodels.analyze method and be given a lot of useful information about your model.

This API call returns insights about your dataset (dataDescription), providing three numerical statistics for each input feature: count, mean and variance. These might be useful, but aren’t anything special. After all, you could have computed them by yourself without creating a new model.

What we really need is the modelDescription field. Indeed, it contains a confusionMatrix structure. Although it’s not that easy to read in a JSON format, this structure will tell you how your model behaves.

In order to process it, Google had to split your dataset into two smaller sets. The first one was used to train the model, and the second one to evaluate it. If you do the math, based on the values and on the dataset size, you will notice that Google applied a 90/10 split.

I admit that some percentage values would have been easier to read, and maybe some precision/recall statistics would also have been nice. You can always compute those by yourself but let’s say that – very intuitively – you should see a lot of zeros around, and higher numbers on the main diagonal. Every non-zero value outside the main diagonal means that your model wrongly classified N of your records.

With this dataset and the applied data split, here is how the Confusion Matrix looks:

It is not too bad, considering the required effort and the total absence of configuration and data normalization. Google has successfully created a reliable model and we can now generate new predictions, based on new unlabelled data.

In order to simplify this demo, I am assuming that we have already computed every input feature on our smartphone, sent it to our server, and stored it into a local CSV file.

Therefore I am just reading the file and calling Trainedmodels.predict, which takes a csvInstance input, in the form of a simple list of values.

This API call is pretty fast (with respect to other ML services) and will return the following:

If your model average accuracy is high enough, you can just take outputLabel as your prediction result. In case you can’t blindly trust your model – or if you can make more advanced decisions based on you application context – you may want to inspect outputMulti and take your final decision based on each class’ reliability measure.

I believe Google’s black box reached a pretty high level of abstraction for developers, although a more flexible dataset configuration and better analysis visualization would make the product easier to use for everyone, especially non-coders.

One nice feature is that you can always keep your model updated, adding new data on the fly, without going through the whole training phase again. This is especially nice for systems that span long periods of time, so that you can easily adapt your model to new data and conditions, without the need for a new modelling phase.

As far as speed and performance, Google Prediction API seems like a great candidate for your real-time predictions. With respect to other ML services – and with this open dataset – it achieved the highest accuracy, taking only a couple of minutes for training and an average response time below 1.3 seconds for real-time predictions.|||

Learn Cloud Computing, get Certifications and Change your JOB!