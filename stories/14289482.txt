Kubernetes is my go-to platform for deployment. I can’t imagine deployment without it and going back to my father’s ways. There may be solutions that hog less resource but just for the convenience that K8s brings and engineer hours saved, I am willing to chip in a few extra dollars each month.

On GKE, I’m currently running 2 clusters (with 4 nodes and 1 node) together with two managed Postgres. One cluster is for txtpen and the other for everything else from hobby code to side projects to static websites, all consuming the same machine. Instead of messing around with the complexity of a full Linux OS (setting up nginx, docker-swarm, write proxy_pass, upstream etc…), I can type 1 command and go from dev to prod.

To illustrate the basic functionalities(and convenience) of K8s, I will use the example of a static website to K8s.

Let’s say there is a static website generated with Jekyll or Hugo. Just drop this Dockerfile in the root directory then .

This will create a docker image from the nginx base image that automatically serves static content from root directory with minimal overhead. It usually runs on 2mb of memory and negligible CPU.

After this, push the built docker image to Docker Hub or Google Container Registry.

With the docker image in hand, proceed to deployment on K8s. Create a new file named . This will allocate an isolated environment (called a pod) on your cluster. Don’t worry if you don’t know how to write it, because for most people configuration is copypasting other people’s code from stackoverflow or elsewhere.

To create a deployment, simply plug the config above into K8s.

Now a deployment is created but it is only accessible from within the cluster(default setting, more on this below). We need to expose the port and bind it an external IP. With K8s, this is done using a service. File below is the configuration for service.

There are four types of service (quote from the k8s docs):

So for a frontend service(user-facing, as opposed to a Database or Redis), the options are and , the former exposes the service port directly(think proxy_pass) and the latter distributes incoming requests to different pods. The cool thing about service is that k8s actually uses nginx internally. And k8s comes battery included: bindings with different cloud providers (i.e. GKE, AWS) so getting an external IP does not require any extra steps. In short, the developer can use the best tooling without ANY configuration. Explain why k8s is an overkill?

To expose the port, supply k8s with the above config:

Don’t use it on GKE. It’s basically a CDN cache that charges ridiculous $$$. Its functionality is nothing more than Cloudflare free-tier. But for AWS k8s users, this is a viable option if you find the need to use it, ingress is not in the scope of this blog post. For most static sites, the load balancing above is already an “overkill”(in a good way).

As we can see just how easily we can deploy a static website to k8s and use the best server infrastructure and tooling. Deploying a multi-container pod follows the same flow and is not that much more difficult. Even if it’s an overkill at times, I would not trade anything for the convenience of k8s.|||

Defintely not an overkill