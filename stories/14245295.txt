This article will help you start using Hashicorp's Terraform to create infrastructure resources in the Google Cloud.

You need to have an account in the Google Cloud Platform (GCP). For lucky new users Google offers starter bonus. As well you need terraform installed somewhere too.

First of all we need to allow terraform access GCP. This can be achieved by Service Accounts with corresponding access keys. 

 Within the GCP console tool gcloud it can be done like:

The first line will create new service account named . The reason for new separate service account it that you can grant and revoke rights to it independently to the default service account. The second line creates new access key and export is as file . With this key terraform can be authenticated to the GCP.

How ever the newly created user need writing permission to be able to create any resources. Several possibilites exiting to do achieve this, here we use biding of new service role to predefined editor role

This should be sufficient to start, for more background to what happens here take a look at Google Cloud Documentation.

Now we can start with terraform. First lets teach terraform how to access GCP. Terraform provider's name for GCP is "google". So if we put this

in to we are nearly there. But i've only provided project and region through terraform variables, we still missing credentials property. Since i care about credential's here and do not want to commit them accidentally i better provide them via environment variable GOOGLE_CREDENTIALS. This variable1 need to contain everything from the file we created above. One way to do handle it on Linux:

Now we are ready test everything. Let's declare small Cloud SQL instance in main.tf

You will be asked for value of gce_project variable - provide it and continue. Also you should see an error stating some access restrictions to API. Normally not all Google API's are activated by default. But normaly this kind of error points you to a particular URL where you can activate API access once. Even after the activation it may take several minutes, but then it works.

If you like destroy your resources with:

At this point you might think about further improvements. One of them is the remote state.

Remote states are great feature, when it comes to work in a team. To enable Remote state with GCP you only need to define backend and initialize new configuration.

Unfortunately backend configuration does not support variable interpolation. Even if it not that sensitive information like access.key i probably do not want to show it to everyone. Therefore my solution for this now is to put backend configuration into a separate file e.g. and notice it in the .

This file will be omitted on commits, but considered by terraform as long it's in the same directory with the rest of the resource definitions. Now don't forget to apply new configuration:

You will be asked it you wan't to migrate your local state to new remote location...

However not everything is shiny with Google Cloud integration at the moment. The relative fresh Environment States feature is not supported with GCS backend yet. From now you will receive error if you try to work with environment states.

Also state locking is not supported with GCS, can get important too.

Even if not everything is supported to degree of AWS terraform has good support of most important resources for GCE and additional features like "remote state". Yes, environments should be supported by remote state soon as well to make a better picture. Anyway i would say it worth to start working with terraform to provision GCE. Existing infrastructure can be integrated with . I've tested it with several resources and it worked well even if import feature is systematically not documented on resources (in contrast to AWS).

If you like you can check git repo with code mentioned here (it may evolve in the future). And as always appreciate your comments.|||

