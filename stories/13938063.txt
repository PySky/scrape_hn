As the world becomes overwhelmed with information, with all kinds of news being continuously updated every two seconds or so, a substantial volume of data is generated spreading through the web via some popular social medias, such as Twitter, Facebook and etc. We can be sure that the data harvested from those social media data set is of considerable value as these meta-data contains info involving many real-time events or topics, e.g, personal prepositions, food preference, protests, pandemics, climate disasters, crimes and etc. Besides as a cue of early event detection, the collective sentiment measured from these data can also reflect the potential trends in some social events, such as popularity, elections or movement in stock market.

There have been some methods to scrape data from popular websites, like Yelp, LinkedIn, Twitter and etc. In this writing, I will share with you some techniques to harvest data from Yelp.

In most cases, the first thing that would come to mind when we need to harvest or scrape data from websites is to use the public APIs provided by the site. Just like Facebook Graph API, Twitter REST API, Yelp also provides a REST API for user to deal with web harvesting/scraping need. By using the Yelp API, we can scrape the ratings, reviews, geo-location (city, code). For example, you can locate a specific type of business, like restaurants, and restrict the range of searching to a geographical zone, a neighboring community, apartment building or a city by using the Yelp API. Users  will be responded with a JSON formatted data frame file after sending out http requests. The data frames normally contain an abundance of matching information, including address info, distance, rating, transaction details, images, URLs and etc. Similar to LinkedIn, Yelp also make OAuth for identity authentication, that means users would need to register and apply with Yelp to get a list of identities assigned by the API. After users get authenticated by running the script, an URL request can be generated using REST. For example, the request shown below is a restaurant request for Colorado. The response content will be parsed into a JSON file and sent back with the required info repeatedly.

Yelp has provided detailed and all-inclusive API instruction with data description, instances, and failing report. However, the API is still not perfect with the limit of 100 times for users to to call for the API, 1000 times for test-oriented and 10,000 times if users’ application meets the display requirements of Yelp.

Using API to retrieve data enable fast and accurate data crawling. And many websites have made some partial data public to extend their impact. Yet, there are still certain data fields that are restricted in the public data pool, and would require some other crawling techniques beside the API. For example, parts of the data set in Yelp user comments are not transparent and each company with a specific id is only allowed to retrieve three data item at most. Therefore, we can try some other ways to harvest the data needed, such as using some automated scraper softwares. There are numbers of scraping tools around that provide well-rounded scraping or harvesting service, like Octoparse, Import.io, Mozenda and etc. In this writing, I will share with you about on my very own experience with Octoparse. Of course, you can also visit  for more detailed info.

Octoparse is a Windows-based scraping software. Users are not required to program to scrape or crawl the websites. The workflow designer in Octoparse is pretty user-friendly as shown in the figure below. The tips are quite clear, and the icons and operations are quite straight forward and easy to handle. By simulating and learning a series of human web browsing behaviors, like opening a web page in the built-in browser, pointing and clicking the web elements by selecting the related options in the pop-up designer window, Octoparse will be able to transfer repetitive manual extraction operations into automated web extraction process and retrieve the structured data  that users need.

With a premium plan, you can scrape the site using its cloud service, which supports API, IP rotation and task scheduling.

After data extraction is completed, the data harvested or scraped can be exported to different formats (excel, csv, html, txt, etc) or export directly to various databases (MySQL, SQL Server, and Oracle).

For more information about Octoparse, please click here.

Be the Best Junior Management Consultant: Skills You Need to Succeed|||

There have been some methods to scrape data from popular websites, like Yelp, LinkedIn, Twitter and etc. In this writing, I will share with you some techniques to harvest data from Yelp.