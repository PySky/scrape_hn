Over the past year, we’ve been transitioning two applications into gRPC services: a monolithic Ruby on Rails application and a .NET application. Along the way we’ve experimented with best practices, built tools to help with the process, and learned a lot about what works and what doesn’t. In this post, we’re going to talk about:

What we’re NOT going to talk about (so you can turn back now if you want):

In a word: structured. Although building a JSON API is simple to do, you have a higher risk of breaking service consumers because of its unstructured nature. You can achieve a variance of structure using a format such as Swagger or a Consumer Driven Contract. But it’s not a guarantee.

gRPC and protocol buffers promise a certain structure that will be sent without exception. gRPC also defines the protocol in which the data is transferred and the libraries for it. JSON is usually sent over HTTP, but gRPC doesn’t leave this up for debate. gRPC also provides the framework for error codes, authentication, and streaming messages.

Overall, we’re in the business to deliver value to our customers, not discuss which JSON standard to use. JSON-API? HAL? Hypermedia at all (shivers)? We just want to ship. gRPC lets us do that easily.

Our first approach attempted to keep protocol buffer files (.proto) within the same project they were implemented. However, we found this to be cumbersome because whenever a new service consumer was being built, the developer would have to copy the generated files out of of the service itself into their new project.

After realizing that we can kill two birds with one stone, we moved our protocol buffers into a separate repository on Github called the protorepo:

This new approach allowed us to gain many advantages. First off, our new single repository for protocol buffers acts as an inventory of all Namely’s services using gRPC. This grants us the pleasantry of knowing what those are at a quick glance.

Developers were now only notified for definition changes, rather than spam alerts from pull requests such as “move spacing 2 pixels to the right”. The notifications from a chosen repository became more meaningful. This spurred more conversations with members from multiple teams chiming in on service proposals, proto changes, etc.

By having a single repository for all service definitions, it allowed us to localize all of our build tools into one place.

One of the great things about gRPC and protocol buffers is the amount of languages it supports. As of writing, it currently supports 10 languages(!). Although, a problem with this is installing the tool to generate the specific languages files from the proto definitions can be a pain, and frankly, why do something a computer can do it for you automatically?

Our folder structure in the is standardized to enable automatic generation of language specific files. For example, if you have a service called "companies", you'd have a folder called the same thing:

Let's focus on a single file here:  . This file informs our build tools which languages need to be built for this service. For our companies service, it looks like this:

Each language is separated with a new line. When changes are pushed to the repository, our tool runs through each folder, reads the  file and runs the appropriate generator for all of the  files in the directory.

You might be thinking, “what awesome tool did you make for this?”. It’s not fancy by any means. In fact, it’s just long bash script. If you dare, here is a link to it.

After you’ve added all of the languages you care about to the file, you'll need to create a new repository in the format . So if you have a project called “heimdall” that uses the Go language, your new repository would be “github.com/namely/protorepo-heimdall-go”. These repositories are where we push all of the newly generated protocol buffer files once any commit is made to the protorepo repository.

This process has made it incredibly easy to write a service definition using the interface definition language provided by protocol buffers and generate the files you want quickly. But where do we go from here?

We’re huge fans of containers at Namely, specifically the Docker toolchain. We have now adopted a system that uses Kubernetes on AWS for a majority of our deployment and orchestration.

When a team is building a service, it’s required that the build process includes building a Docker image and pushing it to our private Docker repository (we use quay.io internally). Whenever a commit is pushed to a project, we take the first 8 characters of the commit’s SHA and use that as the Docker image’s variant.

For example, if you push a new commit that the first 8 characters are and your project is called , then your Docker tag will look like:

It is natural to be thinking “Yikes, but isn’t that using a ton of space?”. Short answer: yes.

The reason we do this is because we allow developers to deploy any version of their code to a development Kubernetes cluster. This means any branch, feature, etc. can be tested by our QA team and run a blackbox test suite against it. Everyone gets a clean environment on demand (out of scope for this post, but it’s damn cool).

However, the process is a little different once we actually deploy to our customers. Instead, we use release tags by using the releases feature on Github.

When a tag is created on a repository, our CI pipeline builds a Docker image using the new release tag as the variant. By using the companies service above as an example, the new Docker tag would be:

Once we have our newly built image, we modify another monorepo full of Kubernetes manifests for each project (similar to our protorepo):

This process is still manual, e.g. someone modifies these yaml files by hand. But! We are considering ways to improve it. For example, we’re using Helm for our development cluster (the one developers and QA use) but we haven’t yet switched to using it for production usage.|||

Over the past year, we’ve been transitioning two applications into gRPC services: a monolithic Ruby on Rails application and a .NET application. Along the way we’ve experimented with best practices…