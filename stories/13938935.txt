This is the first post in a series about JIT compilers. The plan is to take a simple input language and develop some interpreters and JITs for it, in roughtly increasing degree of complexity. It's my hope that by the end of the series readers will have a good understanding of what it takes to develop a JIT compiler and what are some of the tools available to assist with the task.

The input language will be Brainfuck, or BF as I'll be referring to it from now and throughout the series. I think it's a good language for the purpose since it really boils programmability down to the essentials. Even though it's very tedious to program in, BF is fairly "mainstream" as far as programming languages go, with some concepts like memory pointers and loops mapping directly to familiar C constructs.

As the implementation language I'll be using C++. This is, perhaps, not the most commonly used "starter" language; that said, most compilers I know are written in C++ (or C), and hence many of the most popular low-level code-generation libraries in existence are in these languages. In later parts of this series we'll be using some C++ libraries, and this is by far easiest to do from C++ itself. Besides, I try to keep my code straightforward throughout the series - there is very little use of advanced C++ features here.

The BF language is simple to describe, but I don't do this here. Please take a look at the spec, read the Wikipedia page, or one of the other existing resources. An in-browser interpreter such as this one can be very useful. I'll just give an example to develop a taste for the language. The following BF program prints the numbers 1 to 5 to the screen: Line 1 initializes memory cell 0 to the value 48, which happens to be the ASCII code for . Line 2 initializes memory cell 1 to 5, which is our loop counter. Line 3 is a loop that, at each iteration, increments cell 0 and prints its value out, then decrements cell 1 and checks if it has reached the value 0.

To get an initial feel for the language and to have a reliable reference implementation, we'll start with a simple interpreter that processes one BF character at a time and does what's necessary to "execute" it. One of the reasons for my choosing BF as the source language is its simplicity. You'll find a lot of tutorials online that purport to develop interpreters or compilers but end up focusing 90% of their time on writing the parser. I think the later stages of compilation are much more interesting, so my "parser" for BF looks like this: Note that this is a valid implementation according to the BF spec: all characters except the 8 supported ones are to be treated as comments and ignored . This parser is going to serve us throughout the series. With that out of the way, here's the actual interpreter: All these cases are rather trivial. The more interesting ones are the control flow ops - and . We'll start with - jump forward if the current data location is zero. This op makes it possible to skip a loop or implement a simple -like condition. The most important thing to note here is that the and brackets in BF can be nested; therefore, when figuring out where to jump, we have to find the matching bracket. If this seems like something wasteful to do at run-time, you're right - keep reading! For we do something very similar. In BF, is jumping to an earlier if the current data location is not zero. This is how loops advance to the next iteration (or stop). And that's it! The interpreter loop concludes with: The full code for this simple interpreter can be found in simpleinterp.cpp.

The most obvious optimization opportunity for the simple interpreter is to avoid laboriously looking for the matching bracket every time a or is encountered. Imagine a realistic program with a hot inner loop (by "hot" here I mean it runs many, many - possibly billions - of times throughtout the execution of the program). Is it really necessary to scan the source to find the matching bracket every single time? Of course not. We can just precompute these jump destinations ahead of time, since the BF program doesn't change throughout its execution. This is the idea behind optinterp.cpp - our first optimized interpreter. Much of the code is the same as for the simple interpreter, so I'll just highlight the differences. A crucial addition is this function, which is run before the actual interpretation happens: It computes the jump destinations for all the and ops in the program. Its operation is essentially identical to the scanning forward / backward for a matching bracket in the main loop of the simple interpreter. The result is the vector , where for every and at offset in the program, holds the offset of the matching bracket. For any other op at offset , is simply 0. The actual main loop of is the same as in , except for the clauses for brackets, which become simply: As you'd expect, is quite a bit faster; it takes only 18.4 seconds to run and 6.7 seconds to run - more than a factor of 2 improvement!

The optimization applied in the previous section was very beneficial, but it's also fairly trivial - we avoid completely unnecessary work at run-time, if we can just precompute it at compile time. To make our interpreter even faster, we'll have to get more creative. The first step in optimizing anything is measuring and profiling the current code. Some past experience helps avoid needless steps in this process. For example, it's fairly clear that almost 100% of the run-time of the interpreter will be spent in the single function that interprets the program; therefore, function/call profiling won't be of much help. The main loop is fairly small, however, and there doesn't appear to be much to optimize at first glance (disregarding micro-optimizaitons which I won't worry about here). Well, except that this loop runs for every BF instruction encountered in the source program, so it can run tons of times. So what we'll do is get a breakdown of the ops that execute during a typical program run. The code for already has this tracing included - it's protected with the preprocessor macro because it's costly and we want to avoid doing it in "real" runs. Here's the execution profile we get from a typical run of the benchmark on the prime 179424691. On the left is the operation, and on the right the number of times it was executed by the interpreter for the program at hand: The total number of operations is huge: over 3 billion times around the main interpreter loop. It's a good thing we're using C++ for the interpreter - running 3 billion iterations of anything in a higher level language would be painful! The ratio of pointer movement instructions to loops is suspiciously high. There's something like 242 million loop iterations executed (the count for ) but a total of 2.4 billion pointer moves: and . We'd expect and hope for the hot loops to be short and tight - why is every loop doing so much? A cursory glance at the source of provides a clue. Here's a representative snippet: Note the fairly long sequences of , . Just a bit of thought about the semantics of BF makes this clear - these are necessary to get anything done because we want to be able to get from cell to cell to update data. Now let's think what it means to execute a sequence such as in our interpreter. Our main loop executes 7 times, each time doing: Advance and compare it to program size. Switch on the value of the instruction to the right . That's quite expensive. What if we could compress all the long sequences of ? After all, what we do for a single is: So for seven s we could do: This is easy to generalize. We can detect any consecutive sequence in the BF source and encode it as a pair: the operation, and the repetition count. Then at execution time we simply repeat the op the required number of times. The full code for this interpreter is optinterp2.cpp. Previously, we kept a separate jump table correlated to the and instructions in the input program. Now we need extra information for every BF instruction, so we'll just translate the into a sequences of ops of the type: // Every op has a single numeric argument. For JUMP_* ops it's the offset to // which a jump should be made; for all other ops, it's the number of times the The interpretation happens in two steps. First we run to read the program and generate a . This translation is pretty straight-forward: it detects repetitions in ops like and encodes them in the field. A slightly tricky aspect here is handling the jumps, since the offsets of all ops in the program change (a run of seven consecutive s turns into a single , for example). Take a look at the code for the full details. As planned, the main interpreter loop becomes: How fast is it? The benchmark now takes 11.9 seconds, and takes 3.7 seconds; another 40% reduction in run-time.

Our optimized interpreter now runs the benchmark more than 3x faster than the original, naive interpreter. Can we do even better? First, let's take a look at instruction tracing for , repeating the previous experiment: The total instruction count went down almost 3x. Also, now the number of BF loop executions is more comparable to the number of other instructions, meaning that we don't do too much work in every iteration. This was our goal with the optimization of repetitions, after all. In fact, this execution profile is annoyingly flat. Performance gurus don't like flat profiles because there's nothing in particular sticking out that one could optimize. This usually means we should measure / trace something else as well. An interesting question worth answering is - what is every BF loop doing. In other words, what are the hottest loops we are running, and can we spend some more specialized effort to optimize them? This would require more sophisticated tracing machinery, which I've already included in the code of . This machinery traces loops and records the instruction sequence executed by each loop iteration in the program. It then sorts them by the number of appearances and shows the most common (hottest) loops. Here is the result for the benchmark: What do these traces mean? The first, most common one says: Move 10 cells to the right The loop doing this was executed 32 million times! Similarly, a loop doing the simple "decrement the current cell" was executed 28 million times. If you look in the source of , these loops are easy to spot. The first one is ; the second one is just . What if we could optimize these loops entirely away? After all, they are doing something that is much easier to express in a higher-level language. merely sets the current memory cell to 0. is more involved, but not much more: all it does is add the value of the current memory cell 10 cells to the left. The trace shown above features many loops of this kind, along with another; loops like move to the right in jumps of 3 until encountering a non-zero cell. In we've added higher-level ops to the interpreter. We can add some even higher-level ops to optimize away these loops. optinterp3.cpp does just that. It adds a few more operation kinds for encoding common loops: The new ops are which replaces , for loops like and for loops like . We'll now need a slightly more sophisticated translation step that detects these loops in the input program and emits the proper ops. For an example of how it's done, here's the translation for : This function is called when translating the BF program to a sequence of ops. is the index in where the most recent loop starts. The code shown above detects the case where the only contents of the loop is a single (or since in BF memory cells hold unsigned values with wrap-around). In such cases, a op is emitted. When the interpreter itself runs into a , it does just what you'd expect: The other loop optimizations are a tiny bit more involved, but all follow the same basic idea. We expect this optimization to be very significant - we've just taken some of the hottest loops the program runs and folded them into a single, efficient instruction (or a sequence of efficient instructions for pointer-movement loops). And indeed, is very fast: 3.9 seconds on and 1.97 seconds on . The overall speedup is dramatic. is almost 10x faster than on our benchmarks. While we could certainly make it even faster, I think these optimizations are sufficient for our needs; let's talk about what we can learn from them instead.

It turns out there's a surprising amount of insight to be gained from the exercise this post went through. First, let's start with the distinction between compilers and interpreters. According to Wikipedia, a compiler is: a computer program (or a set of programs) that transforms source code written in a programming language (the source language) into another computer language (the target language), with the latter often having a binary form known as object code. would be the canonical example of this: it transforms source code written in C (or C++) into assembly language for, say, Intel CPUs. But there are many other kinds of compilers: Chicken compiles Scheme into C; Rhino compiles Javascript to JVM bytecode; the Clang frontend compiles C++ to LLVM IR. CPython, the canonical Python implementation compiles Python source code to bytecode, and so on. In general, the term Bytecode refers to any intermediate representation / virtual instruction set designed for efficient interpretation. Based on this definition, while is indeed just a BF interpreter, the optimized interpreters described here are more like compilers + bytecode interpreters. Consider for example. The source language is BF; the target language is bytecode with the following instruction set: ... where each instruction has a single argument. first compiles BF to this bytecode, and only then executes the bytecode. So if we squint a bit, there's a JIT compiler here already - with the caveat that the compilation target is not executable machine code but rather this specialized bytecode. Worry not - we'll get to a real JIT in the next part of the series. Finally, I'd like to point out that the loop optimizations performed in are the static version of a tracing JIT. We used tracing to observe which loops occur most commonly in our benchmarks, and optimized these loops. While the loops we optimized were very generic and surely appear in most large BF programs, we could take it further. We could optimize even more of the hottest loops, but with time we'd get into more specialized paths in our particular benchmarks. To be fully generic, we'd have to defer this optimization to run-time, which is what a tracing JIT does. A tracing JIT interprets code in the source language and keeps track of the hottest loops (and for dynamic languages, of the actual run-time types of values flowing through the loops). When the same loop goes over some threshold (say, invoked more than a million times) the loop is optimized and compiled into efficient machine code.|||

