From a recent article, Honesty and transparency are not enough:

The second reason that honesty and transparency do not suffice is that, even with the best of intentions and scholarly practices, researchers will fail. And, when transparency is touted as a solution to the replication crisis, I worry that the reasoning will go in both directions, so that replication failure will be taken as a sort of moral failing of the original experimenter. Yes, sometimes unreplicable research can be associated with ethical violations—even if researchers are innocently unaware of statistical errors in their published work, it is poor behavior for them to refuse to admit they could ever have been wrong—but one can also make mistakes in good faith. Making errors is inevitable; learning from them is not. To learn from errors we want a system of science that facilitates and incentivizes such learning; we don’t want an attitude that automatically links error to secrecy or dishonesty.

First, consider the practical consequences for a researcher who eagerly accepts the message of ethical and practical values of sharing and openness, but does not learn about the importance of data quality. He or she could then just be driving very carefully and very efficiently into a brick wall, conducting transparent experiment after transparent experiment and continuing to produce and publish noise. The openness of the work may make it easier for later researcher to attempt—and fail—to replicate the resulting published claims, but little if any useful empirical science will be done by anyone concerned. I don’t think we’re doing anybody any favors by having them work more openly using data that are inadequate to the task.

In experimental sciences such as psychology, challenges arise not just in reproducing published work but in replicating via new experiments. Conditions for data collection are often unclear (for example, survey forms and complete descriptions of experimental protocols are often not available, even in supplementary materials), which leads to potentially endless ways that a replication cannot be trusted. For example, biologist Simone Schnall points to a report claiming that “even in studies with mice seemingly irrelevant factors such as the gender of the experimenter can make a difference,” and it is presumably not a requirement of published papers to supply full demographic information in all lab assistance.

As a result, scientific claims can seem to be taken as the personal property of their promoters. For example, psychologist Daniel Kahneman wrote in 2014 that if replicators make no attempts to work with authors of the original work, “this behavior should be prohibited, not only because it is uncollegial but because it is bad science. A good-faith effort to consult with the original author should be viewed as essential to a valid replication.” I believe that Kahneman’s suggestions are offered in good faith but it seems clear that they give a privileged role to authors of published, or publicized papers.

Various reforms have been suggested to resolve the replication crisis. I am in favor of more active post-publication review as part of a larger effort to make the publication process more continuous, so that researchers can release preliminary and exploratory findings without the requirement that published results be presented as being certain. Any published paper is just part of a larger flow of data collection and analysis and should be treated as such. Related proposed institutional reforms include reducing the role of high-profile journals in academic hiring and promotion, and making it easier to publish criticisms and replications. . . .

But honesty and transparency are not enough, and I worry that the push toward various desirable procedural goals can make people neglect the fundamental scientific and statistical problems that, ultimately, have driven the replication crisis. In recent years we have seen many published and publicized papers which were essentially dead on arrival because they were attempting to identify small effects in the presence of noisy and often biased data. . . .

Honest reporting is important, even necessary. But design is important too. Dishonest reporting can mislead, but honest reporting is not enough to save a poor design. It doesn’t matter how honest and careful the researcher is, if the data are just too noisy to learn anything, from an exploratory or a confirmatory perspective. . . .|||

