A while back, things used to be easy. A personal computer or server comprised of hardware components bundled with an operating system installed on a hard drive and that was pretty much it.

And then the cloud happened.

Things got slightly more interesting and complicated too. As the need to setup isolated application and server environments without increasing computing costs grew, virtualization become very popular.

Instead of purchasing and installing server on every other hardware unit, you could actually install a complete machine -- called a virtual machine -- inside another machine often called the host.

So this piece of software that enables this kind of magic is termed a "hypervisor". A hypervisor or virtual machine monitor (VMM) is computer software, firmware, or hardware, that creates and runs virtual machines ~ Wikipedia.

So this software can run on the host's operating system or it can be installed on computer hard disk like an operating system. Those that are installed like an OS are called bare metal hypervisors and a good example is Microsoft Hyper-V or VMware ESX/ESXi.

However, you might be more familiar with hypervisors that run the host OS, aka hosted hypervisors. These are really easy to setup and use and they include; VMware Workstation, VMware Player, VirtualBox, KVM, Parallels Desktop for Mac and QEMU.

I've personally used Vmware Player and mostly virtual box to spin up a number of Virtual Machines(VM) on my personal laptop.

Virtual box needs no introduction if you have played with computers for a while. Its open source and owned by Oracle. It's also very use to use since it comes with a cool intuitive interface for creating, cloning, snapshotting and destroying VMs. If you setting up a development environment and wish to isolate web, database and application servers very easily, then you can use virtual box to create Vms for each of those servers on a single host machine.

KVM is another bull in kraal that's recently been making waves in the cloud computing space. Coming from Red Hat and zapped into mainstream Linux, it's used by many cloud providers like Linode and Digital ocean among others. It's majorly used by enterprise users specifically Virtual Private Server/VPS or Cloud VM providers to provision and sale VPS to clients.

VirtualBox and VMware will install on most machines with x86 processors, but KVM requires that the processor support Intel-VT or AMD-VT extensions, and that those extensions are enabled in the BIOS. KVM comes with VirtManager which is a tool for managing VMs. KVM is also used by Openstack which is known for building and managing cloud computing platforms for public and private clouds.

I Installed KVM on my machine once, but never really got hooked. KVM has a slightly steeper learning curve, less documentation and support and only runs on Linux.

Another hypervisor that's traditionally more tied to Linux is Xen. Xen is older than the rest of the hypervisors. It comes from the XenServer project, OracleVM, or any number of OSes like CentOS, Suse, Ubuntu and NetBSD which have Xen built in. It's currently used by cloud giant Amazon AWS to manage its fleet of millions of linux servers.

If you want to dig deeper, full Hypervisor comparison can be found here

Now the thing about Hypervisors discussed above is that virtual machines created require a separate kernel instance to run on. So it's possible to have host os kernel running on a completely different version than the guest or VM kernel. This is great because you have complete application environment isolation and possibly higher security.

However, there are also performance bottlenecks since a complete operating system with its own kernel is needed.

This is where Linux containers come in.

LXC, or Linux Containers are the lightweight and portable OS based virtualization units which share the base operating system's kernel, but at same time act as an isolated environments with its own file system, processes and TCP/IP stack.

So containers don't create the whole Operating system or Kernel again. And multiple containers share the same kernel, and each container can be constrained to only use a defined amount of resources such as CPU, memory and I/O.

They can be compared to Solaris Zones or Jails on FreeBSD. As there is no virtualization overhead they perform much better than virtual machines. ~ Nedo @ stackoverflow.

I have used Linux containers mainly on remote VPS on AWS or Linode and I was simply amazed at how powerful they are. While Linux containers won't give you the full power of VMs created by Hypervisors, they have the benefit of having a small memory and processor footprint. I've been able to spin up 3 LXCs on a 1GB VPN on AWS and they all worked just fine. You can't do that with virtualbox or KVM.

LXC package on Ubuntu specifically is really great because it automatically setups networking between host OS and containers -- something that usually trips newbies.

Hypervisors: are based on emulating hardware 

 Containers: are about virtualizating the operating subsystems(file system, networking etc).

Containers: Easily scalable and more resource efficient. 

 Hypervisors: Require more resources and less scalable.

This is probably the hottest newcomer on the virtualization space. Mainly used by developers and devops guys, docker uses LXC containers to make it easier to create, deploy, and run applications.

Developed by dotCloud, a platform-as-a-service company now renamed Docker Inc, Docker has grown become a go-to tool for developers who wish to to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package.

Usually developers have a development environment where they write and test their code -- normally on their local machine or development server. And then there's a separate production server which runs the live app.

When the application is tested and ready for deployment, the code is pushed to production server. Problems usually arise because the development and production environments aren't the same. It's even common to develop and test on one operating system while the production server is running on another. This has been cause of lots of frustration for developers.

Docker solves this problem by allowing developers to package their apps as containers and simply deploy the container to production server with all the configs it needs.

Personally I have tested it out, but never really got as hooked as I was to LXC. Probably that's because am a systems guy, so I need more flexibility with my containers/VMs which in my opinion LXC gives better than Docker does. But this is just a matter of preference.

Because several containers can be installed on a single server, they can become hard to manage eventually. Luckily there are a number of container management tools including Amazon ECS -- The Amazon EC2 Container Service (ECS), CoreOS Fleet, Mesosphere Marathon, Docker Swarm and more popularly Kubernetes.

My prediction is that containers might replace your traditional virtual machines at least in the cloud space very soon because of their lightweight and resource-efficient nature.

Hopefully, this post has helped clear-out all the dust around virtualization technologies and buzzwords.|||

