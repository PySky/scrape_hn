Few years ago I wrote a MapReduce tool in Go called gomr. I never used it for anything, just wrote it as an experiment to see if I could run MapReduce jobs without requiring a master, and it worked. The distributed consensus is provided by etcd which is typically deployed as a cluster. I am not fond of master/slave or primary/secondary systems. I like it when individual units are responsible and co-ordinate with each other and do their fair share of work.

gomr used S3 to upload user binaries and store map/reduce outputs and results. A running worker would query etcd for pending jobs, fetch binary from S3 and then it.

This week I re-wrote the tool to let Kubernetes take care of a lot of the deployment hassles and called it kubemr. Yes, I am aware I suck at naming things.

kubemr is a MapReduce system that runs within a Kubernetes cluster. Apart from initial complexity of setting up the cluster and managing it, kubemr itself is pretty simple.

Originally I planned to use etcd for state, but instead, I decided to use JSON patch functionality provided by kubernetes to make changes to this state. The operation allows the patch to fail if some condition is not met.

Above operation would fail if the already has a value. So multiple users might try to acquire the lock at the same time but only 1 would succeed.

All state information about a MapReduce task is stored in a kubernetes ThirdPartyResource.

In the case of kubemr, I named it . All I need to kick off some task is submit the following example yaml to Kubernetes.

The image knows how to process this. I used instead of the usual to ask Kubernets to generate a random unique suffix.

Once the job is complete, something like this is stamped onto the object.

If any thing fails along the way, the entire is marked as . Do or do not, there is no try.

The inputs and outputs here are just strings. We do provide utility methods to fetch/store files as S3 objects, but the meaning of the string is up to the user.

The operator watches for s and creates Kubernetes resources for it

This is user supplied docker image. It must have or defined. The actual code needs to basically satisfy JobWorker interface.

The user could actually write the worker in any language, as long as they implement everything that the Go library does. Perhaps in future I could provide wrappers to allow workers to be written as shell commands.

Will probably have some way to stream results, and some way to retry failed jobs, etc.

I had fun building this. Hope you have fun playing with it too.

Looking forward to using this at work in the near future.

Special thanks to Aaron Schlesinger. His talk and the corresponding example code helped a lot.|||

