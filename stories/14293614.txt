I trained an AI to generate piano music. Here are some results.

RagtimeBot composes to the chords of Maroon 5 - This Love:

More results are below

All midis are works for solo piano.

A few representative pieces were selected from each dataset to serve as validation data points.

All midis were transposed 11 times, so 12 copies of each piece were represented, 1 for each key.

All midis were appended and prepended with 3 bars of silence

We will try to predict the "musical event" at time t, given all "musical events" from time 1 to t-1. An LSTM RNN (Long Short Term Memory Recurrent Neural Networks) is a very good candidate to do this. To generate music, we can simply feed silence into the model produced and sample from the output distribution. Since all songs were prepended and appended with silence, it should produce sound after a few bars of silence. In practice I had to jump-start the generation after the silence because the model consistently predicts silence to follow silence. To jump-start the generation, we can just zero out the probability of silence for the first step and rescale; afterwards we can sample normally.

A piece of music is of extremely high-dimension and is thus prone to overfitting. To reduce dimensionality, we can break a song into its musical components and train one RNN for each component, given the context of the component. We can then generate the components sequentially. Incidentally, this is similar to how classical songwriting is done in that the chord progression will be worked out before the rhythm and notes.

More specifically a song can be divided into the following 4 parts.

Musical chords are not very well defined, for example G7 can consist of notes G-B-F or G-B-D-F. Therefore, we will define a chord as the set of tones that were played in a given time step. For my experiments, a time step of 1 beat was used. E.g. One measure in a piece of 4/4 time signature will have 4 beats corresponding to 4 time steps

Input: The chord at the current time step as a binary 12-vector.

Output: A vector of probabilities representating the chance that each chord will be the next one. Preprocessing was used to produce a chord vocabulary for the dataset of interest

The rhythm is defined as the set of time points during this time step in which a note was played. We will round time steps to the nearest 16th of a time step, in this way triplets still sound like triplets.

Input: The current chord as a binary 12-vector, concatenated with the rhythm at the current time step as a binary 16-vector.

Output: A vector of probabilities representating the chance that each rhythm will be the next one. Preprocessing was used to produce a rhythm vocabulary for the dataset of interest

3) Which notes of the current chord will be played at this rhythmic step

Given the chord (defined above) of this time step, which notes of the chord were played at this rhythmic step? For example, if this time step had a chord of [C,D,E,F] and a rhythm of 2 eighth notes, there will be 2 predictions made at 2 separate RNN time steps. A valid prediction would be [C,E], at the 1st time step and [D,F] at the 2nd. Note that the output of this RNN will also correspond to chords.

Input: The current chord as a binary 12-vector, concatenated with the rhythm at the current time step as a binary 16-vector, concatenated with a one-hot encoded 16-vector of the current rhythmic step, concatenated with the previous "sub-chord" as a binary 12-vector

Output: A vector of probabilities representating the chance that each chord will be played at the current rhythmic step

4) Which octaves of those notes will be played at this rhythmic step

Given the subchord at this time step which octaves of each note of the subchord is played at this rhythmic step?

Input: The current chord as a binary 12-vector, concatenated with the current sub-chord as a binary 12-vector, concatenated with the rhythm at the current time step as a binary 16-vector, concatenated with a one-hot encoded 16-vector of the current rhythmic step, concatenated with the notes played at the previous rhythmic step as a binary 88-vector, one for each note on the piano

Output: A vector of probabilities representating the chance that each "octave-set" will be played at the current rhythmic step. Preprocessing was used to produce an "octave-set" vocabulary

All MozartBot and BeethovenBot pieces were generated with a tempo of 120 BPM.

Ragtime pieces were generated with tempo of 90 BPM.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

If we fix the chord progressions, we can create "remixes" of known works, shown below:

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

As you can hear, the underlying work is pretty much unrecognizable, as chord progressions alone do not define a song. We can try to let through more of the original by fixing the rhythms as well as the chords.

MozartBot does Maroon 5 - This Love, with rhythm constraint

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

A good sign of no overfitting is that the original was not merely reproduced. The second sample is very close though.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

Your browser does not support the audio element.

- Clustering the chords. The chord progression model had the largest error out of the 4 models, so we can reduce dimensionality further by clustering chords. (So [G,B,D,F] may be in the same cluster with [G,B,F])

- Including the future as the input to the RNN. Bidirectional RNNs can be used to do this, but it is not straightforward to me how generation would work. This would further parallel human songwriting where songs will typically undergo multiple passes where consideration is given to a section's future as well as past.

- Experimenting with GANs (Generative Adversarial Networks) for each prediction phase

- Predicting the tempo of the piece

- Predicting how long to hold a note. All notes are held for a fixed length currently. This can just be added as a 5th phase, after the note-octave prediction phase.|||

