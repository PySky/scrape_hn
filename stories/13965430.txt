GPUs tend to have thousands of relatively slow cores divided into groups that must execute in lock-step, whereas CPUs tend to have a 2-20 relatively fast cores that can execute completely independently of each other. If you have a task that can be parallelized, such as rendering graphics to a screen, running a Monte Carlo Simulation, multiplying matrices for a machine learning algorithm, or powering a database, GPUs will be much faster by virtue of their parallelism. On the other hand if you are running something that is effectively sequential, like powering a word processor, then CPUs will be faster. A somewhat equivalent analogy is comparing the scavenging capabilities of a falcon with a colony of ants. The falcon can quickly find and devour its prey but the ants, though slow individually, can strip a whole forest in mere hours.

If you’re doing something computationally intensive that can be parallelized, it is a no-brainer to try to shift that workload to GPUs. Most of the major supercomputers are powered by GPUs, most machine learning algorithms run faster on them (particularly deep learning), and increasingly workloads such as analytic database queries are seeing huge speedups using GPU acceleration (full disclosure, I am CEO of MapD, a GPU database and visual analytics platform).

The fundamental difference in philosophy between CPUs and GPUs also has important consequences for their respective computational futures. Despite what you might hear, Moore’s law is still alive and well for the most part, and so every few years we are gifted approximately double the number of transistors to put in our chips due to die shrinks and other advancements. CPU software, despite being able to tap into multi-core CPU parallelism for approximately ten years, still for the most part is optimized for single-threaded workloads. Hence CPU manufacturers like Intel tend to use the extra transistors that Moore’s law grants them to add smarts to their existing cores and additional cache to make unoptimized code run faster. GPU manufacturers such as Nvidia do the opposite - the software paradigm for GPUs is fundamentally parallel and multi-threaded so these extra transistors are largely used to add more (relatively dumb) cores.

The net effect is that for optimized code, CPUs are seeing an approximately 20% improvement per year (measured in raw FLOPS)[1] while GPUs are seeing at least twice that[2]. Compounding this difference is the fact that GPUs tend to leverage the fastest memory technology to feed all of their computation units, so GPU memory may be more than an order-of-magnitude faster than CPU memory, a particular boon for workloads such as database analytics which tend to be memory/IO bound.

So ultimately it boils down to horses for courses. But as someone who founded a company around enabling interactive analytics at scale, I fully bet on GPUs as an architecture that is much more optimal for such a workload than CPUs precisely since it is designed first and foremost to run parallel code as fast as possible.|||

