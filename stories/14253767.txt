You worked hard to save a few CPU cycles in the central loop, but your code is still slow? Time to think about the time complexity of your algorithm.

You are a developer and want to learn Go? A new online Video course is coming soon! Visit appliedgo.com to learn more.

Imagine you just finished writing some code to solve a particular problem. The input is an array of values, and your code calculates some result from this array. You benchmark the code and find that it is reasonably fast for your test data.

Later, you get your hands on some real big data set from your production systems. A hundred times larger than your test data. “Ok,” you tell yourself, “if I run the benchmark with this input, my code should take a hundred times as long as with my test data.”

To your surprise, the code takes a thousand times longer! What happened?

Whenever you design a function that works through a set of input data, ask yourself,

“How does my algorithm scale with the input size?”

In other words, how does execution time change when the size of the input is doubled, tripled, quadrupled, and so forth?

To answer this, you have to look how your code processes the input data.

“Input” can mean anything: A file to process, a single number in an array, a character in a string, a string in a list of strings, a record in the result set of a database query, and so forth. The exact type of input is not relevant here. The number of input items counts.

But for now let’s assume we have an array of integers to process. And let’s assume some imaginary code that does a couple of things with this array:

First, a loop runs over the array and multiplies each number by two. For items, the loop needs units of time.

Then, two nested loops run over the array. The outer one does nothing but executing the inner one, so we can neglect the time this loop uses. The inner loop runs times, and needs four times the time than the single loop to process each item. So for our input items, the double loop runs for time units.

Finally, the code inserts each item into a binary search tree that is balanced. Each insert takes about units of time (the height of a tree of items is ), so all inserts take units of time. (” ” here means the logarithm to base 2.)

In summary, we have an execution time of time units.

Now this term is a bit unwieldy, isn’t it?

Luckily, we do not need to take the complete, unmodified term to answer our question. After all, we just want to get a rough estimation and not a precise timing.

Plotted as a graph where the x axis is our input size and the y axis are the time units needed to process items, our term looks like this:

We can already see that with a raising number of items (x axis), processing time raises quickly - one item takes 5 time units, two items 20 units, three items 40 units, and so forth.

Can we leave some parts of the term away without changing this graph too much?

Obviously, we need to identify the part of the term that has the biggest influence on the shape of the graph. Let’s divide the term into its components:

The orange graph corresponds to the term , the blue one to , and the green one to . It looks like wie can reduce our term to without changing the characteristic shape of the graph.

Can we simplify more? Let’s take away the constant factor “ “:

The new curve (cyan) looks less steep than the original one. However, their shapes are very similar, and so is their behavior when the input size is growing. The curves first grow slowly, then faster, and finally they shoot towards infinity, as we can see in this image where both axes have been scaled by a factor of 100:

So for estimating purposes, we can even leave out the constant factor. Our term has now shrunken to a simple , and the shape still resembles the original curve.

Now we safely can say that our imaginary code needs roughly units of time to process items. Much easier than the bulky term we started with!

Takeaway: From all the terms that describe the time a function needs for processing an input of specific size, we only need that part of the term that has the largest effect.

The simplified term stands for a whole class of terms, so let’s give it a name.

A function that needs roughly units of time to process input items has a time complexity of .

In general, we can say that -

A Big-O function is a mathematical term that gives a rough estimation of how the speed of an algorithm changes with the size of the input data.

Now we have a means to categorize time complexity into classes. Here are the most common classes:

The code always needs the same time, no matter how large the input set is.

Example: Accessing a single element in an array of items is an operation.

Characteristic: The more data, the better.

The time the code needs to process elements raises much slower than for large values of .

Examples: Inserting or retrieving elements into or from a balanced binary tree. Performing a binary search on a sorted array.

Examples: Iterating over an array, reading a file from start to end,

Whoa, who invents these names?! “Linearithmic” is an artificial blend of “linear” and “logarithmic”, similar to “Brangelina”. (Bonus question: Which came first?)

Characteristic: Almost, but not quite, linear.

Somewhere between and . Looks slightly like a flat curve for small values of , and turns into an almost linear curve for large values of .

Characteristic: “Easy, fast, and practical.” (But quickly getting slow.)

Processing time raises fast. For , when input size doubles, execution time quadruples. For , double input size means eightfold execution time. And so on.

Here is where the danger zone comes into sight. With increasing , polynomial algorithms quickly become slow.

Still, Cobham’s thesis considers polynomial problems as “easy, fast, and practical”. Which means there are worse time complexities on the horizon.

Our problem from above belongs this class. This specific case is also called “quadratic time”.

Examples: Bubble sort (O(n^2)). Any nested loops: Two nested loops - . Three nested loops - and so on.

Even for moderate values of , processing time goes through the roof.

Example: The famous Traveling Salesman Problem: Find the shortest route between towns. There are different approaches to this problem, but none of them belongs to a complexity class below .

The different characteristics of these time complexity classes become apparent when we put all those functions into one figure:

Here we can see immediately how the computation time diverges between the different complexity classes.

The graphs of and look as if they move very close together; however, after scaling the y axis to [0-200], the difference becomes obvious.

I know what you are thinking: This looks just like the difference between the graphs of and - I said that difference is irrelevant, and now with this almost identical graph I claim that the difference matters. Let me show the point.

Imagine we can speed up the O( algorithm by any constant factor . The curve will get flatter and flatter with increasing values of m, as the following graph demonstrates.

The black line is the polynomial function , the green line is the exponential function , and the red and blue ones are the exponential function multiplied by a factor of 0.5 and 0.1, respectively. The blue one looks much better than the polynomial curve. So can beat just by a linear speedup?

Yes and no. For small values of this can indeed happen. However, remember that we want to find out how an algorithm behaves when the input gets larger and larger. So let’s scale the graph to see how the curves behave for larger values of n:

Now even the “flat” blue curve crosses the black one! Try this at home with even tinier values - at some , the exponential curve will cross the polynomial one and increase much faster than the polynomial curve ever can. The difference between polynomial and exponential is indeed fundamental. (This applies to any pair of complexity classes.)

Looking at some of the curves, one question arises: Can we improve a given code, mabye even shift it into a faster Big-O class?

Let me give you a very practical answer here:

If the algorithm you strive to improve is one of the well-known, well-researched algorithms, the chances to find an equivalent, yet unknown, algorithm in a better complexity class is roughly zero. If you do find one, you are the hero of the day.

If, on the other hand, you are looking at some piece of lousy home-grown code, hacked together quickly while recovering from a severe hangover, then your chances aren’t that bad. Read on…

In some cases, the time complexity can be improved by re-arranging the input data.

For example, if you need to search through a list repeatedly (and much more often than adding something to this list), it will pay off to sort this list before searching it. A sorted list can be searched in O(log(n)) time using a binary search. Sure, sorting needs time, but you would have to do it only once to speed up those, say, thousands of searches that occur afterwards.

A good sort algorithm takes O(n*log(n)) time but reduces the time for each of the subsequent searches from O(n) to O(log(n)). Or for all searches from to . Big win!

Sometimes it pays off inspecting the way your input data is stored.

Example: Inserting a new value into a sorted linear list takes time: time to find the right cell, and time to shift all subsequent cells by one cell. (Note we are talking about average times here, in individual cases, these operations may take less time. E.g., if the new value is inserted at the end, there is nothing to shift.)

If you insert new data very frequently, better turn this list into a balanced tree. Then inserting a new value only takes time. (Remember, is about the height of a balanced binary tree.)

So far we discussed only time complexity, but the same problem (and the same complexity classes) exists for memory consumption as well. Why not trading in one for the other?

One technique to exploit this is called “memoization”.

The idea behind memoization is: If a function is repeatedly called for the same input that is hard to calculate, store the results in memory and reuse them instead of re-calculating the same over and over.

As an example, calculating the factorial of requires multiplications.

This wrapper function stores all results in a map, and if a given has already been calculated, it returns the result from the map; otherwise it calls .

The memoized function starts with the same time complexity as the original faculty function. Over time, however, most calls to only require one map access (and although the Go Language Specification makes no performance guarantees for map access, we can safely assume that it is better than ).package big-o

Optimization problems like the Travelling Salesman problem (let’s call it “TSP” henceforth) may have just one exact solution, but multiple near-optimal solutions. Algorithms that find near-optimal solutions by approximation and heuristics usually belong to a better O(n) class than the exact algorithm.

As an example, in the nearest neighbour algorithm, the salesperson, when arriving in one of the cities that are on the agenda, simply chooses the nearest unvisited city as the next destination, until all cities are visited.

On average, the path resulting from this strategy is 25% longer than the result from an exact algorithm, while the Nearest Neighbor algorithm only needs time.

Given that goroutines can execute in parallel (provided that more than one (physical) CPU core is available), a question comes to mind: Can a parallel version of an algorithm belong to a better complexity class than the original serial algorithm?

For example, could the TSP be solved in rather than in (while still using an exact algorithm and no heuristics)?

To explain this, we do not even dive into complexity theory. We just need to look at the hardware: Two CPU cores can provide double speed at most, three can provide triple speed, and so forth. Adding CPU cores therefore speeds up execution only by a constant factor, and as we have seen in the introduction, constant factors are not relevant when talking about time complexity classes.

Time complexity and Big-O classes might seem tedious stuff but are enormously useful for predicting how code behaves when the input size grows. There is an abyss of theory behind all this, but this basic set of complexity classes should suffice for everyday use.

All but the first three graphs were created with the wonderful Desmos online graph calculator.|||

When optimizing your Go code, be aware of Big-O.