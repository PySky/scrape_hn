The Big Data world has seen many new projects over the years such as Apache Storm, Kafka, Samza, Spark, and more. However, the basics are rather the same. We have data files residing on disk (or for hotness on memory) and we query them. Hadoop is the basic platform that hosts these files and enables us with simple concepts to do map reduce on them. In this post, we shall revisit the ground workings of MapReduce as it's always a good idea to have the basics right!

Following are the ten key concepts of Hadoop MapReduce.

This class can take a file and, if we have multiple blocks, split it to multiple mappers. This is important so that we can split work or data between multiple workers. If our workers are mappers, each should get its own data.

You should have a  that is not  and not   to receive the command line parameters.   splits the file (which usually has multiple blocks to multiple   and uses the  . Similarly, we have  .

When referring to  , for example, in output, you refer to  .

You can give the   multiple   for all files in directory or all directories or just use the   api to add multiple paths.

You have access to the main   object, which handles the job from your  .

The   of the   object brought to   is the offset from the file (it has also the actual  ).

If you write a string as an output from , you write it with  .

is a pure function it gets input   and emits   or multiple keys and values. So as its as much pure as possible its not intended to perform states meaning it will not combine results for mapping internally (see   if you need that).

In this tutorial, we scanned the main building blocks and components that compose Hadoop MapReduce jobs. We have seen the input to reducers, mappers, using * for multiple files, and the different text objects that we use in order to write outputs.|||

Review some of the key concepts of Hadoop MapReduce, like main, mapper, and reducer, the code that's used to split data and scan multiple files, and more.