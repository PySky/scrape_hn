TLDR; HTTPS helps keep the Internet safe by encrypting application and wesite data in transit. However, there are some performance caveats to consider. You can make your application's faster by terminating SSL closer to the user using an Application Delivery Network or a Content Delivery Network.

HTTPS is slowly becoming ubiquitous. In this article, we will take a look at what it protects you from, how it keeps you secure, where it falls short, explore what’s on the horizon and introduce you to Fly.

When you visit a site that is protected by HTTPS, you are sending HTTP over an encrypted Secure Socket Layer/Transport Layer Security (SSL/TLS) connection. To achieve HTTPS, you — as the visiting client — perform a three step dance known as the SSL/TLS handshake with the hosting server.

Each time you access a hostname over HTTPS, the following public key exchange plays out in fractions of a second:

If you are crafting an application that contains sensitive user data, the importance of security is clear. Jumping into application security leads to many questions:

Without further ado, we will get you some answers.

You may feel that having a site with no login portal and no sensitive information means that you do not require encryption. Alas, this is not the case. Even if you are hosting a simple blog or static page, if you choose not to encrypt then you are putting your visitors at risk.

In 2015 Google, along with with the University of California, Berkeley, discovered that more than 5% of unique daily IP addresses that hit Google have been affected by an intrusion technique known as ad injection.

Through unencrypted pages, an attacker can create misleading Buy Now links to scam credit cards or falsify Register Here buttons to steal personal information from your visitors. Whether your open HTTP packets are being sent through public Wi-Fi networks, the routers of your ISP, or the last stretch before your datacenter, they are free for exploitation.

Worst of all, in addition to being vulnerable to dangerous content injections, HTTP packets travel in plaintext. Any Person-in-the-Middle of the client and the server is able to read travelling data as though it were in plain text.

Encrypting your site, no matter the complexity, protects your application and your users while helping to improve the overall safety of the greater Internet. You can consider the vaccine metaphor: just as the general public is safer when individuals inoculate against contagious pathogens, the overall web is safer when every site is encrypted.

Let us look at some of the key advantages you receive by encrypting your application traffic…

Above all, HTTPS is vital for a healthy Internet. Applying it can be time-consuming. In addition, there are a few myths around performance that dampen SSL/TLS enthusiasm, despite the clear benefits. What are some of the performance drawbacks that prevent HTTPS from being applied to every site? Let us take a look.

In recent history, HTTPS has been maligned for allegedly taking a bite out of application performance. There is nuance to performance related claims; we need to qualify what type of performance is being impacted. First, we will take a look at claims of the computational burden placed on hosting servers. After that, we will look at visitor rendering times.

Encryption requires that your servers do additional work. Between computing the ciphers and accommodating extra requests within the handshake, some assume that the additional cost and residual slow-down makes HTTPS not worth the trouble. However, the CPU performance tax of SSL/TLS is contentious. Since 2010, when Google migrated popular mail application Gmail to HTTPS, the debate has quieted.

Google found that their production front-end servers applied less than 1% of their CPU load and less than 2% of their network overhead when crunching SSL/TLS. They observed that their production hardware was capable of a mighty 1500 SSL/TLS handshakes per second, per core before optimization.

While claims over CPU drain are exaggerated, let us explore the impact on page rendering times. Here, the situation is more prickly. Rendering time is a significant factor in providing a pleasant user experience. In one of the many supporting data points, Amazon discovered that every 100ms in latency leads to a 1% reduction in sales.

To look into rendering time, we will run a quick experiment. We can use to figure out the length of time it takes to form a standard TCP connection, compared with that of an HTTPS connection. The HTTPS connection contains 3 additional roundtrips, as we witnessed within the opening public key exchange.

We see that a regular TCP connection had a round-trip time of seconds while the SSL connection took seconds. This shows an increase of seconds per SSL/TLS request — a 300% increase! If your application makes multiple secure requests during the user experience, the time adds up quickly.

Luckily, there are advancements on the horizon that should result in general improvement.

HTTP/2 introduces a dizzying array of improvements to the HTTP protocol. Let us look at three useful adjustments within HTTP/2 that improve the performance of HTTPS:

The improvements of SSL/TLS over HTTP/2 are significant enough that it will drastically out perform an unencrypted site using HTTP/1.x. Instead of being punished for securing your application, modern HTTPS over HTTP/2 can make your application faster.

Along with the improvements within HTTP/2 , the TLS protocol itself is soon to be updated from to . TLS V1.3 will require one less roundtrip during public key exchange and it introduces the concept of 0-RTT Data. With 0-RTT Data, when a user returns to a site where they have already completed the exchange handshake, they do not need to repeat it.

Unfortunately, TLS V1.3 is not here yet and with HTTP/2 there is still a rendering penalty for SSL/TLS connections; the penalty increases based on visitor distance from the server. Never fear, though, you can mitigate the impact by terminating SSL connections as close to the visitor as possible.

If you have a server within Chicago and a user from Frankfurt wants to visit your application, rendering time inflates due to distance. Each roundtrip becomes slower. Given that the SSL/TLS handshake requires three roundtrips, this can cripple an experience!

For example, consider that each round-trip adds to the overall response time…

What seemed a relatively small amount of overhead ballooned over distance, resulting in a molasses like impact on the user’s experience.

To hedge the expense of the handshake, you may consider using a Content Delivery Network; the CDN will have edge servers located around the world to provide SSL termination. SSL termination is where SSL/TLS is decrypted, sending quicker unencrypted data upstream. By terminating closer to the user, we have shortened the route and reduced the roundtrip time.

For example, your CDN has a Frankfurt edge server. Each roundtrip to the edge server takes for a total of to complete the handshake instead of the full when traveling all the way to Chicago

These intercepting CDN edge servers terminate early then send unencrypted data to wherever your application server is located. This is ponderous. While rendering performance is enhanced, the danger is that your data is exposed as it travels from within the CDN to your application!.

A WikiLeaks cable released in 2013 revealed a sassy demonstration of how the NSA targeted ‘beyond the Load Balancer’ after SSL termination to sniff unprotected traffic within Google data centres. Spooky!

Cheeky buggers! How the NSA used Load Balancer SSL termination to spy on unencrypted data in the Google data centres.

While Google has the capability to apply sophisticated solutions and encrypt within their data centres, most applications do not have the resources to maintain a secure configuration post-SSL-termination.

At Fly, we crafted an agent to help with this. We have dubbed it the wormhole. The wormhole sits next to your application. When a visitor makes a request to your application, it will arrive at the closest edge server. After arrival, packets travel over a secure SSH tunnel between your application and the edge server using SSH remote port forwarding.

You terminate SSL/TLS closer to the user, avoid compounding SSL/TLS rendering penalties, and have an encrypted tunnel open directly to your application. Data arrives at the application encrypted, is never exposed within the datacenter, and does not bounce around a far-reaching CDN.

The tunnel from our edge-server to your application does not encapsulate entire TCP packets. Instead, it takes essential request and response data and sends it through an SSH tunnel protected by various ciphers. This method of encryption is secure and does not come with a performance penalty. The best part: it is free to get started.

Using HTTPS makes the Internet safer for everyone. Unfortunately, securing your application has been a daunting task. Protocol improvements like HTTP/2 and TLS V1.3 and services like CDNs will help balance the trade-offs between accessibility, security and performance… But vulnerable data routes can still be left exposed.

You can engineer complex and creative solutions to secure these routes. Or, you can take-off with an Application Delivery Network like Fly and get back to building nifty things for your users.|||

