I’m going to dedicate the rest of this week to a series of papers addressing the important question of “how the hell do I know what is going on in my distributed system / cloud platform / microservices deployment?” As we’ll see, one important lesson is that the overall system structure has to be inferred – under constant deployments it doesn’t make sense to talk about a central master configuration file. Other important techniques include comprehensive instrumentation, sampling (to cope with the volume of data), centralised storage for information, and an API with tools built on top for exploring the results.

First up is Google’s Dapper. Hard to believe this paper is five years old! There are three supporting systems referenced in it: one for capturing debug information, one for exceptions, and of course Dapper itself for collecting traces. The basic idea of Dapper is easy to grasp, it’s the necessity of something like Dapper and the uses to which it is put that are the really interesting part of this story for me.

Why do we need distributed tracing in the first place?

Suppose you’re trying to troubleshoot such an application. The first problem is that it’s hard to even pin down which services are used: “new services and pieces may be added and modified from week to week, both to add user-visible features and to improve other aspects such as performance or security.” And since the general model is that different teams have responsibility for different services, it’s unlikely that anyone is an expert in the internals of all of them. Finally, all of this takes place on shared infrastructure: “services and machines may be shared simultaneously by many different clients, so a performance artifact may be due to the behavior of another application.”

Lots of services, different teams responsible for them, continuous deployment – it’s not just Google that builds systems in this way any more. These are of course some of the same patterns underpinning the microservices approach. Thus the Dapper paper also serves as an illustration of the necessity of a distributed tracing infrastructure in a microservices world. In such a world, it’s impossible to know in advance what you’re going to need – hence monitoring needs to be ubiquitous and continuously deployed. This in turn means that it must be low overhead. It also rules out any reliance on developers to manually insert and maintain all of the required trace points:

A Dapper trace represents a tree of activity across multiple services. Each trace is identified by a probabilistically unique 64-bit integer known as its trace id. Nodes in the trace tree are called spans, and represent the basic unit of work. Edges between spans indicated causality. Spans have a human-readable span name as well as a span id and a parent id. Developers may supplement the automatically provided trace information through an annotation mechanism. Currently, 70% of all Dapper spans and 90% of all Dapper traces have at least one application-specified annotation. Safeguards are in place to avoid overwhelming the system:

Span data is written to local log files, and then pulled from there by Dapper daemons, sent over a collection infrastructure, and finally ends up being written into BigTable. “A trace is laid out as a single Bigtable row, with each column corresponding to a span.” Trace logging and collection happens out-of-band from the main request flow.

In terms of overhead, the most expensive thing Dapper does is writing trace entries to disk. This overhead is kept to a minimum by batching writes and executing asynchronously. In a worst-case scenario test, Dapper never used more than 0.3% of one core of a production machine. Dapper also has a very small memory footprint, and contributes to less than 0.01% of the network traffic in Google’s production environment.

Key to keeping the overhead low is collecting only a sample of traces:

The Dapper overhead attributed to any given process is proportional to the number of traces that process samples per unit time. The first production version of Dapper used a uniform sampling probability for all processes at Google, averaging one sampled trace for every 1024 candidates. This simple scheme was effective for our high-throughput online services since the vast majority of events of interest were still very likely to appear often enough to be captured…. We are in the process of deploying an adaptive sampling scheme that is parameterized not by a uniform sampling probability, but by a desired rate of sampled traces per unit time. This way, workloads with low traffic automatically increase their sampling rate while those with very high traffic will lower it so that overheads remain under control.

Is 1 in 1024 trace records really enough to catch problems??

A second level of sampling happens when writing to BigTable. Without it, the trace infrastructure comes close to saturating BigTable:

Write sampling is based on the trace id:

The Dapper team built an API for querying trace records supporting access by trace id, bulk processing of trace records, and indexed access allowing for lookup by service name, host machine, and timestamp in that order. An interactive web-based UI sits on top of this API.

Since Dapper shows what is really going on, as opposed to hunches that engineers might have, it has found many uses inside of Google. The Ads Review team used Dapper extensively to help them rewrite one of their services from the ground up, and credit it with helping them to achieve a two-order of magnitude improvement in latency. Causality captured in trace trees can also be used to infer service dependencies:

(Note here the passing reference to the impossibility of maintaining a central/master service dependency configuration file).

Dapper is used to provide context for exception traces:

Dapper has also been used to understand tail latency, the network usage of different services, and to determine causes of load in shared storage systems.|||

Dapper, A Large Scale Distributed Systems Tracing Infrastructure - Sigelman et al. (Google) 2010 I'm going to dedicate the rest of this week to a series of papers addressing the important question of "how the hell do I know what is going on in my distributed system / cloud platform / microservices deployment?" As we'll…