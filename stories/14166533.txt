Chatbots have a long and venerable history dating back to the 1960s and the famous Eliza bot that fooled some people into thinking they were chatting with a real human. Since then, computer programs capable of conducting conversations have become progressively more advanced with greater ability to understand content and respond appropriately.

And yet the ability to reproduce human speech convincingly still eludes chatbots. Talk to one for any length of time and it soon becomes clear that it is a machine.

One reason is that computers are unable to gauge the emotional content of conversations and empathize accordingly. This lack of emotional intelligence inevitably gives chatbots away.

Today, that looks set to change, thanks to the work of Hao Zhou at Tsinghua University in Beijing and a few pals who have developed a chatbot capable of assessing the emotional content of a conversation and responding accordingly.

The work opens the door to a new generation of chatbots that are emotionally aware. “To the best of our knowledge, this is the first work addressing the emotion factor in large-scale conversation generation,” say Hao and co.

Psychologists generally classify emotion into six basic categories: happiness, sadness, disgust, anger, surprise, and fear. We convey these emotions with written speech using words that have a specific emotional valence. The way this valence changes throughout a conversation is a measure of the emotional content.

For example, words such as laugh and smile are associated with happiness, depressing and cry are associated with sadness, and so on. Indeed, psychologists have created huge tables listing the emotional valence of words.

It is this kind of database that certain apps use to determine whether tweets are positive or negative—a technique known as sentiment analysis. In fact, using a sliding-window technique, social scientists can study the way the emotional charge changes throughout entire novels.

Now Hao and his team have used the same kind of technique to analyze and control the emotional content of conversations. This task consists of two parts. The first is to analyze the emotional content of the human user’s conversation using techniques similar to sentiment analysis.

But the next part is more tricky. It involves generating responses that are both relevant and emotionally appropriate.

Hao and company’s method is relatively straightforward. They start with a dataset of 23,000 sentences collected from the Chinese blogging service Weibo and manually annotated with their emotional charge—anger, disgust, happiness, like, sadness, or an additional charge associated with liking something. (They ignore surprise and fear, which are relatively rare.)

The team employs this dataset to train a deep-learning algorithm to classify sentences according to their emotional valence.

Finally, they use an ordinary chatbot conversation generator to produce responses while utilizing the deep-learning algorithm to ensure that the response has the correct emotional content. They call their system the Emotional Chatting Machine.

For example, to reply to the statement “Worst day ever. I arrived late because of the traffic,” the Emotional Chatting Machine can generate different responses, depending on the required emotion.

For happiness, it responds, “Keep smiling! Things will get better.” For sadness, it responds, “It’s depressing.” For disgust, it says, “Sometimes life just sucks.” For anger, it says, “The traffic is too bad!” And to express liking, it says, “I am always here to support you.”

This is interesting work that could have significant application. The ability to empathize (or seem to empathize) is an important component of human communication. Various studies have shown that humans are much more likely to have a positive reaction to an empathetic conversation. And that would certainly be useful in many call center–type situations.|||

Chatbots have never been able to empathize. That looks set to change, thanks to a Chinese team that has built a chatbot capable of conveying specific emotions.