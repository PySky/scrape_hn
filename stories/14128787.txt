An easy-to-read solution to the Hard Problem of Consciousness.

Consider the following question: Why are we conscious?

I get it; pondering consciousness sounds like an activity only enjoyed by unashamed philosophy majors who are either high or who have found a moment of post-yoga stillness.

But consider that we don’t tell our heart to beat or our cells to grow, we don’t have to think about focusing our eyes, and we don’t consciously will our bodies to inject adrenaline into our bloodstream when something scary happens. All of these things happen (mostly) automatically.

If such highly complex tasks can happen on their own, why should we be conscious of choosing what to eat for breakfast? How hard is choosing which flavor of yogurt to eat? And do we really need to be conscious to determine that we should peel the banana before biting it?

Post-yoga philosophy sessions aside, it is also important to realize that everything you’ve ever thought of — including every time you’ve experienced the feeling of being thirsty or the feeling of being loved, every ambiance you’ve been aware of, every piece of visual, musical, and kinesthetic thoughtstuffs that you’ve utilized, every ounce of hard earned wisdom, and every single bit of fluff passing through your mind — has and continues to be encoded by brain cells that pulse.

Contrast this with how computers processes information, via recording the state of tiny binary on-off switches and electromagnetic storage. In exceedingly rare cases, (which inexplicably have historically involved the Russians) information is encoded by the states and status of a three-state ternary switch.

But the neurons in your brain don’t use any particular cell state to encode information. Instead, your neurons keep track of information by pulsing, rapidly releasing electro-chemical discharges and then recharging. Like what your heart does, but quicker and more nimble, and on a much a smaller scale.

More specifically, information in your brain is encoded in the rate (and the change in the rate) of pulsing, rather than any particular “charged” or “discharged” state.

I admit that using the rate of a pulse sounds like an overly simplistic mechanism for keeping track of information. If you were to describe a piece of music by the mechanism of foot-tapping, you could have one person to tap how quickly the song should go, the volume could be indicated by how quickly another person would tap, yet another could tap slowly for minor chords and quickly for major chords, and so on.

Clearly, you’d need to hire a lot of people to describe Wonderwall, (though you could probably do it on the cheap by promising interns “exposure.”) And you’d need an army of foot-tappers to store the information of a Beethoven symphony (experience says interns wouldn’t cut it and that you could use grad students instead).

But if you are forced to be clever — perhaps because you have limited resources — you could start to get interesting interactivity with only four neurons. You might have one neuron that only pulses when three other neurons all pulse quickly. Or you could have one neuron that only pulses when three other neurons pulse at the same rate, in agreement. Connect up several of these four-neuron units and you could set up a system of voting and tie-breaking. You could even get the neurons fire in the same cascade when given the right input; we could call this a memory. Pretty sweet for a handful of cells that only know how to change how fast they pulse.

Consider that each neuron in your brain connects to, on average, about seven-thousand other neurons. In total, you are thought to contain roughly eighty-six billion neurons, to say nothing of the connections between them (each neuron is estimated to connect to seven thousand other neurons), or other types of brain cells.

The number of neurons in your body is so large I feel morally, spiritually, and ethically obligated to repeat more than once: eighty-six billion. Each one feverishly pulsing, changing its rate over time, like club-goers over the course of an evening. No matter what you are doing, your neurons are always dancing on electricity, and tripping the light fantastic with approximately seven thousand brethren. (Click here to hear the pulsing of a neuron translated to sound.)

Within the crazed activity of our brains, so much information is being processed and transformed and manipulated by our pulsating neurons that we are conscious of only a tiny part of it. Yet any effort to understand why we are conscious is fruitless without articulating why all of our neurons are so concerned with pulsing in the first place.

Many parts of our brains are dedicated trying to figure out what will happen next. For example: while I write this sentence, some parts of my brain are working to figure out how parts of your brain will think about what is going to happen when you read it. Other parts of our brain attempt to predict the errors in the main prediction systems, all in an effort for accuracy.

However, some things are too complex for brains to predict on their own. It turns out that you can’t predict how Mars will move across the night sky by yourself. If you observe it each night, you’ll find that Mars sometimes appears to go backward. Every so often, it seems to reverse its trajectory for a little while and then gets right back to continuing its original trek across the sky.

If you wanted, you could come up with a formal Martian prediction scheme. To do it justice, you’d need research, tools, measurement, theory, other people, and coffee. If you are doing things right, this means single-origin beans for the hipsters and caffeinated turmeric tea for the hippies. The caffeine would help your researchers deal with the following fundamental difficulty: Mars’ movement contains more variation and unpredictability than what we are used to. We’ve evolved to learn and then intuitively predict the movement of a ball when it flies at us, not so much when Mars does something similar.

But despite our limitations, we want to predict the motion of the planets. We want to predict lots of things, including how people will respond to different medications, when a heart will stop, when we might find ourselves in the midst of a maple syrup shortage, or most importantly, precisely when certain individuals (like me) will run out of crunchy peanut butter.

Even though we may start off absolutely terrible at any given prediction, we tend to improve them. To aid our prediction attempts, we have — throughout the entirety of human existence — come up with rules of thumb, mental models, and various tools of thought that make it easier to learn from experience as well as navigate the world we live in. These mental models and concepts serve to translate brain-cell pulsing to real world phenomena and vice versa.

The retrograde motion of Mars, for instance, makes perfect sense as long as the following concepts are part of your worldview: Mars is permanently further away from the sun than Earth, planets orbit the sun due to gravitational attraction, and when we look at the night sky we see a snapshot of where Mars is in three dimensional space. Each of these concepts builds on more basic elements, like that an orbit consists of an elliptical or roughly circular shape around a point, the very notion of space, and of physics, gravity, and so on. And each of these concepts is a “tool” we use to think with.

Or for another example, take communication. Language consists of words and other tools of thought we use to predict other people’s mental states and cause other people to predict ours. When these predictions are accurate enough, we say that we have successfully communicated. In other words, when we communicate, we trade recipes for predicting (at least some part of) what we are thinking to other people. Most of the time, our brains do this prediction so blazing fast it’s as if we have skipped the recipe and instead exchanged actual “meaning.” But the fact remains that when you say “chair” you rely on me to infer what you mean, as it turns out that there are many more types of chairs than anyone can conceive of.

Having the right tools of thought is like having a bike that fits your body — different combinations of gears and lever lengths on a bike will beneficially interact with the different lengths and strengths of your limbs. The bike geometry serves to translate your effort into forward motion most effectively for the environment you find yourself in.

The right set of thought-tools can help translate the stuff you interact with into, literally speaking, a quirky configuration of pulsing neurons that fits within your other quirky neuronal configurations. Without the germ theory of disease, for instance, people interested in advancing human health can work hard, have creative ideas, and burn the midnight oil, they just won’t get too far for their efforts.

However, it isn’t clear that one needs to be conscious to be equipped with the right set of concepts or thought-tools to predict what might happen in the world — the field of Machine Learning is dedicated to getting computers to do a form of unconscious prediction. The basic premise in Machine Learning is that computers form models of the world and, through a lot of trial end error, figure out how to tweak the models they use to get better results.

At the risk of wading too far into a swamp of poorly defined terms, the model and parameters that a computer uses are a computer’s “tool of thought.” Without the right model and parameters, just like humans without the right concepts, accurately predicting aspects of the world becomes impossible.

One glaring difference between a computer’s predictive models and ours’ is that we consciously experience utilizing what we know. Not only are we aware of information passing through minds and bodies, but if you pay attention, you’ll notice that thinking about different subjects or different people corresponds to a different flavor of subjective experience. Yet computers capable of machine learning pose a challenge: if reasonably accurate predictions can be made without conscious awareness, why in Darwin’s name are we conscious at all?

To understand what advantages consciousness confers for predicting the world, let’s consider the case of unconscious computer prediction. Imagine you had a digital camera strapped to your bike helmet. You could, if you wanted, instruct a computer to predict what kinds of colors it would see next during your commute. If you were to use modern machine learning algorithms, you would avoid giving the computer “smart rules” regarding how to go about making the predictions. “Smart rules” are the kinds of things we humans can observe and articulate when we think we are being clever, like yellow is a rare color in many urban areas, so don’t expect it or mauve and teal are only likely during fashion week in New York, or whenever Christo rolls through town.

Verbalized smart rules often fail to cover edge case, and are usually not precise enough to be encoded. By the time you’d come up with all the exceptions, and exceptions to the exceptions, and so on, French verb conjugations would feel like a walk in the park. And by the time you pin down precise instructions regarding what constitutes a patch of color in a computer image (do you count individual pixels? What do you do about the fact that subsequent pixels rarely encode the same color?), and how you’ll account for the fact that your eyes see color rather differently than computers (e.g. your eye and brain automatically color corrects for changes in the ambient light), and so on, not only your code would be a mile longer than this sentence, your sanity would have completely evaporated, leaving you to be nothing more than a hollowed-out shell of a person, only capable of making the simplest of sacrificial offerings to Mondrian and Rothko, who painted in seemingly simple blocks of pure color.

Instead of smart rules, modern machine learning algorithms take another approach entirely. If we apply the modern machine learning approach to the problem of bike-route color prediction, part of the instructions — or code — you’d develop would define how the computer would know if its predictions were improving. I admit that “minimize the error associated with each prediction” isn’t exactly a James-Bond-worthy mission to charge your computer with. But as long as computers can’t drink martinis, we should be okay with giving them boring things to do. And once you define how the computer should measure its own error, you can leave it to work things out on its own.

Prediction-error has turned out to be both very difficult and a very fruitful thing to attempt to minimize. One of the biggest single advances in unconscious computer prediction happened about thirty years ago, when David Rumelhart, George Hinton, and Ronald Williams all figured out how a key aspect of convincing a computer to minimize its own prognostic errors. Their breakthrough was in figuring out how to get the computer to fairly distribute responsibility (or blame) for the prediction errors across all of the parts of the prediction process.

It sounds simple in retrospect: instead of coming up with smart rules, you can have the computer come up with the rules itself, as long as it knows how to improve. Since then, almost all progress in machine learning and artificial intelligence has consisted of developing fancier ways of utilizing the technique they discovered — the reason for the boom in AI over the past decade is that computer hardware has finally gotten cheap enough to effectively distribute blame.

But what would you do if you weren’t sure of the problem in advance? What if the only thing you knew was that your information processor would come across completely novel problems?

Well, you’d probably take the best general prediction-error minimizer you could conjure, and hook it up to something that could define its own problems. This way, no matter what problem your information processor defines for itself, it can learn.

Roughly speaking, this describes how (parts of) you work. When you learn to throw a baseball, you don’t think about the path of your arm in terms of coordinate positions, or in terms of numerically specified velocity. Instead, you attempt to throw a ball, observe the result, and try again. The specific learning takes place on its own. This general idea applies to other parts of your brain too — you consciously choose which problems to solve, and then use a combination of conscious and unconscious processing to solve them.

Choosing what problems are worth pursuing in the first place is much harder than doing the “learn by error minimization” task, especially if you use the same paradigm of being blind to the qualities of information you work with. In a computer, the processor is robbed of context — it “knows” nothing about what the rest of the computer is doing, only that certain operations should be applied to bits. More than that, the main processing chip in your laptop does the same sorts of things regardless of whether you are watching a video of a hydraulic press squish a toy or whether you are working on financial models via a spreadsheet.

If you made a computer conscious and kept everything else the same, their internal experience of information they process would all feel uniform. Or to flip this metaphor around, if we were to be mostly unconscious, understanding links on Reddit would feel exactly the same as processing the information from the wind on your face. It is this uniformity of information makes it nearly impossible to define problems worth solving, because all problems worth solving, not to mention almost all of the tasks we do on a daily basis, have lots of aspects to consider.

Why? If a problem has a lot of aspects to consider, it is technically “multi-dimensional.” Having only one experience of data, no matter what it is you are processing or computing, doesn’t work for trying to quickly understand and solve multi-dimensional problems. The problems we solve on a daily basis are so multi-faceted, in fact, that we use our attention to ignore what we deem irrelevant — there is no way to efficiently process everything. But by consciously experiencing information in fundamentally different ways (emotion, music, sound, touch), we gain access to fundamentally, irreducibly different types of data. And we call the perception of data experience.

Physiological thirst, for example, corresponds to your body detecting the water content (and osmotic pressure) of select cells. When enough cells are low on water, we experience this information as the visceral feeling of thirst. Imagine if the information content that corresponds with being thirsty and everything else were to use the same mechanism and present itself as push notifications to your smart phone. It would be much more difficult to distinguish vital signals from ones you could safely ignore. As such we’d likely all develop elaborate systems to help filter the signals we were given. Instead, it is much more efficient to have the feeling of thirst be visceral. By being able to perceive the world in multiple, irreducible ways, we are able to use a larger set of tools to quickly perceive, reflect, decide, and learn.

For instance, one way to understand a complex stream of data is via Chernoff Faces, a technique of mapping numbers to parameters that describes faces. The three Chernoff faces below, by Christo Allegra, each represent several aspects of Apple’s financial data for their respective year.

In addition to Chernoff Faces, one can imagine Chernoff landscapes or forests. Yet a Chernoff inspired data representation is only possible with conscious experience. Without conscious experience, working with the same information would be like working with a large spreadsheet full of numbers or a long list of pixel-values. Non-verbal, conscious sensations and different senses allow more than one dimension of data to be experienced.

In other words, consciousness is the most efficient route to context. It is within the context of how our body is feeling and what we have in the fridge and how we feel about the foodstuffs in said fridge that we make the decision about what to have for breakfast. It is much easier to make decisions by perceiving these contextual pieces of information as different types of sensations rather than have them all be reduced to the same kind of information processed by a blind mechanism.

When I said that the entirety of human existence could be described as a progression towards better predictions and better tools for predicting stuff, I wasn’t kidding. As I alluded to above, evidence for a long-term trend of striving for more predictability can be found in several places, particularly in science.

Scientific progress has generally concerned itself with explaining and predicting what would otherwise be random data points and facts that don’t quite make sense on their own. Want to know what happens when you split an atom? Is there a formal way to figure out what happens if you take one thing and mix it with another? What if you took three pounds of Taylor Swift CDs and used them to reflect light onto a small sphere? How much useful energy could you create? Would there be any meaningful difference between using her early and her late recordings? Would Mozart symphonies be any better?

As covered above, predicting these things all require domain-specific concepts, mental models and formalized rules of thumb that have been developed as a part of the scientific method. In certain cases, the scientific progress towards attempting to make the world more predictable hasn’t been in terms of explaining data per say, but in these cases the progress has been in terms of how to go about the process of making the predictions more accurate. For instance, in the 14th century, a monk named Roger Bacon articulated the abstract concept of experimentation. In what is one of the more important breakthroughs of all time, he helped make the rate of true-knowledge acquisition (aka scientific progress) much more predictable.

My point is that the history of progress can be seen as a long-term trend of striving towards more predictability. While some aspects of the world are more predictable now, the large increase in the planet’s population means that the world isn’t always becoming uniformly more predictable — human beings are the most complex and hardest to predict things we know of, and more than seven billion of them makes for an interesting and random world. However, an orientation towards more predictability is, on some level, a constant in every person’s life, in every society, in every country, and every city on the planet.

If you think about it, this isn’t just a human trend — the history of evolution consists of lifeforms evolving to better predict the environment they find themselves in. In terms of predicting where food will be next, we’re a hell of a lot better at it than bacteria are.

What’s curious is that the laws of physics tell us that it always takes energy to move and do any sort of work, including getting food. This means that any living thing must attempt to conserve energy, and one great way to do so is to not move to the right when the deer (or complex sugars) are to the left because you couldn’t see them or detect them or predict that they would be there.

The implication is that over the long term, any spark of life will eventually evolve to include lifeforms that are on some level aware of the data they process. Given time, the mechanism of random mutation, in the context of a world with multi-dimensional problems, the impulse towards living and reproducing will change from a bacterium to a creature that is conscious of some of the information passing through it. All that it takes is a) a system of non-homogenous self-replicating units, b) said units to have the ability to utilize information to alter their behavior, c) issues of survival to be able to be described in terms of high information dimensionality, and d) lots of time.

Whether it is on Mars or on a planet way beyond Alpha Centauri, once there is life, you can extrapolate that at some point in the future the creatures that evolve from it will have conscious experiences. Though the conditions for life to flourish are exceedingly rare, the impulse towards increasing in complexity resulting in conscious is not rare at all.

The title for this piece comes from a phrase I heard when noted neuroscientist and consciousness chaser Christof Koch spoke at Berkeley a few years ago. When he spoke about the concept then, I understood it on some level, but was unable to articulate it as I understand it now: just as the conditions for plasma or ice are baked into the physical laws that govern our world, so too are the conditions for consciousness. In the most non-mystical way possible, conscious experience is an inevitable product of the structure of the universe.

Bridges, John Henry. “The Life & Work of Roger Bacon: An Introduction to the Opus Majus.” (1914).

Koch, Christof, et al. “Neural correlates of consciousness: progress and problems.” Nature Reviews Neuroscience 17.5 (2016): 307–321.

McKinley, Michael J., and Alan Kim Johnson. “The physiological regulation of thirst and fluid intake.” Physiology 19.1 (2004): 1–6.

Xu Yang, Terry Regier, and Barbara C. Malt. “Historical semantic chaining and efficient communication: The case of container names.” Cognitive science (2015).|||

I get it; pondering consciousness sounds like an activity only enjoyed by unashamed philosophy majors who are either high or who have found a moment of post-yoga stillness. But consider that we don’t…