Using many computers to count words is a tired Hadoop example, but might be unexpected with TensorFlow. In 50 lines, a TensorFlow program can implement not only map and reduce steps, but a whole MapReduce system.

The design will have three roles, or jobs. There will be one task in the job, distributing units of work. There will be one task for the role, keeping track of results. There could be arbitrarily many tasks doing the job, but for this example there will be two.

TensorFlow programs often use (parameter server) and tasks, but this is largely a convention. The program here won't follow the convention.

The four tasks can run on four computers, or on fewer. To stay local, here's a cluster definition that runs them all on your local machine.

I have a script that produces four shell scripts (1, 2, 3, 4). Each should be sourced (like ) in a separate shell. This setting of environment variables is work that could be handled by a cluster manager like Kubernetes, but these scripts will get it done.

Every task will run . The first few lines establish the cluster.

For more on TensorFlow clusters, see TensorFlow Clusters: Questions and Code.

To avoid lots of indentation, I'm not using as a context manager, which is fine for this example. I'll similarly avoid blocks when reading files later.

For distributed data processing to make sense, you likely want a distributed file system like HDFS providing a way for workers to grab chunks of data to work on. You might have hundred-megabyte TFRecords files prepared, for example.

For this example, we'll use a dataset of 22 small text files. We'll generate a list of filenames. Then, assuming every member of the cluster can access the file system, we can give a worker a filename and have it read the file.

The task will host a queue of filenames.

This code will be executed by every member of the cluster, but it won't make multiple queues. The context specifies that the queue lives on the task machine, and the uniquely identifies this queue. So just one queue gets made, but every member of the cluster can refer to it with the Python variable name .

The next part of the code only runs on the task:

Device assignment here is left up to TensorFlow, which is fine.

The task uses normal Python file reading to get filenames from and put them in the queue.

I'm loading the queue explicitly to avoid getting into and ; you could also use a , for example.

Then the task runs , which keeps it running so that the queue doesn't disappear. We'll have to kill the process eventually because it won't know when to stop. This is another thing a cluster manager could be responsible for.

There's just going to be one variable storing the total word count.

Like the code defining the queue, every task in the cluster will run this code, so every task in the cluster can refer to this variable. Variables in specific places are "de-duplicated" using instead of .

There's very little code that only the task runs.

The task initializes and then displays the current value of every two seconds.

It would be a bit more like Hadoop MapReduce to have the reducer explicitly receive data emitted from mappers, perhaps via another queue. Then the reducer would have to run some code to reduce down data from that queue.

The absence of any reducing code in the task demonstrates the way distribution works in TensorFlow. The task owns a variable, but we can add to that variable from another machine.

Like the task, the task doesn't have any way of knowing when the counting process is done and then shutting down, which I think is okay for this example.

The task has already run code establishing the filename queue and the total word count variable. Here's what each task does:

Each task pulls a filename from the queue, reads the file, counts its words, and then adds its count to the total.

A task will run until the queue is empty, and then it will die with an . This would be a little more careful:

Inside the task, the actual work that's done is not part of the TensorFlow graph. We can execute arbitrary Python here.

There's a five second pause in the loop so that things don't happen too fast to watch.

The total word count is stored off in the task, possibly on a different computer, but that doesn't matter. This is part of what's cool about TensorFlow.

To execute, we run in four places with the appropriate environment variables set. That's it. We've counted words with many computers.

Programming a cluster with TensorFlow is just like programming a single computer with TensorFlow. This is pretty neat, and it makes a lot of things possible beyond just distributed stochastic gradient descent.

The example above demonstrates a distributed queue. If you can do that, do you need a separate queueing system like RabbitMQ? Maybe not in every situation.

The example above sends filenames to workers, which is a pretty general model. What if you feel comfortable sending executable filenames, or some representation of code? You might implement something like Celery pretty quickly.

The example I've shown can probably fail in more ways than I even realize. It would be more work to make it robust. It would be again more work to make it more general. But it's pretty exciting to be able to write something like this at all.

And while even the typical TensorFlow distributed training is itself a kind of MapReduce process, TensorFlow is general enough that it could be used for wildly different architectures. TensorFlow is an amazing tool.

The files produced by (1, 2, 3, 4) look like this:

And here's all together:

All the files needed to demo this are together in a repo on GitHub.

I'm working on Building TensorFlow systems from components, a workshop at OSCON 2017.|||

