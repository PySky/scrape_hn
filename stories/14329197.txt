This question is particularly relevant for supervised training problems. The typical premise underlying such problems is that a small high-quality dataset (say N entities) can help your model approximate an underlying function, which can generalize to your entire dataset (1000N entities). The allure of these approaches, of course, is that humans do the hard work on a small amount of data, and machines learn to replicate the work for a wider range of examples.

In the real world though, problems don’t always have an underlying pattern that can be identified. Humans draw on external general knowledge to solve cognitive challenges more often than we realize, which often leads us to falsely expect our algorithms to be able to solve the same challenges, without the benefit of the general knowledge that we posses.

Two of the three names represent the same product. Can you find the odd one out?

Most Americans wouldn’t have a problem with this, because the fact that AR=Arkansas and AR!=Arizona is common knowledge. Some might not even notice the nuance in the question. Find someone who isn’t familiar with the US though, and they’ll probably get their answers mixed up.

Your freshly minted neural network has little chance of solving a problem like this, unless it’s seen a specific instance of Arkansas equating to AR. There’s no underlying rule here to mimic, since the mapping of abbreviation to state name is linguistically arbitrary.

Problems like these, (some difficult to even grasp in the first place), arise all the time when dealing with real-world challenges. It can be incredibly hard to look past your own conditioning, and identify what additional knowledge your network requires to think the way you do. This is why an active effort to step outside of your own conditioned mental frameworks can be quite useful.|||

Working on data-science problems can be both exhilarating and frustrating. Exhilarating because the occasional insight that boosts your algorithm’s performance can leave you with a lasting high…