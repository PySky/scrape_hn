This is the second post covering Baidu’s Deep Voice paper that applies Deep Learning to Text to Speech Systems.

In this post, we’ll cover how we actually train each part of this pipeline using labeled data.

The last post covering the inference pipeline is here for your reference:

2. Predict the durations and frequencies of each phoneme.

3. Combine the phonemes, the durations, and the frequencies to output a sound wave that represents the text.

But how do we actually train the models used in each of the above steps to produce reliable predictions?

Deep Voice uses the training pipeline shown above to train the models used during inference.

Let’s walk through each of the pieces in this pipeline and see how they help us train the overall system. Here’s the data we use for training:

Text, along with recordings of actors voicing out the given text.

The first step in inference is to convert text into phonemes using a Graphene-to-Phoneme model. You may recall this example from the previous post:

Note that in many cases this system can just pass the text into a phoneme dictionary (like this one from CMU) and return the output!

But what if it sees a new word it hasn’t seen before? This is likely to happen as we constantly add new words to our vocabulary (“google”, “screencast”, etc.). Clearly, we need a fallback that predicts phonemes when we encounter new words.

Deep Voice uses a neural network to accomplish this task. In particular, it leverages the work done by Yao and Zweig at Microsoft Research around Sequence to Sequence (Seq2Seq) learning to predict phonemes for text.

Rather than trying to explain these models in-depth myself, I’m going to point you to some of the best resources I’ve found that explain them:

So what do the training data and the labels for this actually look like?

We obtain our input and label pairs from a standard phoneme dictionary like this one from CMU.

As you may recall, during inference we need to predict both the duration of a phoneme and its fundamental frequency (underlying tone). We can obtain both the duration and the fundamental frequency easily from the audio clip of the phoneme.

Deep Voice uses what they call a Segmentation model to get these audio clips of each phoneme.

The Segmentation model matches up each phoneme with the relevant segment of audio where that phoneme is spoken. You can see a high level overview of its inputs and outputs in the figure below:

What’s particularly interesting about the implementation of the Segmentation model is that instead of predicting the position of individual phonemes, the model actually predicts the position of pairs of phonemes. Additionally, this model is unsupervised as we don’t have ground truth labels of the position of phonemes in the audio clip. It is trained on CTC loss which you can read about here.

Here’s what the data for this model looks like:

Why predict the position of pairs rather than individual phonemes? When we predict the probability of a given timestamp corresponding to a phoneme, the probability is maximum at the middle of the utterance of that phoneme.

But, with pairs of phonemes, the probability is a maximum at the position that happens to be their boundary (see above). Hence, using pairs allows us to easily find the boundaries between phonemes.

At the end of this process, we should have a clear idea of where each phoneme occurs in the audio clip.

Now that we have the durations and the fundamental frequencies from the segmentation model, we can now train models to predict both these quantities for new phonemes.

Deep Voice uses a single, jointly trained model that outputs both of these values. Here’s what the shape of the data will look like for training this model.

And with that, we’ll be able to carry out duration prediction and F0 prediction!

Finally, we need to train the piece of our pipeline that actually generates human sounding audio. Much like DeepMind’s WaveNet, this model has the following structure:

And here’s what the inputs and labels will look like to the model.

With this, we’ve trained all pieces of our pipeline and can successfully run inference.

Congratulations on making it this far! By now, you’ve seen both how Deep Voice generates new audio and how it is trained in the first place. To summarize, here are the steps to training Deep Voice:

And that’s it! Thank you for taking the time to read through this pair of posts on Baidu’s Deep Voice. If you have any suggestions on how I can make it better feel free to drop a comment and I’ll do my best to improve.

For the next paper, I’ll aim to cover one of the many recent papers applying Convolutional Neural Networks to problems in medical imaging. Keep an eye out for it and see you then!|||

This is the second post covering Baidu’s Deep Voice paper that applies Deep Learning to Text to Speech Systems. 3. Combine the phonemes, the durations, and the frequencies to output a sound wave that…