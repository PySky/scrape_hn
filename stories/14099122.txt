Torch implementation for learning an image-to-image translation (i.e. pix2pix) without input-output pairs, for example:

This package includes CycleGAN, pix2pix, as well as other methods like BiGAN/ALI and Apple's paper S+U learning.

 The code was written by Jun-Yan Zhu and Taesung Park.

 See also our ongoing [PyTorch] implementation for CycleGAN and pix2pix.

The test results will be saved to .

 Please refer to Model Zoo for more pre-trained models. is an example script that downloads the pretrained Van Gogh style network and runs it on Efros's photos.

The test results will be saved to a html file here: .

Download the pre-trained models with the following script. The model will be saved to .

Models are saved to (can be changed by passing in train.lua).

 See in for additional training options.

This will run the model named in both directions on all images in and .

 Result images, and a webpage to view them, are saved to (can be changed by passing in test.lua).

 See in for additional test options. Please use if you only would like to generate outputs of the trained network in only one direction, and specify or to set the direction.

Download the datasets using the following script:

Optionally, for displaying images during training and test, use the display package.

By default, the server listens on localhost. Pass to allow external connections on any interface:

Then open in your browser to load the remote desktop.

To train CycleGAN model on your own datasets, you need to create a data folder with two subdirectories and that contain images from domain A and B. You can test your model on your training set by setting in . You can also create subdirectories and if you have test data.

You should not expect our method to work on just any random combination of input and output datasets (e.g. ). From our experiments, we find it works better if two datasets share similar visual content. For example, works much better than . achieves compelling results while completely fails. See the following section for more discussion.

Our model does not work well when the test image is rather different from the images on which the model is trained, as is the case in the figure to the left (we trained on horses and zebras without riders, but test here one a horse with a rider). See additional typical failure cases here. On translation tasks that involve color and texture changes, like many of those reported above, the method often succeeds. We have also explored tasks that require geometric changes, with little success. For example, on the task of transfiguration, the learned translation degenerates to making minimal changes to the input. We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method. In some cases, this gap may be very hard -- or even impossible,-- to close: for example, our method sometimes permutes the labels for tree and building in the output of the cityscapes photos->labels task.

If you use this code for your research, please cite our paper:

If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection:

 [Github] [Webpage]

Code borrows from pix2pix and DCGAN. The data loader is modified from DCGAN and Context-Encoder. The generative network is adopted from neural-style with Instance Normalization.|||

CycleGAN - Software that can generate photos from paintings,  turn horses into zebras,  perform style transfer, and more.