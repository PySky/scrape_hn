This post describes the theory behind the newly introduced BEGAN.

As a bonus, I’ve added a comparison between the BEGAN and the Improved WGAN at the end.

You can comment on the reddit page associated this post, or on the reddit page associated to the paper.

Generative Adversarial networks (GANs) have achieved impressive (and often state-of-the-art) results in various domains such as:

Here at Heuritech (a french start-up) we aim to use these models (and many others) to help consumers, searchers, corporations and kids with their day-to-day deep learning needs. As things turned out, a big part of our focus currently lies within the world of fashion (because Heuritech is also a stylish, well-dressed company), but who knows where we’ll be just a few years from now!

Heuritech has been founded by four doctors in Artificial Intelligence and is very research oriented. If you’re interested to be part of the adventure you can contact us here.

Recently, a new GAN architecture (the Boundary Equilibrium GAN) caught my interest by successfully generating anatomically coherent faces at a resolution of 128×128 pixels. As of early April 2017, this is the current state of the art. Have a look:

It’s all about providing a better loss to the networks.

It has been previously shown that the first GAN architecture minimizes the Kullback-Leibler divergence between the real data distribution and the generated data distribution . An unfortunate consequence of trying to minimize this distance is that the discriminator D gives meaningless gradients to the generator G if D gets too good too quickly.

Since then, a few publications focused their effort on trying to find better loss functions:

The main goal behind the BEGAN is also to change the loss function. This time, it is achieved by making D an autoencoder. The loss is a function of the quality of reconstruction achieved by D on real and generated images. This idea (of making D an autoencoder) is inspired by the Energy Based GAN (EBGAN). Below is the architecture of the EBGAN shamelessly copy-pasted from the paper:

Let’s start by clarifying something important. The reconstruction loss is not the same thing as the real loss that the nets are trying to minimize. The reconstruction loss is the error associated to reconstructing images through the autoencoder/discriminator. In the EBGAN schema the reconstruction-loss is referred to as « Dist » and the real loss is referred to as « L ».

The main idea behind the BEGAN is that matching the distributions of the reconstruction losses can be a suitable proxy for matching the data distributions. The real loss is then derived from the Wasserstein distance between the reconstruction losses of real and generated data. Later, the networks are trained by using this real loss in conjunction with an equilibrium term to balance D and G.

I’d like to spoil you right away with the solution to give you an understanding of where we’re going.

Points 1 and 2 can be rephrased as « D tries to discriminate real and generated distributions ». So G can only succeed with point 3 by generating more realistic images.

Let’s first derive the real losses and we’ll focus on the equilibrium term later. What we want at this point is to use the Wasserstein Distance between the reconstruction losses to derive the real losses.

If we use an L1 norm between the input image of D and its reconstructed version, then the loss distribution (refered to as ), is approximately normal (at least it looks like it experimentally). We’re about to use this fact to simplify the Wasserstein distance between the reconstruction losses (of real and generated images).

Here is the Wassertein distance mathematically:

 

 with:

In the end, this is the simplified distance that we want to minimize:

 

 And from this formulation we can derive the GAN objective.

The reconstruction losses and can only be positive, so as D wants to maximize the distance between the losses, it has only two choices:

Case 1 doesn’t make sense of course because we want to go 0 (real images should be reconstructed perfectly). We can enforce case 2 by applying the following loss function to D:



G can then work adversarially to that by trying to minimize:



In the end these losses (which are not the final ones) look very similar to those of the WGAN except:

But this is not the end of the story.

In the next section I describe the second main contribution of the BEGAN paper: . In short, the role of ‘s is to balance the losses and to stabilize the training. This is done in an adaptive fashion, during training, with the help of its surrogate .

Let me first spoil you with the final solution and I’ll explain how and work in the next section.

What we aim to do now is to balance the losses however we want. This will help us control the equilibrium between G and D in real time, stabilize the training, get an approximate measure of convergence and adjust the trade-off between image diversity and realism with just one hyperparameter. Magic!

The reconstruction losses are considered to be at equilibrium when:



And the thing is we don’t want this equilibrium to ever be reached because this would mean that D become incapable to distinguish generated samples from real ones. Said differently this would mean G wins. For the training to go smoothly, neither network should win over the other.

The strategy is to define a ratio between the 2 losses.



is called the diversity ratio and it is should be in . It is always positive because the reconstruction losses are always positive. And it is below 1 in practice because we are going to make it so (because we want to have )

We will make sure this ratio is maintained during the training in the next section, but first I would like to give you an intuition of the importance of .

D has 2 competing goals: auto-encode real images and discriminate real from generated images. helps us balance these goals:

Take a look at what happens with different values of the hyperparameter :

The strategy, now, is to maintain the ratio between the 2 reconstruction losses over time. In practice we can control by adding an adaptive term .

What makes the adaptive is something called « Proportional Control Theory ». It is a fancy name to describe what you’re doing when you’re driving at constant speed. If you’re going too fast you slow down proportionally to how much faster (than the cruise speed) you’re going. If you’re going too slow you accelerate proportionally to how much slower (than the cruise speed) you’re going.

Here, the ratio we want to maintain (the speed of the car) is the ratio defined as:

 

 So in a perfect world we should have exactly this during training:

 

 which is equivalent to:



In practice is never equal to 0 during training. This value represents how off we are from the stable point (this is how much faster or slower your car in really going).

Now that we have this information we need to control this ratio dynamically during the training (we need to maintain the correct speed while driving). And the factor that controls this ratio is through the formula: .

is adapted in the right direction like this: with being the learning rate that adapts over time. is also called the proportional gain for k and in practice (in the experiments) it is set to 0.001.

It is this formulation of that justifies the complete BEGAN objective presented above.

Training procedure: In most GANs, D and G are trained alternatively. This is not the case with here. They can be trained simultaneously at each time step, because D and G are automatically balanced. The training is still adversarial, it is just simultaneous.

There is no need to have show up in : This is because the ratio (that controls) is there to balance the goals of D. It has nothing to do with the generator which has only one objective. So, in short, G does its own thing and D adapts. D adapts how much it wants to focus on its discrimination task (in which case D gets better faster than G) relative to its auto-encoding task (in which case G gets better faster than D).

An convenient consequence of this BEGAN formulation is that it makes the derivation of an approximate convergence measure possible.

Let’s look at the complete BEGAN objective from another point of view:

The combination of both points implies that the reconstruction losses get closer to 0 (and therefore to one another). This means that D has more and more trouble maximizing the distance between and (by minimizing ). Said differently the task of discriminating real from generated images becomes more difficult as time goes on. And there is only one way that this happens: G is getting better!

In short, the combination of (1) and (2) implies that goes to 0 and that gets closer to .

Now here is the awesome part: (1) and (2) have such simple forms that we can simply add them up to get the following convergence measure:



Why is this awesome? Because global measures of GAN convergence are hard to come by. There isn’t any in the typical GAN setup and to my knowledge only the WGAN provides one.

But now there is a way to know if the network converges or collapses:

Interpolation: To display the interpolations of real images in latent space (Figure 4) the authors needed to obtain the embedding of real images. This is done by training by providing the real image as a target of the generator. It is an L1 loss that is minimized in this case (between the generated image and the target image): . cannot be used in place of because there is nothing that constrains to converge toward .

Overview: In the end, the architecture is pretty simple. As they say in the paper: « no batch normalization, no dropout, no transpose convolutions and no exponential growth for convolution filters ». Any of those techniques, though, may improve the results even more.

The secret trick: In Figure 2 of the paper the authors put side to side the results from the EBGAN and those from the BEGAN. The BEGAN results look much much better than the EBGAN results, but you should keep in mind that the models were not trained on the same datasets.

As a side note: it is common knowledge that « averageness » is a strong indicator of beauty (there are fourteen sources to corroborate this statement on the wikipedia page for Beauty). The idea is that the more you blend faces together the more beautiful they look.

The faces generated by BEGAN are not just visually realistic they also seem to be very attractive (the authors even claim that they saw few older people and more women than men). My understanding is that G could have developed a good representation of the « average » face to be more efficient, and the beauty of the generated faces could be a consequence of this.

First, they have a lot in common:

So which one is better? It doesn’t matter. Everybody is a winner!

My name is Loris Felardos and you can contact me at: felardos [dot] loris {at} gmail (dot) com.

The author (that’s me) would like to thank Charles Ollion for his very insightful comments and advice.

Here is the reddit page associated this post

Here is the reddit page associated to the paper.|||

TL;DR This post describes the theory behind the newly introduced BEGAN. As of early April 2017, the BEGAN is the state of the art when it comes to generating realistic faces. It is inspired from the EBGAN in that its discriminator is an auto-encoder. It shows fast and stable convergence even in the absence of…