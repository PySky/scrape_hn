This is a TensorFlow implementation of the model described in:

The Skip-Thoughts model is a sentence encoder. It learns to encode input sentences into a fixed-dimensional vector representation that is useful for many tasks, for example to detect paraphrases or to classify whether a product review is positive or negative. See the Skip-Thought Vectors paper for details of the model architecture and more example applications.

A trained Skip-Thoughts model will encode similar sentences nearby each other in the embedding vector space. The following examples show the nearest neighbor by cosine similarity of some sentences from the movie review dataset.

First ensure that you have installed the following required packages:

You can download model checkpoints pretrained on the BookCorpus dataset in the following configurations:

You can now skip to the sections Evaluating a Model and Encoding Sentences.

To train a model you will need to provide training data in TFRecord format. The TFRecord format consists of a set of sharded files containing serialized protocol buffers. Each proto contains three sentences:

Each sentence is a list of words. During preprocessing, a dictionary is created that assigns each word in the vocabulary to an integer-valued id. Each sentence is encoded as a list of integer word ids in the protos.

We have provided a script to preprocess any set of text-files into this format. You may wish to use the BookCorpus dataset. Note that the preprocessing script may take 12 hours or more to complete on this large dataset.

When the script finishes you will find 100 training files and 1 validation file in . The files will match the patterns and respectively.

The script will also produce a file named . The format of this file is a list of newline-separated words where the word id is the corresponding 0- based line index. Words are sorted by descending order of frequency in the input data. Only the top 20,000 words are assigned unique ids; all other words are assigned the "unknown id" of 1 in the processed data.

Execute the following commands to start the training script. By default it will run for 500k steps (around 9 days on a GeForce GTX 1080 GPU).

Optionally, you can run the script in a separate process. This will log per-word perplexity on the validation set which allows training progress to be monitored on TensorBoard.

Note that you may run out of memory if you run the this script on the same GPU as the training script. You can set the environment variable to force the script to run on CPU. If it runs too slowly on CPU, you can decrease the value of .

If you started the script, run a TensorBoard server in a separate process for real-time monitoring of training summaries and validation perplexity.

The vocabulary generated by the preprocessing script contains only 20,000 words which is insufficient for many tasks. For example, a sentence from Wikipedia might contain nouns that do not appear in this vocabulary.

A solution to this problem described in the Skip-Thought Vectors paper is to learn a mapping that transfers word representations from one model to another. This idea is based on the "Translation Matrix" method from the paper Exploiting Similarities Among Languages for Machine Translation.

Specifically, we will load the word embeddings from a trained Skip-Thoughts model and from a trained word2vec model (which has a much larger vocabulary). We will train a linear regression model without regularization to learn a linear mapping from the word2vec embedding space to the Skip-Thoughts embedding space. We will then apply the linear model to all words in the word2vec vocabulary, yielding vectors in the Skip- Thoughts word embedding space for the union of the two vocabularies.

The linear regression task is to learn a parameter matrix W to minimize || X - Y * W ||2, where X is a matrix of Skip-Thoughts embeddings of shape , Y is a matrix of word2vec embeddings of shape , and W is a matrix of shape .

First you will need to download and unpack a pretrained word2vec model from this website (direct download link). This model was trained on the Google News dataset (about 100 billion words).

Also ensure that you have already installed gensim.

The model can be evaluated using the benchmark tasks described in the Skip-Thought Vectors paper. The following tasks are supported (refer to the paper for full details):

You will need to clone or download the skip-thoughts GitHub repository by ryankiros (the first author of the Skip-Thoughts paper):

You will also need to download the data needed for each evaluation task. See the instructions here.

For example, the CR (customer review) dataset is found here. For this task we want the files and .

In the following example we will evaluate a unidirectional model ("uni-skip" in the paper) on the CR task. To use a bidirectional model ("bi-skip" in the paper), simply pass the flags , and instead. To use the "combine-skip" model described in the paper you will need to pass both the unidirectional and bidirectional flags.

The output is a list of accuracies of 10 cross-validation classification models. To get a single number, simply take the average:

In this example we will encode data from the movie review dataset (specifically the sentence polarity dataset v1.0).

ipython In [ ]: absolute_import division print_function numpy np os.path scipy.spatial.distance sd skip_thoughts configuration skip_thoughts encoder_manager In [ ]: The following directory should contain files rt-polarity.neg and In [ ]: Set up the encoder. Here we are using a single unidirectional model. To use a bidirectional model as well, call load_model() again with configuration.model_config(bidirectional_encoder=True) and paths to the bidirectional model's files. The encoder will use the concatenation of encoder encoder_manager.EncoderManager() encoder.load_model(configuration.model_config(), , , ) In [ ]: data [] (os.path.join( , ), ) f: data.extend([line.decode( ).strip() line f]) (os.path.join( , ), ) f: data.extend([line.decode( ).strip() line f]) In [ ]: Generate Skip-Thought Vectors for each sentence in the dataset. encodings encoder.encode(data) In [ ]: ( , ): encoding encodings[ind] scores sd.cdist([encoding], encodings, )[ ] sorted_ids np.argsort(scores) ( ) ( , data[ind]) ( ) i ( , num ): ( (i, data[sorted_ids[i]], scores[sorted_ids[i]])) In [ ]: Compute nearest neighbors of the first sentence in the dataset. get_nn( )|||

models - Models built with TensorFlow