Clicking on web 1.0 home pages is like flicking through a 90’s magazine or show: it’s hard to ignore the colorful backgrounds, blinking bars and glittery gifs matching any mood or desire. It seems, the color and movement stimuli chanell our attention towards apolitical aesthetics of 90’s artifacts. Maybe that’s why, the history of the web too often reads something like: a handful of geeks invented a neatly organized net in secret military chambers, then a huge crowed of childish amateurs and crazy futuristic artists invaded this space and transformed it into an excessively colorful, noisy and dazzling awful looking cyber mess. After a short period of anarchy, the commercial web designer and his buddies, the psychologist and the programmer, came to save the online world from disorder through data informed professional web design. Followed by wars, annexations, communism turned nationalism turned algorithmic capitalism, thumbs and birds, angels and unicorns–happy times, bad times and alternative times.

As the web matured, the home pages of the vernacular web remained taken for granted as playful, silly, immature and somehow embarrassing. The spelling home page stresses the distinction from the homepage, which is the landing or first page of commercial websites. Olia Lialina and Dragan Espenscheid coined the early years of the web between the mid 90’s and early 2000’s as digital folklore. They argue, its specific value can be attributed to the real person [who] created the site and not some marketing department or a content management system. On One Terabyte of Kilobyte Age, a blog reflecting on the reconstruction of the once popular free web hosting platform Geocities (which was shut down by Yahoo! in 2009), Lialina and Espenscheid take the amateur as the point of inquiry. Describing the ‘look and the feel’, the colors and gifs, these posts catch attention because of the intimate information on these home pages. That is, one’s emotions are triggered by personal stories.

The following is an attempt to narrate the stories of home pages’ elements over time – foregrounding the character of the element in its own right. Where did the web counter travel and what is the history of the alert message? Where did they go after web 2.0 and can we still find these elements in contemporary web design? If so, how did their function change over time and how does that affect the users of the net?

As the visible interface of a participatory technology, the professional homepage design contains ‘grammars of action’. These are certain ways to (re)act that are deliberately designed into the layout to encourage standardized user interaction (standardization is a necessary means for scalable profit extraction in capitalistic economic systems). Philip Agre differentiates between two modes of data collection; the surveillance model and the capture model. The latter is a more technological approach, and the rational basis for the former model of surveillance, which is often emotionally loaded with previous experience such as the work of secret services or whistleblower information more recently. In his analysis Agre notes that the latter model of data capturing can never be a purely technical process, as the software has to be programmed by people (with particular histories, assumptions and world views) in the first place. For the information that’s processed by the computer there is always a corresponding attribute to certain people and things in the world –  therefore he argues, data capturing has always a sociotechnical nature.

Both professional and amateur web design is based on software. Its nature to collect data allows it to also feed it back into its informational structures. As data is being fed back into the layout, the ever newly constructed cycle that affords new forms of social interaction consists of grammars that have not been invented but discovered. Web layout then, is a set of grammars that always constitutes phases of imposition which in turn reorganize existing activities. Understanding this dynamic process as an orchestrated performative event where all elements such as images, links, buttons and clinks are carefully coordinated according to the historically collected user data and reinforced by behavior psychology to encourage desired ways of user (re)action. In this essay, I suggest to understand professional layout as particular grammars of action that have been accumulated over time and culture and elaborated through the emergence of professional web design.

When dealing with artifacts from the past temporality becomes an issue. On their blog Lialina writes that in a place where time is invisible, it’s the Geocities’ archive that provides a sense of what it means to age in the digital. But, how do web elements age over time if they are abandoned on the live web – do they become smaller and weaker or wiser and more powerful?

To understand the relationship between the vernacular and contemporary web design I will follow two lines of inquiry. Firstly, I will look at home pages that were made in the 90’s, but were abandoned and left alone to their personal fate in digital desolation. The layout of these private unprofessional home pages has not been changed since around 2000, yet at the same time, they still exist on the live web. A difference to archived material is that their structure changes accordingly to mutable circumstances. Their age becomes visible through signs of the times as new technologies change their appearance: new browser standards, screen resolution, extinction of linked websites and loss of linked images.

The second line of inquiry follows the logical nature of the documentary movie narratives to see how this approach can enrich the understanding and reflection upon contemporary issues. Documentary movies often work with biographical storytelling, of subjects and objects, in order to draw a critical portrait of social and economic actions – what does a critical portrait of a home page look like?

As stated in the title, I think of these home pages as desolated objects in the digital. The mental picture of abandoned buildings makes it possible to reflect upon digital ruins in a way that connects social activities of the past, present and future societies in one big picture. In analogy to abandoned houses, schools, cinemas, warehouses etc., they are contemporary witnesses of leftovers of societies that resonate modern practice to past arrangements.

Someone’s always playing corporation games

 Who cares they’re always changing corporation names

 We just want to dance here, someone stole the stamege

 They call us irresponsible, write us off the page.

By analyzing processes of algorithmic culture sorting and provision, Tatelon Gillespie demonstrates how culture becomes woven into technology itself. In such an environment it can be difficult to trick the logic of Google’s most famous search algorithm PageRank to find web sites that are hidden on the bottom of the popularity and authority hierarchy of present-day web. The location of these websites is problematic due to the absence of automatic crawling and indexing. By the end of the 90’s there were many different directories and search engines that operated on these directories with fix keyword catalogue function. No autofill, no autocorrect functions – misspelling of the URL resulted in a 404 error! page. Hence, tracing these URLs down nowadays, one has to decipher and follow the native logic in which the web functioned in the 90’s.

For obvious reasons typing a few key words into a search bar and hitting enter didn’t return any home pages. Instead, I collected a few 90’s URLs from blogs and threads and checked their original appearance via the Wayback Machine – most home pages were archived at some point. To estimate their creation date there were two ways: for private home pages that were not hosted on free services such as Tripod or Angelfire, any free domain check tools returns the registration date of the domain. However, home pages from free hosting services always return the same registration date of Angelfire or Tripod itself and not the particular home page in question. So, I checked for the date of the first screenshot on the Wayback Machine, or else looked for indicators on the page, such as a notion of time on the bottom or in the text.

While the protocols reassemble the packages into a viewable website, the sound of the MIDI already leaves the digital platform and spreads into the living space of the user. The digitally native sound of the MIDI is gone from the mainstream net, yet, there are traces left in the form of the autoplay attribute for implemented audio or video material on websites. The autoplay attribute is usually not used for some sort of ambient background music, as one might find on websites from labels and music industry websites. Since it has been abused in advertising, autoplaying audio/video is simply super annoying, particularly, if it’s an embedded YouTube video, tutorial or game advertisement that start playing automatically. And especially so, if it’s one of the other 112 tabs, which are open in 13 different windows and 2 browsers.

However, after successfully spreading its charm around the desktop bound web, the MIDI then moved on to mobile phones. This was before it was abandoned by the professionalizing online community and long before the first website discovered a smart phone. The MIDI was used on websites because of the low bandwidth and its very slim structure. For the same reason, it became the technology behind ringtones on brick mobile phones from 1998.

In Digital Folklore Lialina notes that ‘MIDIs are generally considered free to collect, use and share. Audio files never made it to this status, they’re clearly the intruders from a world outside of the web’. As culturally free goods and resources, that exist for reasons external to capitalism they prove essential to capitalist structures. MIDIs, have been used for surplus extraction only after they had already become an established part of digital culture. Background music is often discussed as a primitive design element. Challenging this perception by grounding these practices historically, can provide a critical portrait of the complex relationships between technology, society and economy.

It’s up to the people in which way technology is used by different interest groups. On the net,e amateurs turned the MIDI into a familiar grammar when they started to create these alternative worlds through the realization of possibilities that were provided to them by ‘computer people’. Vilém Flusser writes in Digitaler Schein, that the realization of possibilities allows to realize both an inside (Innen) and an outside (Außen). That is, through the realization of alternative worlds people realize themselves.

For Flusser, it’s because of their exactly calculable thinking why computers are machines that allow to realize intra-personal, inter-personal and extra-personal possibilities.

The line of thought being: assuming that everything computational is grounded in a network of personal relations. Assuming further that the we can be understood as a node of formal digitalized possibilities that as a projection of discrete thinking can be synthesized into new constellations. Simply put, software enables to disregard the limitations of the ‘image’ and allows to realize ideas in infinite ways. Then, what is at stake if we do not study the grammar of digital folklore as an integral part? Do we jeopardize to lose in digital desolation the power to construct alternative worlds according to the ideals of non-professionals and amateurs – the ordinary people? The power to oppose and infiltrate the commodified structure of our contemporary ‘we’?

The header might be still one of the most popular web layout elements today. The evolution of web design is driven by many different factors, one of which is the bandwidth. With faster bandwidth high resolution images, high quality animations and videos can be uploaded. Today, headers are made to be aesthetically persuasive, which is achieved by means of the visual language of the image itself, that’s the content not the layout. The core idea of the header’s position is to catch the visitor’s attention, make her curious for what is to come and keep her on the page in the hope to turn the visitor into a customer through marketing rhetoric.

I’d think amateurs didn’t put a big and catchy visual on the top of their page for financial reasons. Maybe it was a mix of associations with familiar media, such as books and documents, as well as their inscription into the digital technology itself, along with the pure possibility to play with what the new technology had to offer. The header was not a natively digital grammar: the illusion of this particular metaphorical concept was programmed into the computer and continued to transport the illusions of printed mediums such as paper, folders, windows etc., which Alan Kay and Tim Berners-Lee made the main principles of the user interface (UI).

One could look at UI from different angles. Scholars such as the critical media theorist Florian Cramer point out from a linguistics perspective that the design of computer control languages is grounded in human choices, culture and thinking style. These involve ‘fuzzy factors like readability, elegance and usage efficiency.’ It’s unavoidable for humans not to attribute   some meaning to any symbols – in this case the symbols of computer control language. Cramer rightly reminds the reader that these computational symbols themselves do not denote a semantic statement and consequently, it’s the people who make metaphorical associations and thereby read meaning into these symbols.



 Writing from a different perspective, Wesley Phoa, a computer scientist, also identifies this metaphorically entangled sociotechnical context as the ‘problematic nature’ of software systems. In a 1993 essay asking Should computer scientists read Derrida? he notes that, ‘[t]he formal methodists can only apprehend the meaning of their formalisms by means of metaphors deriving from preformal practice …, it must be derived by analogy from some preformal experiences, and so the problematic nature of the informal intrudes itself at the very origin of the formal’. In other words, through the choice of a formal code language e.g.: HTML, for the preformal idea.

Agre combines both perspectives within the model of capture stating that particular grammars, or ways of (re)action, are inscribed into technology through metaphorical references and consequently these grammars shape society as they are imposed through everyday use of this technology. Since technology itself doesn’t produce meaning, it’s important to distinguish between the grammars that programmers write into the system and which metaphors the broader public refers to, who uses these for their own interests.

Coming back to the vernacular use of headers: it’s a familiar grammar to articulate and transport associations, prompt feelings and communicate the taste and mentality or psyche of the home page owner. A very personal and honest way to communicate with the other users. Years before O’Reily’s Web 2.0 became a buzzword, Christopher Locke noted in The Cluetrain Manifesto, that people who make up this new [online] market naturally bring a lot of baggage from their previous experience of mass media. Drawing on the historical unfolding of the web, he compares the period when it became a mass-medium to the previous mass-medium: the TV. Here he distinguishes between TV’s ‘passive couch-potato target demographic’ and the ‘something special’ dimension of the web. This ‘something special’ is the web’s ability to connect the TV audience to itself.

What Locke is pointing out in his early management manifesto is that the logic and the language of the web differ from the logic and the language of classic business management and strategy. Most companies with Net-dot-dollar-signs in their eyes today are still missing the ‘something special’ dimension, he notes. With the proliferation of Web 2.0, online companies picked up the grammar of the amateur web to transport costly researched and constructed, personalized ‘something special’ messages to a target audience. Today, computational data analysis, standardized template web design and human influences use browsing history data, friends, likes and recommendations to personalize advertisements and accelerate messages.

For some reason, amateurs are broadly understood as naive, even silly users. But naivety isn’t interchangeable with stupidity, as it often seems to be implied. Naivety, as in simple-heartedness, has to do with trust and excitement. Marketeers give their best to reconstruct exactly these positive emotions of the vernacular web through content and its relation to each other through layout composition. Behavior research, such as the famous heat map of eye movements, is used to underpin scientifically the layout of a page according to the newest insight of similar research. Yet, what such research is taking as its point of departure are the expectations and impressions of online users, embedded in visual grammars, which were first shaped in the early days. These in turn are based on the grammars which have been inscribed into the software in accordance to experience from other media mixed with data that has then been collected through templates to make users fit the metrics more easily.

The looks of the low resolution image, whose political importance Hito Steyerl described in in great detail in The poor image, should not distract from the fact that amateurs were exposed to these digitalized grammars and carried them on in the digital domain. Putting a header on the top of the page – was a personal statement.

The success and persuasiveness of modern web design, then is the commodification of a website layout which isn’t grounded in intrinsic user behavior, but on visual habits (grammars) which are unconsciously manifested, in users and creators alike. So, what is it that market and consumer behavior research is capturing really? Is it the intrinsic natural behavior of customers, or is it the expectation based on familiarity and habits which in turn are informed by particular grammars and contexts? Do users read the first five Google search results because they are in the top left corner? Or is it because they expect the best results to be within the first couple of results because they are simply used to? Do large headers really communicate the website’s identity and the message that the owners want to get across because of its prominent position and professional content? Why is it that behavior research is being conducted on people who ‘speak’ the internet language? Are less literate users really able to read the intended message in the header? Are they really able to intuitively follow the ‘easy to use’ mantra?

While the page is loading, alerts are a kind way to communicate with visitors and provide insider information about the home page. As software based communication channels, home pages have been captured and analysed by web hosting services. The result of this early user analytics was one of the most hated tools in the advertiser’s toolkit: the pop-up ad.

In 2014, Ethan Zuckermann published an essay on The Atlantic telling his personal story of inventing what he calls the internet’s original Sin. Zuckermann, who is now at the Center for Civic Media at MIT, was working as a designer and programmer at Tripod from 1994 to 1999. In his essay he explains that the only business model that covered their server cost and made profit out of their online service was their data base analysis. Zuckermann apologetically notes, ‘What I wanted to do was to build a tool that allowed everyone to have the opportunity to express themselves and be heard from anywhere from a few friends to the entire globe.’ Zuckermann made the new business model from the analysis of amateur home pages.

A classical direct payment per use was cross-subsidized through a process of selling consumer’s data to companies to finance the free home page hosting. As a solution to a customer’s demand who didn’t want to be associated with the content of pages on which the ad was embedded, Zuckermann wrote a simple JavaSript function: window.open(‘‘). While the alert method only allowed to write a message in a small standard window, the window.open or pup-up window automatically openes a second window that can carry any digital content. The code spread quickly and within a few weeks Geocities was already using his code too. Speculating it was the (mis)use of this simple and innocent alert messages which amateurs re-appropriated from the computational grammar, then that was the first event of a long series that eventually turned the user as a cyber individual into a transparent digital citizen.

Commercial TV has been operating on this cross-subsidizing business model successfully for a long time. Their audience commodification has always been based on statistical assessments of audience rates and characteristics. Prior to the pop-up there were of course spam mails and later banners for advertising. What the implementation of the pop-up started was the financially successful use of personal user data for commercial implementation. Granular data that both TV and Radio were lacking. The home page analytics captured more than quantitative personal data. Analyzing amateurs allowed to capture precisely that ‘something else’, which Locke pointed out in his manifesto, the voice of the people: their knowledge.

The possibility to track users’ surfing history is part of the way protocols communicate. Appropriating this data for advertising, married the very process of tracking of social knowledge to the new ‘free’ economy in the digital. It not only extended attention economy into the digital domain, it extended marketing into monitoring marketing. Drawing on Hesmogaldah’s concept of alienation and Tiziana Terranova’s work on free labor, Mark Andrejevic shows how monitoring marketing leads to what he calls ‘commercial surveillance’. As Andrejevic further explains, the monitoring includes an element of power and control. By imposing a feedback mechanism which locks the user in a cycle of production and consumption, monitoring marketing can now use personal information to predict user behavior and to influence it systematically.

The roots of user-vulnerability by ‘commercial surveillance’ are in the vernacular web, today surveillance methods operate on grammars which developed from early home page users. Conversely, this re-appropriation of the grammar for advertising has changed user behavior. Ad-blockers, regular cleaning of one’s surfing history, cache and cookies, the routinely use of adblockers and the private mode, or Virtual Private Networks (VPN), the turning off of particular features on the smartphone and the fight for legal clarification of user’s right ‘to be forgotten’ are just a few simple examples of the everyday protest against regular surveillance.

On the technical side, data is vulnerable due to the way computer systems read data. They need to be told what to remember and what to forget – it’s necessary to formalize what needs to be forgotten. Because of this computer logic, tracking is necessary to measure the click through and viewing rate, as well as to prevent fraud. For this reason it doesn’t only capture either knowledge or personal data, it captures it all, and needs to be told what to forget. Precisely what companies do not do. Although, after a specific elasticity of supply has been passed (at the same time there is no equivalent to market satisfaction for material commodities), additional information doesn’t add up to additional financial profit. On the other side of the equation, there are almost no marginal costs for the collection and sharing of more granular information about users. Hence, data is accumulated because the technology is cost neutral. How it will be used and profit extracted in the future, is often unknown at the time of data collection.

In a nutshell, the pop-up is build upon the grammars of the re-appropriated alert method for social interaction, or at least on the analysis of the interaction within a social network, namely Tripod’s home page environment. What follows then is that online economy is based on the digital formalization of preformal social networks, which in turn allows to extract what Matteo Pasquinelli refers to as a ‘sort of network surplus value’. In this alternative (hi)story, the exploitation of the digital domain is more than the quantitative capturing and indexing of information and social network flows. This personal story tells the story of ‘Google as a machinic parasite of the common intellect’ that has the very parasitic exploitation of literary social value of a network inscribed into the very machinic, without which it could not operate to exploit the common intellect in the first place. Paradoxically, dataveillance is both an unavoidable necessity as well as an unavoidable consequence of economic action – because surveillance is the technical fundament upon which such an online economy has been built in the first place.

There is something about quantity that has always attracted the online community. ‘We’ve seen it with our very own eyes!! We’re celebrating 5000 hits!!’ put the home page owners on their header image. Other home pages asked you directly to spread the word of trust: ‘Do you like this site? Tell a friend!’ Indeed, it sounds familiar from social buttons and pop-up messages as well as viral and social media business models. Such as the ‘tell a friend’ button and specific social media buttons for Facebook, Twitter and LinkedIn. And of course, the quantitative indications for likes, tweets and comments. The possibilities of automatic tracking have already been deployed widely in the form of visitor counters on 90’s home pages.

Today metrics shape digital culture. The politics of the URL has become a major concern for the mediation of culture via search engines and their algorithms. Search Engine Optimization (SEO) practice for higher profit extraction through growth hacking, Google Analytics for different kinds of consumer behavior analyses, information bubbles, digital labor, vectoral power, power distribution and the like economy etc. In this development discrete grammars become a cultural phenomenon through their application e.g. for user tracking, and thereby shape society and rise serious questions regarding diverse topics such as law, net politics, economy, user rights and security. It appears, the very technology to count was not the root of the stalker economy, as Al Gore put it. For it to proliferate onto almost every website and into every home, financial pressure and (programmers’) naivety, as in simple-heartedness, was needed first. But this was the story of the Alert!. The web counter’s story points the attention towards a maybe less obvious impact.

Within the Like Economy, it’s the counter’s technical capabilities to track that afford the so called social value of social buttons. Caroline Gerltz and Anne Helmond engage critically with what kind of social is imposed through the techniques of linking, sharing and tweeting. They point out that through a feedback loop of information, social buttons not only create and organize interactivity and sociality, but that social buttons actually turn into the fabric of the web. In their analysis of the relation between the button and the link, the authors point out that the main difference between sharing and linking is found within the strategic use of links of web designers and bloggers as opposed to the qualitative and emotional value which is added through the active click on the button by the user. Hence, they argue, sharing creates user-generated links, which in turn are mediated via commercial platforms such as Facebook. They distinguish clearly between the hit counter which was merely counting the number of visitors without being able to tell anything about the visitor’s attitude or affective reaction to a website, and the Like button, which adds quality to this quantitative metric while at the same time functioning like a hit counter through embedded cookies.

Similarly to Andrejevich’s feedback loop that, as was suggest above, might not merely impose a feedback cycles on users, but operates on grammars which are natural to the vernacular web, maybe social media buttons are not actually adding qualitative value, but rather make the emotional value visible again. To add such qualities, would require that these have not been inscribed into the grammar of the hit counter. However, following the logic of the vernacular web – a web of amateurs who used the hit counter before professionals discovered ways to extract value from digital quantities through analytics, would mean that the hit counter itself already has the emotional, qualitative value inscribed into it. That’s, because users were consciously going to URLs of their interest or because they had heard about them from other users and home pages they trust. Thus, social buttons operate on familiar grammars of hit counters. It’s the use of cookies that makes the social value from the vernacular web’s hit counter visible on modern websites again.

Following this line of thought, we need to ask critically which roles are actors, such as Google and Facebook given in our reflection due to basic assumptions upon which this reflection is done? There is no doubt that social buttons affect the social within the digital greatly. Nevertheless, the suggestion is to understand the relation between Web 1.0 and Web 2.0 more granular, in order to understand the roles of powerful actors in more depth and in order to be able to differentiate between business models that are based on invented new technologies and those that commodify and extend older cultural patterns to extract further profit.

What is at stake, if we do not study the grammar of the vernacular Web? Which new insists can be derived from studying desolated websites? Can the documentary storytelling as a critical portrait of social and economic actions be applied to the study of the web, and if so how can this enrich our understanding and reflection upon contemporary issues? Can a natively computational theory of grammars of action help to study websites in a way, which allows to differentiate between different phases of appropriation and imposition? These were the initial questions and ideas which I explored within and thorough this essay.

Scrutinizing the language of the vernacular web, with my experiment I tried to connected grammars of the contemporary web to those of the amateur web 1.0. This link shows how the former operates upon particular grammars of the latter. By exploring the links between what can be called digital folklore and contemporary practice, it’s possible to inquiry the value of web history itself. Additionally, the picture of digital desolation helped to think home pages in the past, present and future in one picture.

Drawing on the theory of grammars of action, the vernacular language is automatically pulled into sight of modern research. Indeed, the greater implication of my experiment is that the investigation into assumptions upon which research into digital culture is conducted, intends to disturb what research and practice take for granted and thereby unconsciously ignore that which is outside the established scope of knowledge. By reexamining the structure of desolated home pages in the light of contemporary practice and by acknowledging the limitations of existing assumptions, we can maybe start considering other ways of accessing and categorizing websites and digital culture – broadly speaking. With this intervention, I hope to show how future designers as well as ordinary users can construct their pages, use the web and leave their traces for the future. Foregrounding how the use of seemingly irrelevant and naiive elements can become a global socio-political system, highlights the (un-)consciously slow process of grammar infiltration nowadays that can shape the language of the future web.

About the author:

 Tatjana Seitz is interested in the intersection of economic, aesthetic and data driven concepts within the context of networked interfaces. Likewise, she is interested in interdisciplinary approaches to data interpretation and visualization- a field in which she is currently working. She has a MA in Digital Media and Culture from the University of Warwick, UK, where she was working as a GraduateResearcher. During her BA in Media Studies and Economics at the Media University Stuttgart, she managed projects with the Mosfilm Studios in Moscow and Independent Producers in Germany.

Andrejevic, M. (2013) ‘Estranged free labor.’, In: Scholz, T. ed. Digital labor: The Internet as playground and factory. Routledge, pp. 149-164.

Boltanski, L. / Chiapello, E. (2007) The New Spirit of Capitalism, London: Verso.

Müller-Brockmann, J. (1981) Grid systems in Graphic design: A Visual Communication Manual for Graphic Designers, Typographers and Three-Dimensional Designers. Niederteufen: Arthur Niggli.|||

