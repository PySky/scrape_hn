The author of this post works for Undo and uses examples that reference his company’s product. The methods mentioned however are agnostic from any specific tool.

The most successful development teams are those that are agile and can respond quickly to rapid business changes while also maintaining the quality of their hardware or software development. To help realize team agility, Continuous Integration (CI) has quickly become an industry best practice. CI is a practice that requires developers to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early. Whilst this mode of working allows new features to be integrated quickly, it is important to maintain a proper backlog in order to discover potential hiccups, and in a busy development environment, these hiccups can sometimes be neglected.

Currently, a developer is usually informed about a failure via an automated report, which might be little more than a flag that a particular test has failed after a certain commit was made, together with some logging statements that may or may not identify the problem. Backtracking from this terse statement of failure to the source of the problem is often laborious, time- consuming and painful for the developer, and that is just the case for failures that are ​reproducible.

If a test doesn’t fail on the machine that a developer was working on, they will wait for it to be reproduced on a different machine before they try to fix the failure. Irreproducible failures or intermittently failing tests add an extra dimension of complexity to the investigation. They sometimes appear in 1 in every 300 runs of the program, and can be difficult to find. It is easier (and more cost effective) to ignore them and hope that they do not appear again. Consequently, some testing systems can accumulate a backlog of “known-failing” tests, being a dump of (possibly important) test failures that no one has the time, inclination or energy to fix.

Falling victim to irreproducible failures can be costly. Without proper backlogs, it can be extremely difficult to find and fix problems. In fact, a recent study by University of Cambridge Judge Business School showed that debugging is a $312 billion industry problem each year.

There are many different types of software bugs, all of which can impact software performance. For C/C++ programmers, common bugs include execution state corruption, data structure corruption, race conditions, deadlocks and memory leaks. These bugs can appear regularly in software development.

Fundamentally, even in well-developed software, bugs occur because people don’t understand what their software really does. However, there are now a range of tools that allow insight into the murky depths of software execution. Deterministic recording technology allows developers to capture exact replicas of failures—effectively a perfect reproduction of a bug. Using reversible debugging, such recordings can be replayed and rewound to home in on the root cause of the failure.

For example, if your program fails in the cloud, you could download a recording which is an exact copy of the failure to determine what went wrong from the comfort of your laptop.

 In the series of images below, we’ve outlined an example session in a stripped-down version of something that could have appeared in an embedded environment.

For this example, we have used Undo’s recording and reverse debugging products.

 To illustrate the power of record-and-replay technology, let’s assume we were running a test with recording enabled, and on this run, the failure we’re interested in occurs. We would load the recording and run to the end of the program’s execution:

To get a closer look at the error manifestation, set a breakpoint at the place where the error was detected, and let the program run backwards:

Unsurprisingly, we find that ret is wrong. Ret being the factorial of a number <10, it should never be larger than factorial (9) = 362880. Let’s use that as a conditional watchpoint going backwards and search for the point at which the ret becomes okay again:

If the watchpoint fired here, that means going back one more step will show us where the bad thing happened, right?

However, we’re back in main, and there is nothing suspicious. This is the last time the function was called where ret was okay, which is not quite what we wanted. The debugger tries to be clever and only enables the watchpoint where the variable is actually in scope. We need to watch the address itself if we want it to trip on every write access, but first let’s step back into the function by rewinding and replaying the program to explore the code as much as needed:

As you may have noticed, the timeout_fn() is writing to this, causing an overflow, but the timer is being deleted in line 66. In addition, the main page of timer_delete()actually warns that the treatment of any pending signal generated by the deleted timer is unspecified. Even though this example is using threads instead of signals, that seems to be exactly what is going wrong: Later invocations of the timer_fn() will try to change the value of init_param that was passed by addressing during setup(), only setup() has returned since. Therefore, the pointer to it is invalid (C99 standard, section 6.2.4) and was in this particular case pointing to local variables for the function factorial().

In duplicating this debugging session, developers would find that it only takes a matter of minutes to complete. Seasoned engineers that spend time debugging similar issues can attest that other approaches can take weeks to get to the same resolution.

Thanks to the power of record-and-replay technology, developers can reduce debugging time and ship quality C/C++ code on time and on budget. This approach applies equally during day-to-day development as when fixing test failures. Using record-and-replay technology to find the root of the failure can speed up the QA process and allow for a faster development cycle.|||

To help realize team agility, Continuous Integration (CI) has quickly become an industry best practice. CI is a practice that requires developers