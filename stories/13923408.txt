Oops, spoiler alert. This section is a comparison of the two databases in terms of features relevant to data analytics.

CSV is the de facto standard way of moving structured (i.e. tabular) data around. All RDBMSes can dump data into proprietary formats that nothing else can read, which is fine for backups, replication and the like, but no use at all for migrating data from system X to system Y.

A data analytics platform has to be able to look at data from a wide variety of systems and produce outputs that can be read by a wide variety of systems. In practice, this means that it needs to be able to ingest and excrete CSV quickly, reliably, repeatably and painlessly. Let's not understate this: a data analytics platform which cannot handle CSV robustly is a broken, useless liability.

PostgreSQL's CSV support is top notch. The and commands support the spec outlined in RFC4180 (which is the closest thing there is to an official CSV standard) as well as a multitude of common and not-so-common variants and dialects. These commands are fast and robust. When an error occurs, they give helpful error messages. Importantly, they will not silently corrupt, misunderstand or alter data. If PostgreSQL says your import worked, then it worked properly. The slightest whiff of a problem and it abandons the import and throws a helpful error message.

MS SQL Server can neither import nor export CSV. Most people don't believe me when I tell them this. Then, at some point, they see for themselves. Usually they observe something like:

This is especially baffling because CSV parsers are trivially easy to write (I wrote one in C and plumbed it into PHP a year or two ago, because I wasn't happy with its native CSV-handling functions. The whole thing took perhaps 100 lines of code and three hours – two of which were spent getting to grips with SWIG, which was new to me at the time).

If you don't believe me, download this correctly-formatted, standards-compliant UTF-8 CSV file and use MS SQL Server to calculate the average string length (i.e. number of characters) of the last column in this file (it has 50 columns). Go on, try it.

Naturally, determining this is trivially easy in PostgreSQL – in fact, the most time-consuming bit is creating a table with 50 columns to hold the data. Poor understanding of CSV seems to be endemic at Microsoft; that file will break Access and Excel too.

Sad but true: some database programmers I know recently spent a lot of time and effort writing Python code which "sanitises" CSV in order to allow MS SQL Server to import it. They weren't able to avoid changing the actual data in this process, though. This is as crazy as spending a fortune on Photoshop and then having to write some custom code to get it to open a JPEG, only to find that the image has been altered slightly.

Every data analytics platform worth mentioning is Turing complete, which means, give or take, that any one of them can do anything that any other one can do. There is no such thing as "you can do X in software A but you can't do X in software B". You can do anything in anything – all that varies is how hard it is. Good tools make the things you need to do easy; poor tools make them hard. That's what it always boils down to.

PostgreSQL is clearly written by people who actually care about getting stuff done. MS SQL Server feels like it was written by people who never have to actually use MS SQL Server to achieve anything. Here are a few examples to back this up:

And there are plenty more examples out there. Each of these things, in isolation, may seem like a relatively minor niggle; however, the overall effect is that getting real work done in MS SQL Server is significantly harder and more error-prone than in PostgreSQL, and data analysts spend valuable time and energy on workarounds and manual processes instead of focusing on the actual problem.

Update: it was pointed out to me that one really useful feature MS SQL Server has which PostgreSQL lacks is the ability to declare variables in SQL scripts. Like this:

PostgreSQL can't do this. I wish it could, because there are an awful lot of uses for such a feature.

Anyone who follows developments in IT knows that cross-platform is a thing now. Cross-platform support is arguably the killer feature of Java, which is actually a somewhat lumpy, ugly programming language, but nonetheless enormously successful, influential and widespread. Microsoft no longer has the monopoly it once enjoyed on the desktop, thanks to the rise of Linux and Apple. IT infrastructures are increasingly heterogeneous thanks to the flexibility of cloud services and easy access to high-performance virtualisation technology. Cross-platform software is about giving the user control over their infrastructure. (At work I currently manage several PostgreSQL databases, some in Windows and some in Ubuntu Linux. I and my colleagues freely move code and database dumps between them. We use Python and PHP because they also work in both operating systems. It all just works.)

Microsoft's policy is and always has been vendor lock-in. They don't open-source their code; they don't provide cross-platform versions of their software; they even invented a whole ecosystem, .NET, designed to draw a hard line between Microsoft users and non-Microsoft users. This is good for them, because it safeguards their revenue. It is bad for you, the user, because it restricts your choices and creates unnecessary work for you.

Now, this is not a Linux vs. Windows document, although I'm sure I'll end up writing one of those at some point. Suffice it to say that, for real IT work, Linux (and the UNIX-like family: Solaris, BSD etc.) leaves Windows in the dust. UNIX-like operating systems dominate the server market, cloud services, supercomputing (in this field it's a near-monopoly) and technical computing, and with good reason – these systems are designed by techies for techies. As a result they trade user-friendliness for enormous power and flexibility. A proper UNIX-like OS is not just a nice command line – it is an ecosystem of programs, utilities, functionality and support that makes getting real work done efficient and enjoyable. A competent Linux hacker can achieve in a single throwaway line of Bash script a task which would be arduous and time-consuming in Windows.

(Example: the other day I was looking through a friend's film collection and he said he thought the total number of files in the file system was high, considering how many films he had, and he wondered if maybe he had accidentally copied a large folder structure into one of his film folders. I did a recursive count of files-per-folder for him like this:

The whole thing took about a minute to write and a second to run. It confirmed that some of his folders had a problem and told him which ones they were. How would you do this in Windows?)

For data analytics, an RDBMS doesn't exist in a vacuum; it is part of a tool stack. Therefore its environment matters. MS SQL Server is restricted to Windows, and Windows is simply a poor analytics environment.

"Pure" declarative SQL is good at what it was designed for – relational data manipulation and querying. You quickly reach its limits if you try to use it for more involved analytical processes, such as complex interest calculations, time series analysis and general algorithm design. SQL database providers know this, so almost all SQL databases implement some kind of procedural language. This allows a database user to write imperative-style code for more complex or fiddly tasks.

PostgreSQL's procedural language support is exceptional. It's impossible to do justice to it in a short space, but here's a sample of the goods. Any of these procedural languages can be used for writing stored procedures and functions or simply dumped into a block of code to be executed inline.

MS SQL Server's inbuilt procedural language (part of their T-SQL extension to SQL) is clunky, slow and feature-poor. It is also prone to subtle errors and bugs, as Microsoft's own documentation sometimes acknowledges. I have never met a database user who likes the T-SQL procedural language.

What about the fact that you can make assemblies in .NET languages and then use them in MS SQL Server? This doesn't count as procedural language support because you can't submit this code to the database engine directly. Manageability and ergonomics are critically important. Inserting some Python code inline in your database query is easy and convenient; firing up Visual Studio, managing projects and throwing DLL files around (all in GUI-based processes which cannot be properly scripted, version-controlled, automated or reviewed) is awkward, error-prone and non-scalable. In any case, this mechanism is limited to .NET languages.

Regular expressons (regexen or regexes) are as fundamental to analytics work as arithmetic – they are the first choice (and often only choice) for a huge variety of text processing tasks. A data analytics tool without regex support is like a bicycle without a saddle – you can still use it, but it's painful.

PostgreSQL has smashing out-of-the-box support for regex. Some examples:

Get all lines starting with a repeated digit followed by a vowel:

Get the first isolated hex string occurring in a field:

Break a string on whitespace and return each fragment in a separate row:

Case-insensitively find all words in a string with at least 10 letters:

MS SQL Server has , , and so on, which are not comparable to proper regex support (if you doubt this, try implementing the above examples using them). There are third-party regex libraries for MS SQL Server; they're just not as good as PostgreSQL's support, and the need to obtain and install them separately adds admin overhead.

Note also that PostgreSQL's extensive procedural language support also gets you several other regex engines and their various features - e.g. Python's regex library provides the added power of positive and negative lookbehind assertions. This is in keeping with the general theme of PostgreSQL giving you all the tools you need to actually get things done.

This is a feature that, technically, is offered by both PostgreSQL and MS SQL Server. The implementations differ hugely, though.

In PostgreSQL, custom aggregates are convenient and simple to use, resulting in fast problem-solving and maintainable code:

Elegant, eh? A custom aggregate is specified in terms of an internal state and a way to modify that state when we push new values into the aggregate function. In this case we start each customer off with zero balance and no interest accrued, and on each day we accrue interest appropriately and account for payments and withdrawals. We compound the interest on the 1st of every month. Notice that the aggregate accepts an clause (since, unlike , and , this aggregate is order-dependent) and PostgreSQL provides operators for extracting values from JSON objects. So, in 28 lines of code we've created the framework for monthly compounding interest on bank accounts and used it to calculate final balances. If features are to be added to the methodology (e.g. interest rate modifications depending on debit/credit balance, detection of exceptional circumstances), it's all right there in the transition function and is written in an appropriate language for implementing complex logic. (Tragic side-note: I have seen large organisations spend tens of thousands of pounds over weeks of work trying to achieve the same thing using poorer tools.)

MS SQL Server, on the other hand, makes it absurdly difficult.

Incidentally, the examples in the second link are for implementing a simple string concatenation aggregate. Note the huge amount of code and gymnastics required to implement this simple function (which PostgreSQL provides out of the box, incidentally. Probably because it's useful). MS SQL Server also does not allow an order to be specified in the aggregate, which renders this function useless for my kind of work – with MS SQL Server, the order of string concatenation is random, so the results of a query using this function are non-deterministic (they might change from run to run) and the code will not pass a quality review.

The lack of ordering support also breaks code such as the interest calculation example above. As far as I can tell, you just can't do this using an MS SQL Server custom aggregate.

(It is actually possible to make MS SQL Server do a deterministic string concatenation aggregation in pure SQL but you have to abuse the query functionality to do it. Although an interesting academic exercise, this results in slow, unreadable, unmaintainable code and is not a real-world solution).

Long gone are the days when ASCII was universal, "character" and "byte" were fungible terms and "foreign" (from an Anglocentric standpoint) text was an exotic exception. Proper international language support is no longer optional.

The solution to all this is Unicode. There are a lot of misconceptions about Unicode out there. It's not a character set, it's not a code page, it's not a file format and it's nothing whatsoever to do with encryption. An exploration of how Unicode works is fascinating but beyond the scope of this document – I heartily recommend Googling it and working through a few examples.

The key points about Unicode that are relevant to database functionality are:

PostgreSQL supports UTF-8. Its . and types are, by default, UTF-8, meaning they will only accept UTF-8 data and all the transformations applied to them, from string concatenation and searching to regular expressions, are UTF-8-aware. It all just works.

MS SQL Server 2008 does not support UTF-16; it supports UCS-2, a deprecated subset of UTF-16. What this means is that most of the time, it will look like it's working fine, and occasionally, it will silently corrupt your data. Since it interprets text as a string of wide (i.e. 2-byte) characters, it will happily cut a 4-byte UTF-16 character in half. At best, this results in corrupted data. At worst, something else in your toolchain will break badly and you'll have a disaster on your hands. Apologists for MS are quick to point out that this is unlikely because it would require the data to contain something outside Unicode's basic multilingual plane. This is completely missing the point. A database's sole purpose is storing, retreiving and manipulating data. A database which can be broken by putting the wrong data in it is as useless as a router that breaks if you download the wrong file.

MS SQL Server versions since 2012 have supported UTF-16 properly, if you ensure you select a UTF-16-compliant collation for your database. It is baffling that this is (a) optional and (b) implemented as late as 2012. Better late than never, I suppose.

A common misconception is that all databases have the same types – , , and so on. This is not true. PostgreSQL's type system is really useful and intuitive, free of annoyances which introduce bugs or slow work down and, as usual, apparently designed with productivity in mind.

MS SQL Server's type system, by comparison, feels like beta software. It can't touch the feature set of PostgreSQL's type system and it is beset with traps waiting to ensnare the unwary user. Let's take a look:

PostgreSQL can be driven entirely from the command line, and since it works in operating systems with proper command lines (i.e. everything except Windows), this is highly effective and secure. You can SSH to a server and configure PostgreSQL from your mobile phone, if you have to (I have done so more than once). You can automate deployment, performance-tuning, security, admin and analytics tasks with scripts. Scripts are very important because unlike GUI processes, they can be copied, version-controlled, documented, automated, reviewed, batched and diffed. For serious work, text editors and command lines are king.

MS SQL Server is driven through a GUI. I don't know to what extent it can be automated with Powershell; I do know that if you Google for help and advice on getting things done in MS SQL Server, you get a lot of people saying "right-click on your database, then click on Tasks...". GUIs do not work well across low-bandwidth or high-latency connections; text-based shells do. As I write I am preparing to do some sysadmin on a server 3,500 miles away, on a VPN via a shaky WiFi hotspot, and thanking my lucky stars it's an Ubuntu/PostgreSQL box.

PostgreSQL is very, very easy to connect to and use from programming environments, because libpq, its external API, is very well-designed and very well-documented. This means that writing utilities which plug into PostgreSQL is very easy and convenient, which makes the database more versatile and a better fit in an analytics stack. On many occasions I have knocked up a quick program in C or C++ which connects to PostgreSQL, pulls some data out and does some heavy calculations on it, e.g. using multithreading or special CPU instructions - stuff the database itself is not suitable for. I have also written C programs which use setuid to allow normal users to perform certain administrative tasks in PostgreSQL. It is very handy to be able to do this quickly and neatly.

MS SQL Server's external language bindings vary. Sometimes you have to install extra drivers. Sometimes you have to create classes to store the data you are querying, which means knowing at compile time what that data looks like. Most importantly, the documentation is a confusing, tangled mess, which makes getting this done unnecessarily time-consuming and painful.

Data analytics is all about being a jack of all trades. We use a very wide variety of programming languages and tools. (Off the top of my head, the programming/scripting languages I currently work with are PHP, JavaScript, Python, R, C, C++, Go, three dialects of SQL, PL/PGSQL and Bash.) It is hopelessly unrealistic to expect to learn everything you will need to know up front. Getting stuff done frequently depends on reading documentation. A well-documented tool is more useful and allows analysts to be more productive and produce higher-quality work.

PostgreSQL's documentation is excellent. Everything is covered comprehensively but the documents are not merely reference manuals – they are full of examples, hints, useful advice and guidance. If you are an advanced programmer and really want to get stuck in, you can also simply read PostgreSQL's source code, all of which is openly and freely available. The docs also have a sense of humour:

MS SQL Server's documentation is all on MSDN, which is an unfriendly, sprawling mess. Because Microsoft is a large corporation and its clients tend to be conservative and humourless, the documentation is "business appropriate" – i.e. officious, boring and dry. Not only does it lack amusing references to the historical role of Catholicism in the development of date arithmetic, it is impenetrably stuffy and hidden behind layers of unnecessary categorisation and ostentatiously capitalised official terms. Try this: go to the product documentation page for MS SQL Server 2012 and try to get from there to something useful. Or try reading this gem (not cherry-picked, I promise):

Has the word "report" started to lose its meaning yet?

MS SQL Server's logs are spread across several places - error logs, Windows event log, profiler logs, agent logs and setup log. To access these you need varying levels of permissions and you have to use various tools, some of which are GUI-only. Maybe things like Splunk can help to automate the gathering and parsing of these logs. I haven't tried, nor do I know anyone else who has. Google searches on the topic produce surprisingly little information, surprisingly little of which is of any use.

PostgreSQL's logs, by default, are all in one place. By changing a couple of settings in a text file, you can get it to log to CSV (and since we're talking about PostgreSQL, it's proper CSV, not broken CSV). You can easily set the logging level anywhere from "don't bother logging anything" to "full profiling and debugging output". The documentation even contains DDL for a table into which the CSV-format logs can be conveniently imported. You can also log to stderr or the system log or to the Windows event log (provided you're running PostgreSQL in Windows, of course).

The logs themselves are human-readable and machine-readable and contain data likely to be of great value to a sysadmin. Who logged in and out, at what times, and from where? Which queries are being run and by whom? How long are they taking? How many queries are submitted in each batch? Because the data is well-formatted CSV, it is trivially easy to visualise or analyse it in R or PostgreSQL itself or Python's matplotlib or whatever you like. Overlay this with the wealth of information that Linux utilities like top, iotop and iostat provide and you have easy, reliable access to all the server telemetry you could possibly need.

How is PostgreSQL going to win this one? Everyone knows that expensive flagship enterprise products by big commercial vendors have incredible support, whereas free software doesn't have any!

Of course, this is nonsense. Commercial products have support from people who support it because they are paid to. They do the minimum amount necessary to satisfy the terms of the SLA. As I type this, some IT professionals I know are waiting for a major hardware vendor to help them with a performance issue in a £40,000 server. They've been discussing it with the vendor for weeks; they've spent time and effort running extensive tests and benchmarks at the vendor's request; and so far the vendor's reaction has been a mixture of incompetence, fecklessness and apathy. The £40,000 server is sitting there performing very, very slowly, and its users are working 70-hour weeks to try to stay on schedule.

Over the years I have seen many, many problems with expensive commercial software – everything from bugs to performance issues to incompatibility to insufficient documentation. Sometimes these problems cause a late night or a lost weekend for the user; sometimes they cause missed deadlines and angry clients; sometimes it goes as far as legal and reputational risk.

Every single time, the same thing happens: the problem is fixed by the end users, using a combination of blood, sweat, tears, Google and late nights. I have never seen the vendor swoop to the rescue and make everything OK.

So what is the support for PostgreSQL like? On the two occasions I have asked the PostgreSQL mailing list for help, I have received replies from Tom Lane within 24 hours. Take a moment to click on the link and read the wiki - the guy is not just a lead developer of PostgreSQL, he's a well-known computer programmer. Needless to say, his advice is as good as advice gets. On one of the occasions, where I asked a question about the best way to implement cross-function call persistent memory allocation, Lane replied with the features of PostgreSQL I should study and suggested solutions to my problem – and for good measure he threw in a list of very good reasons why my tentative solution (a C static variable) was rubbish. You can't buy that kind of support, but you can get it from a community of enthusiastic open source developers. Oh, did I mention that the total cost of the database software and the helpful advice and recommendations from the acclaimed programmer was £0.00?

Note that by "support" I mean "help getting it to work properly". Some people (usually people who don't actually use the product) think of support contracts more in terms of legal coverage – they're not really interested in whether help is forthcoming or not, but they like that there's someone to shout at and, more importantly, blame. I discuss this too, here.

I've already talked about scriptability, but database dumps are very important, so they get their own bit here. PostgreSQL's dump utility is extremely flexible, command-line driven (making it easily automatable and scriptable) and well-documented (like the rest of PostgreSQL). This makes database migration, replication and backups – three important and scary tasks – controllable, reliable and configurable. Moreover, backups can be in a space-effecient compressed format or in plain SQL, complete with data, making them both human-readable and executable. A backup can be of a single table or of a whole database cluster. The user gets to do exactly as he pleases. With a little work and careful selection of options, it is even possible to make a DDL-only plain SQL PostgreSQL backup executable in a different RDBMS.

Neither PostgreSQL nor MS SQL Server are crash-happy, but MS SQL Server does have a bizarre failure mode which I have witnessed more than once: its transaction logs become enormous and prevent the database from working. In theory the logs can be truncated or deleted but the documentation is full of dire warnings against such action.

PostgreSQL simply sits there working and getting things done. I have never seen a PostgreSQL database crash in normal use.

PostgreSQL is relatively bug-free compared to MS SQL Server. I once found a bug in PostgreSQL 8.4 – it was performing a string distance calculation algorithm wrongly. This was a problem for me because I needed to use the algorithm in some fuzzy deduplication code I was writing for work. I looked up the algorithm on Wikipedia, gained a rough idea of how it works, found the implementation in the PostgreSQL source code, wrote a fix and emailed it to one of the PostgreSQL developers. In the next release of PostgreSQL, version 9.0, the bug was fixed. Meanwhile, I applied my fix to my own installation of PostgreSQL 8.4, re-compiled it and kept working. This will be a familiar story to many of the users of PostgreSQL, and indeed any large piece of open source software. The community benefits from high-quality free software, and individuals with the appropriate skills do what they can to contribute. Everyone wins.

With a closed-source product, you can't fix it yourself – you just raise a bug report, cross your fingers and wait. If MS SQL Server were open source, section 1.1 above would not exist, because I (and probably thousands of other frustrated users) would have damn well written a proper CSV parser and plumbed it in years ago.

Does this matter? Well, yes. Infrastructure flexibility is more important than ever and that trend will only continue. Gone are the days of the big fat server install which sits untouched for years on end. These days it's all about fast, reliable, flexible provisioning and keeping up with cutting-edge features. Also, as the saying goes, time is money.

I have installed MS SQL Server several times. I have installed PostgreSQL more times than I can remember - probably at least 50 times.

Installing MS SQL Server is very slow. It involves immense downloads (who still uses physical install media?) and lengthy, important-sounding processes with stately progress bars. It might fail if you don't have the right version of .NET or the right Windows service pack installed. It's the kind of thing your sysadmin needs to find a solid block of time for.

Installing PostgreSQL the canonical way – from a Linux repo – is as easy as typing a single command, like this:

How long does it take? I just tested this by spinning up a cheap VM in the cloud and installing PostgreSQL using the above command. It took 16 seconds. That's the total time for the download and the install.

As for updates, any software backed by a Linux repo is trivially easily patched and updated by pulling updates from the repo. Because repos are clever and PostgreSQL is not obscenely bloated, downloads are small and fast and application of updates is efficient.

I don't know how easy MS SQL Server is to update. I do know that a lot of production MS SQL Server boxes in certain organisations are still on version 2008 R2 though...

As if the enormous feature set of PostgreSQL is not enough, it comes with a set of extensions called contrib modules. There are libraries of functions, types and utilities for doing certain useful things which don't quite fall into the core feature set of the server. There are libraries for fuzzy string matching, fast integer array handling, external database connectivity, cryptography, UUID generation, tree data types and loads, loads more. A few of the modules don't even do anything except provide templates to allow developers and advanced users to develop their own extensions and custom functionality.

Of course, these extensions are trivially easy to install. For example, to install the extension you do this:

PostgreSQL is free as in freedom and free as in beer. Both types of free are extremely important.

The first kind, free as in freedom, means PostgreSQL is open-source and very permissively licensed. In practical terms, this means that you can do whatever you want with it, including distributing software which includes it or is based on it. You can modify it in whatever way you see fit, and then you can distribute the modifications to whomever you like. You can install it as many times as you like, on whatever you like, and then use it for any purpose you like.

The second kind, free as in beer, is important for two main reasons. The first is that if, like me, you work for a large organisation, spending that organisation's money involves red tape. Red tape means delays and delays sap everyone's energy and enthusiasm and suppress innovation. The second reason is that because PostgreSQL is free, many developers, experimenters, hackers, students, innovators, scientists and so on (the brainy-but-poor crowd, essentially) use it, and it develops a wonderful community. This results in great support (as I mentioned above) and contributions from the intellectual elite. It results in a better product, more innovation, more solutions to problems and more time and energy spent on the things that really matter.|||

