Computer software now governs virtually every aspect of our lives, from cars to kitchen appliances; the human element has been removed from most machinery we use daily. But computer software can deceive us, and this was the disturbing message from the recent Volkswagen scandal, where the German carmaker fitted millions of cars with software that could outsmart emission-control testing. This demonstrates the need for a code of ethics, a Hippocratic oath, for our computer engineers and the software they create.

As part of an international campaign to market "clean" diesel vehicles, Volkswagen sold 11 million cars worldwide, an estimated 77,000 in Australia and nearly a half-million in the United States, that were supposedly clean. They were not. The cars were equipped with "defeat devices", software that monitored variables including speed, engine features and steering-wheel position, thus detecting when the car was undergoing emission testing.

During testing, the car immediately switched into a diminished power/performance mode that cut engine emissions sufficiently to pass the test. When the testing was completed, the car reverted to its normal driving mode, where pollutants far exceeded permissible levels. Once the deceptive software was developed and installed in cars, the scheme could be carried out without human involvement. By the same measure, it was virtually impossible for anyone unaware to uncover the deception (it was discovered serendipitously by professional engineers who were initially surprised by their findings).

The fallout from the scandal is going to cost Volkswagen hundreds of millions of dollars, but the health costs to the public are incalculable. The nitrogen oxide pollutants released from the tainted vehicles aggravate the symptoms of people with asthma, bronchitis, emphysema or heart disease. Some people will certainly die prematurely, even if the victims can never be directly linked to Volkswagen's subterfuge.

This scandal should begin a vigorous debate about the values that software developers incorporate in their software, not just in the automotive industry but in every industry. At present, the public has little appreciation of the importance of software ethics. (What exactly does the familiar Google motto, "Don't be evil", really mean?) Moreover, like law and medicine, the debate about software ethics appears to be a topic of discussion more in universities than in the real world.

This must change, and that's why what happened at Volkswagen is so important. Issues of safety, privacy and confidentiality have to become essential professional concerns. The Volkswagen case should become a case study in every company that depends on software, as well as in the academic community.

Ethics for software engineers – what the public expects from the profession, and what the profession expects from itself – will not replace the law but can complement it. Software engineers are less likely to cut corners if they know their behaviour will earn the opprobrium of colleagues and engender public suspicion. Ignorance of the importance of software ethics is surely one more step in the descent into a more mistrustful society.

In a sense, the person who first brought the issue of software ethics to public attention was the brilliant scientist and science fiction author Isaac Asimov. In a 1942 magazine article, he developed the Three Laws of Robotics, a primitive code of behaviour for robots, one applicable to today's software as well:

First Law: A robot may not injure a human being, or through inaction, allow a human being to come to harm.

Second Law: A robot must obey the orders given by human beings, except where such orders would conflict with the First Law.

Third Law: A robot must protect its own existence, as long as such protection does not conflict with the First or Second Law. (Asimov subsequently developed a Fourth Law that superseded the first three laws: A robot may not harm humanity, or, by inaction, allow humanity to come to harm. Volkswagen egregiously violated this law.)

When it came to malevolence, Asimov was hardly naive. He understood that the world was entering a new era of man and machine, one which would reveal uncharted territory of human behaviour (so brazenly demonstrated by Volkswagen).

Asimov once said of his laws, "Whenever someone asks me if I think my Three Laws of Robotics will actually be used to govern the behaviour of robots, once they become versatile and flexible enough to be able to choose among different courses of behaviour, my answer is, 'Yes, the Three Laws are the only way in which rational human beings can deal with robots – or with anything else'. But when I say that, I always remember (sadly) that human beings are not always rational."

Dr Cory Franklin is the author of Cook County ICU: 30 years of Unforgettable Patients and Odd Cases.|||

The Volkswagen scandal points to a need for a code of ethics for the creators of software applications.