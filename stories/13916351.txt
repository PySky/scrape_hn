An exciting new study from the University of Sheffield and published in the journal Swarm Intelligence has demonstrated (free pre-print version) a method of allowing computers to make sense of complex patterns all on their own, an ability that could open the door to some of the most advanced and speculative applications of artificial intelligence. Using an all-new technique called Turing Learning, the team managed to get an artificial intelligence to watch movements within a swarm of simple robots and figure out the rules that govern their behavior. It was not told to look for any particular signifier of swarm behavior, but simply to try to emulate the source more and more accurately and to learn from the results of that process. It’s a simple system that the researchers think could be applied everywhere from human and animal behavior to biochemical analysis to personal security.

First, the history. Alan Turing was a multi-talented British mathematician who helped to both win the Second World War and invent the earliest computers, both while leading the Allied code-breaking efforts at Blechley Park. However, this impact on history may have been even greater through his academic work; his seminal paper On Computable Numbers laid down the foundations for modern computer theory, and his thinking on artificial intelligence is still some of the most influential today. He devised the famous Turing Test for true AI: if an AI can endure a detailed, text-based interrogation by a human tester or testers, and those testers cannot accurately determine whether they are speaking to a human or a robot, then true artificial intelligence has been achieved. With all we now know about the ability of neural networks to find patterns in behavior, this does seem like a somewhat low bar to consciousness — but it’s easy to remember, historically important, and it has alliteration, which means it’s famous.

This new learning process is called Turing Learning because it basically puts a very simple version of this pass-fail differentiation test into practice, over and over again. It can be applied in many contexts, but for their study the team used robot swarms. In all contexts though, you have an original, a copy, and a comparison algorithm.

In this study one swarm of robots, the “agent” swarm, moves according to simple but unknown rules, while a second “model” swarm starts out with largely meaningless, random behaviors. (As an aside, yes, the “model” swarm should really be the one that is used as the model, but whatever.) These two swarms are then compared by a “classifier” algorithm but, crucially, this classifier is not told which rubrics it is supposed to be comparing. It simply looks at a swarm, notices all the attributes it can, and tries to determine whether it is looking at the agent or model swarm — does this swarm conform to the patterns associated with the agent swarm, yes or no?

At first this will of course be a total guess, but when the classifier algorithm does correctly identify the swarm, it is given a metaphorical “reward” that slightly increases the probability that aspects of the path it took to that answer will be repeated in the future. In principle, even starting from totally random modes of comparison between the two swarms, the classifier should be able to quickly deemphasize irrelevant aspects of the agent swarm while focusing in on those that actually impact the accuracy of its guesses. For its part, the model swarm adjusts its own movement after each guess, receiving its own probabilistic reward for “tricking” the classifier into incorrectly identifying it as the agent swarm.

What this means is that of the three aspects of this learning system, only the agent swarm remains static, because that’s the thing we’re trying to study. The other two elements, the model swarm and the classifier, evolve in a complementary fashion to one another. The accuracy of one directly offsets the accuracy of the other and drives a need for both to keep getting more accurate over time. In the University of Sheffield study, this evolutionary approach, in which the model provides both the machine learning predator and the prey, produced more accurate guesses at the agent swarm’s programming than traditional pattern-finding algorithms.

In the above Turing Learning test, the classifier is eventually seeing through to the simple rules that govern the movement of the agent swarm, even though the actual behavior of the swarm is much more complex than that due to interactions between robots and with the environment. To continue to distinguish between the two increasingly similar swarms, the algorithm is forced to infer the deep, underlying laws that give rise to the more nuanced distinctions. This insight then drives the model swarm to correct such errors, inexorably nudging its programming to be just a little more similar to the unknown programming of the agent swarm.

So, what’s the utility of this? Well, much the same as existing neural networks, but with less need for human direction and thus less possibility of human bias. More traditional neural network models are already capable of providing real insight into long-standing problems by applying the cold, inhuman mind of a computer. Computers aren’t biased toward any particular outcome (unless we tell them to be), which for instance allows them to find a much wider and more powerfully predictive suite of visual characteristics for lung cancer in tissue micrographs despite such identification having been studied and refined for decades by medical doctors.

That sort of ability can be applied widely. What if we wanted to learn about the defining aspects of the work of a great painter? We might ask historians of this artist, but that would produce largely canonical explanations and perhaps overlook the same things that have been overlooked since the very beginning. But a learning model could find aspects nobody — including the artist themselves — had ever considered. It could find the small but important stimuli that cause schools of fish to move this way rather than that. It could slowly refine AI pathfinding and general behavior in video-games to create more lifelike allies and opponents.

Perhaps most intriguingly, though, Turing Learning could assist in analyzing human behavior. Give a model like this a never-ending feed of human movements through a subway station and a simulated station full of simple moving actors, and those actors might very soon move according to rules that provide real insight into human psychology. By the same token, a dystopian surveillance agency might one-day run a simulation in which a human model behaves in certain simulated ways, those behaviors simultaneously evolving ever-closer to your own and to the model of a closeted dissident. The idea of some all-seeing AI that can sniff out malcontents is a lot easier to imagine when that AI doesn’t have to have been specifically programed to know what every potentially shady behavior looks like, but can figure that out as it goes.

These are the sorts of abilities in machine learning that forecast the most incredible and worrying predictions of science fiction. Neural networks have had the ability to watch us and find patterns for many years now, but this breakthrough shows just how quickly those abilities are moving forward.|||

Soon, the machines really might be watching your every move -- and, for the first time, independently making sense of ...