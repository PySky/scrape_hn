I’ve been a part of SQream DB for over three years, now entering the fourth year.

 I started out in the core of the product – the SQL parser and compiler written in Haskell.

My experience with the product started out in pure amazement at what it can do (I came from mySQL), and has evolved to a point where using any other database for big data just doesn’t satisfy me anymore.

 (Honorable mention goes to memSQL for being really fun for quick and dirty in-memory stuff)

I’ve grown to appreciate SQream DB’s strengths and weaknesses, and that’s what I want to write about today.

There are millions of projects all around the world that require relatively straight-forward analytics, but bringing them from gigabytes or terabytes into hundreds of terabytes is unfortunately not as easy.

 With the typical clustered solutions that grew in popularity around the beginning of this decade, you would ‘simply add more nodes’. Unfortunately, this creates all sorts of limitations, which data scientists and BI professionals don’t appreciate.

“You can’t join these three tables, because we didn’t shard the tables for this kind of query” is all too common.

In order to keep growing, you have to rethink the solution – and GPUs are the perfect match.

 By bringing thousands of cores per TESLA card, and with multiple cards per 1U, 2U or 4U host (be it x86-64 or Power8 with NVLINK), you can effectively avoid the complexities of the (now) classic distributed solution.

Like with any other database, the answer is maybe.

 I feel there is no one-size-fits-all in databases, and you should pick the right tool for the job.|||

