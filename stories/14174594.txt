t-SNE is great at capturing a combination of the local and global structure of a dataset in 2d or 3d. But when plotting points in 2d, there are often interesting patterns in the data that only come out as "texture" in the point cloud. When the plot is colored appropriately, these patterns can be made more clear.

First we load some data: a 128 dimensional embedding output from a VAE, and a 2 dimensional representation of those vectors based on running t-SNE.

When we visualize the raw data itself the "texture" is clear, but not as clear as the different "islands" and clusters.

One way of pulling out a real feature from the data is to look at the nearest neighbors in 2d and see how far away they are on average in the original high dimensional space. This suggestion comes from Martin Wattenberg. First we compute the indices for all the nearest neighbors:

And then we compute the distances in high dimensional space, and normalize them between 0 and 1.

In this case the distances look sort of gaussian with a long tail. We clip the ends to draw out the details in the colors.

One technique is to compute t-SNE in 3D and use the results as colors. This can take a long time to compute with large datasets. This is the technique that was used for the Infinite Drum Machine.

Another approach is to use PCA, which is must faster but does not show as much structure in the data.

Instead of using PCA to 3D, we can also do PCA to 24 dimensions, comparing the dimensions to the median of each, and using those comparisons as bits in a 24-bit color. This suggestion comes from Mario Klingemann This technique can work well with only 12 dimensions (4 bits per color). It doesn't make sense in a "continuous" space (normalizing and multiplying the shuffled bits by the basis directly, rather than testing against the median first). That just makes the colors all muddled, more similar to the 3D PCA.

Another approach is to use ICA, which can be a little slower than PCA, but shows different features depending on the data.

We can also do ICA to 24 dimensions and pack it into colors. This might make less sense than PCA theoretically.

Another approach that shows up in the LargeVis Paper is to compute K-Means on the high dimensional data and then use those labels as color indices. We can try with 8, 30, and 128 cluster K-Means.

Some of the "boundaries" between colors seem fairly arbitrary, but K-Means has a nice property of allowing us to identify the centers of these color regions if we want to provide exemplars.

sklearn.neighbors can be slow, but the mrpt library is much faster.

Another technique, arguably the most evocative, is to use the argmax of each high dimensional vector. The motivation for using argmax is that high dimensional data is so sparse that "nearby" points should have a similar ordering of their dimensions: if you sorted the dimensions of two nearby points, the difference should be small. This means that their argmax (the largest dimensions) should probably be shared. If we do this without any modification to the high dimensional data, we get a fairly homogenous plot:

This is because a few dimensions dominate the argmax.

If we standardize each dimension then there is a more even distribution of possible argmax values, and therefore more even distribution of colors.

In this case the high dimensional data has both negative and positive components, so it might make more sense to take the absolute value before computing the argmax. In this case, it makes things visually "messier" with too many overlapping colors.

Because some of the dimensions are correlated with each other in this case, it might make sense to do PCA before taking the argmax. Again if we take the argmax without standardizing the high dimensional data, a few colors dominate.

Here we can see the distribution of argmax results are concentrated toward the first dimensions of the PCA projection.

Once we standardize it we see a more even distribution.

And now we can take the standardized argmax, and the argmax of the absolute values of the standardized data.

I prefer the non-absolute value argmax in this case. Now we can run the whole process again for different number of output components from PCA. Here it is for 16 and 128 dimensions.

It should be possible to "tune" the amount of color variation by an element-wise multiplication between each PCA projected vector and a vector with some "falloff" that gives more weight to the earlier dimensions and less weight to the final dimensions.

We can try the same technique, but using ICA instead of PCA. Here for 8, 30, and 128 dimensions.

Finally, we can try computing K-Means on top of the dimensionality reduced vectors.|||

Coloring-t-SNE - Exploration of methods for coloring t-SNE.