So as you already know, I’ve enrolled to Functional Programming in Scala Specialization on Coursera. Currently I’m passing through the 4th course of the series “Big Data Analysis with Scala and Spark”. If to be more precise, two days ago I completed an assignment for weeks #2 and #3. As usually I’m going to share my experience.



By the way, if you are interested in my impressions of week #1 of Spark course, you can freely read it latter.

Week #2 is dedicated to pair RDDs. It’s totally about this separate world of data. Because comparing to regular RDDs you get a set of powerful functions which you can apply to exactly to pair RDDs.

Wait a moment! Do you understand what is a pair RDD? It’s something like a map data structure (dictionary, properties etc). So in general when you have a data in a format of key-value pairs, you have a pair RDD in context of Spark.

So why pair RDDs are so special in Spark? Actually, because they give a more easier way to operate with data. For example when you want to group or aggregate a data based on some of its properties. For this purposes, Spark has a special set of functions such as: , , etc. Separately I want to underline join operations. Pair RDDs allows you to apply SQL-like joins.

Small example. You have an RDD of persons. You want to group these persons by their age. In order to do this with Spark, you need (A) to create a pair RDD, where a role of key will play an age field, a role of value will play an appropriate person, (B) apply function to the pair RDD.

If you are attentive and curious student, you would definitely learn all this from video lectures. All topics are explained pretty well.

So in the end of week #2 I met an assignment about Stackoverflow dataset. From the first seconds, when I started reading it, I was terrified. The assignment is about k-means algorithm.

I thought: Again! Like in previous courses about Scala! K-means hell!

I was needed to implement some of functions which were required for a correct work of k-means algorithm.

The second thing which confused me, was absence of unit tests. So I was not able to check the code I wrote.

So I started from writing some unit tests. Step by step I implemented functions…

These tests are good just for general verifications. Because in context of the algorithm efficiency, they are not so useful.

One interesting fact, I submitted 29 solutions before I achieved 10 out of 10. Intermediate results were 0, 3 and 4. The main problem was a poor performance.

I read a lot of threads on discussions forum. It was really helpful. There were a lot of tips from other students and moderators. And there I understood that the assignment requires knowledge from the week #3 video lectures.

Without knowledge about Spark shuffling internals and partitioning strategies there were no chances for me to complete the task. So I dived into the video lectures from week #3 and finally completed the k-means algorithm.

So my main advise is be attentive!

In this section, I want to share some tips which can be useful in context of solving the assignments. Since many people tried to steal the real solutions, I decided to modify them a little.

Let’s assume that we are working with RDD of entities from Quora service (question-answer platform). All questions and answers are represented as :

In order to groupe all questions with related to them answers we need to perform something like this:

When we want to find the most higher rated answer for each question we do something like this:

Vectors creation form the questions and their higher rated answers.

Within 2 days of my attempts to complete the assignment, I learned a lot of interesting things about Spark. Definitely more that was covered in the video lectures. Probably the most useful knowledge is about Spark UI. So when you are starting Spark code execution in the IDE, you are able to see in the logs URL to UI. And there placed a lot of interesting and useful data about your jobs, their status etc.|||

Big Data Analysis with Scala and Spark week 2 & 3. Impressions from passing the first week of the Spark course on Coursera. Assignments analysis.