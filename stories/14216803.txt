Recently, research about artistic style transfer, which trains computers to be artists, become popular. Gatys et al. turned this task into an optimization problem and utilized convolution neural network to solve this problem. However, this method for image stylization doesn't work well for videos due to its failure to consider temporal consistency. To solve this problem, Ruder et al. proposed a method which integrated temporal loss into the loss function. But this method is pretty slow. Stylizing a 15-second-video takes more than 7.5 hours. Earlier this year, Johnson et al. made the image stylization procedure real-time by training a neural network for this optimization problem instead of optimizing each image separately. By combining the ideas of Ruder et al. and Johnson et al., we came up with a new method for video stylization, which keeps the temporal consistency but works about 10 times as fast as the method proposed by Ruder et al. Our method makes it possible to stylize movies and animations with reasonable time costs.

With the help of Waifu2x super-resolution tool, we are able to make 1080p and 4K HD stylized videos without too much computational cost for stylization.

Styles (from left to right): Composition VII, The Great Wave off Kanagawa, The Starry Night

The following videos compare our method with other methods

All time cost above does not contain time cost for calculating optical flow. This procedure can be paralleled with video stylization and calculating optical flow itself can be paralleled. In addition, we don't need to calculate optical flow again if we just want to change style for a video. We use CPU cluster to calculate optical flow. For Doraemon video, optical flow calculation cost ~50 hours.|||

