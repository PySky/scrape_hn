For some time now I’ve been interested in better ways to construct LR(1) parsers. LALRPOP currently allows users to choose between the full LR(1) algorithm or the LALR(1) subset. Neither of these choices is very satisfying:

The Lane Table algorithm published by Pager and Chen at APPLC ‘12 offers an interesting alternative. It is an alternative to earlier work by Pager, the “lane tracing” algorithm and practical general method. In any case, the goal is to generate an LALR(1) state machine when possible and gracefully scale up to the full LR(1) state machine as needed.

I found the approach appealing, as it seemed fairly simple, and also seemed to match what I would try to do intuitively. I’ve been experimenting with the Lane Table algorithm in LALRPOP and I now have a simple prototype that seems to work. Implementing it required that I cover various cases that the paper left implicit, and the aim of this blog post is to describe what I’ve done so far. I do not claim that this description is what the authors originally intended; for all I know, it has some bugs, and I certainly think it can be made more efficient.

My explanation is intended to be widely readable, though I do assume some familiarity with the basic workings of an LR-parser (i.e., that we shift states onto a stack, execute reductions, etc). But I’ll review the bits of table construction that you need.

To explain the algorithm, I’m going to walk through two example grammars. The first I call G0 – it is a reduced version of what the paper calls G1. It is interesting because it does not require splitting any states, and so we wind up with the same number of states as in LR(0). Put another way, it is an LALR(1) grammar.

I will be assuming a basic familiarity with the LR(0) and LR(1) state construction.

The key point here is that if you have , you could build an or a from that (in fact, there can be any number of tokens). You ultimately decide based on whether the tokens are followed by a (in which case you build an ) or a (in which case you build a ).

LR(0), since it has no lookahead, can’t handle this case. LALR(1) can, since it augments LR(0) with a token of lookahead; using that, after we see the , we can peek at the next thing and figure out what to do.

We begin by constructing an LR(0) state machine. If you’re not familiar with the process, I’ll briefly outline it here, though you may want to read up separately. Basically, we will enumerate a number of different states indicating what kind of content we have seen so far. The first state indicates that we are at the very beginning out “goal item” :

The indicates that we have started parsing a ; the is how far we have gotten (namely, nowhere). There are two items because there are two ways to make a . Now, in these two items, immediately to the right of the we see the symbols that we expect to see next in the input: in this case, an or a . Since and are nonterminals – i.e., symbols defined in the grammar rather than tokens in the input – this means we might also be looking at the beginning of an or a , so we have to extend to account for that possibility (these are sometimes called “epsilon moves”, since these new possibilities arise from consuming no input, which is denoted as “epsilon”):

This completes the state . Looking at these various possibilities, we see that a number of things might come next in the input: a , an , or a . (The nonterminals and can “come next” once we have seen their entire contents.) Therefore we construct three successors states: one ( ) accounts for what happens when see an . The other two ( and below) account for what happens after we recognize an or a , respectively.

S1 (what happens if we see an ) is derived by advancing the past the :

Here we dropped the possibilities, since those would have required consuming a or . But we have kept the etc. Again we find that there are nonterminals that can come next ( and again) and hence we have to expand the state to account for the possibility that, in addition to being partway through the and , we are at the beginning of another or :

Here again we expect either a , an , or a . If we again check what happens when we consume an , we will find that we reach again (i.e., moving past an gets us to the same set of possibilities that we already saw). The remaining states all arise from consuming an or a from or :

We can represent the set of states as a graph, with edges representing the transitions between states. The edges are labeled with the symbol ( , , etc) that gets consumed to move between states:

Reducing and inconsistent states. Let’s take another look at this state S1:

There are two interesting things about this state. The first is that it contains some items where the comes at the very end, like . What this means is that we have seen an in the input, which is enough to construct an . If we chose to do so, that is called reducing. The effect would be to build up an , which would then be supplied as an input to a prior state (e.g., ).

However, the other interesting thing is that our state actually has three possible things it could do: it could reduce to construct an , but it could also reduce to construct a ; finally, it can shift an . Shifting means that we do not execute any reductions, and instead we take the next input and move to the next state (which in this case would be S1 again).

A state that can do both shifts and reduces, or more than one reduce, is called an inconsistent state. Basically it means that there is an ambiguity, and the parser won’t be able to figure out what to do – or, at least, it can’t figure out what to do unless we take some amount of lookahead into account. This is where LR(1) and LALR(1) come into play.

In an LALR(1) grammar, we keep the same set of states, but we augment the reductions with a bit of lookahead. In this example, as we will see, that suffices – for example, if you look at the grammar, you will see that we only need to do the reduction if the next thing in the input is a . And similarly we only need to do the reduction if the next thing is a . So we can transform the state to add some conditions, and then it is clear what to do:

Note that the shift vs reduce part is implied by where the is: we always shift unless the is at the end. So usually we just write the lookahead part. Moreover, the “lookahead” for a shift is pretty obvious: it’s whatever to the right of the , so we’ll leave that out. That leaves us with this, where (for example) means “only do this reduction if the lookahead is ”:

We’ll call this augmented state a LR(0-1) state (it’s not quite how a LR(1) state is typically defined). Now that we’ve added the lookahead, this state is no longer inconsistent, as the parser always knows what to do.

The next few sections will show how we can derive this lookahead automatically.

The first step in the process is to naively convert all of our LR(0) states into LR(0-1) states (with no additional lookahead). We will denote the “no extra lookahead” case by writing a special “wildcard” lookahead . We will thus denote the inconsistent state after transformation as follows, where each reduction has the “wildcard” lookahead:

Naturally, the state is still inconsistent.

In the next step, we iterate over all of our LR(0-1) states. In this example, we will not need to create new states, but in future examples we will. The iteration thus consists of a queue and some code like this:

To resolve an inconsistent state, we first construct a lane table. This is done by the code in the module (the module maintains the data structure). It works by structing at each conflict and tracing backwards. Let’s start with the final table we will get for the state S1 and then we will work our way back to how it is constructed. First, let’s identify the conflicting actions from S1 and give them indices:

Several of the items can cause “Confliction Action 0” (C0), which is to shift an . These are all mutually compatible. However, there are also two incompatible actions: C1 and C2, both reductions. In fact, we’ll find that we look back at state S0, these ‘conflicting’ actions all occur with distinct lookahead. The purpose of the lane table is to summarize that information. The lane table we will up constructing for these conflicting actions is as follows:

Here the idea is that the lane table summarizes the lookahead information contributed by each state. Note that for the shift the state S1 already has enough lookahead information: we only shift when we see the terminal we need next (“e”). But state C1 and C2, the lookahead actually came from S0, which is a predecessor state.

As I said earlier, the algorithm for constructing the table works by looking at the conflicting item and walking backwards. So let’s illustrate with conflict C1. We have the conflicting item , and we are basically looking to find its lookahead. We know that somewhere in the distant past of our state machine there must be an item like

that led us here. We want to find that item, so we can derive the lookahead from (whatever symbols come after ).

To do this, we will walk the graph. Our state at any point in time will be the pair of a state and an item in that state. To start out, then, we have , which is the conflict C1. Because the is not at the “front” of this item, we have to figure out where this came from on our stack, so we look for predecessors of the state S1 which have an item like . This leads us to S0 and also S1. So we can push two states in our search: and . Let’s consider each in turn.

The next state is then . Here the lies at the front of the item, so we search the same state S0 for items that would have led to this state via an epsilon move. This basically means an item like – i.e., where the appears directly before the nonterminal . In our case, we will find . This is great, because it tells us some lookahead (“c”, in particular), and hence we can stop our search. We add to the table the entry that the state S0 contributes lookahead “c” to the conflict C1. In some cases, we might find something like instead, where the we are looking for appears at the end. In that case, we have to restart our search, but looking for the lookahead for .

The next state in our case is . Again the lies at the beginning and hence we search for things in the state S1 where is the next symbol. We find . This is not as good as last time, because there are no symbols appearing after X in this item, so it does not contribute any lookahead. We therefore can’t stop our search yet, but we push the state – this corresponds to the state I mentioned at the end of the last paragraph, except that in this case is the same nonterminal we started with.

Looking at , we again have the in the middle of the item, so we move it left, searching for predecessors with the item . We will (again) find S0 and S1 have such items. In the case of S0, we will (again) find the context “c”, which we dutifully add to the table (this has no effect, since it is already present). In the case of S1, we will (again) wind up at the state . Since we’ve already visited this state, we stop our search, it will not lead to new context.

At this point, our table column for C1 is complete. We can repeat the process for C2, which plays out in an analogous way.

Looking at the lane table we built, we can union the context sets in any particular column. We see that the context sets for each conflicting action are pairwise disjoint. Therefore, we can simply update each reduce action in our state with those lookaheads in mind, and hence render it consistent:

This is of course also what the LALR(1) state would look like (though it would include context for the other items, though that doesn’t play into the final machine execution).

At this point we’ve covered enough to handle the grammar G0. Let’s turn to a more complex grammar, grammar G1, and then we’ll come back to cover the remaining steps.

G1 is a (typo corrected) version of the grammar from the paper. This grammar is not LALR(1) and hence it is more interesting, because it requires splitting states.

The key point of this grammar is that when we see and we wish to know whether to reduce to or , we don’t have enough information. We need to know what is in the , because means we reduce to and means we reduce to . In terms of our state machine, this corresponds to splitting the states responsible for X and Y based on earlier context.

Let’s look at a subset of the LR(0) states for G1:

Here we can see the problem. The state S3 is inconsistent. But it is reachable from both S1 and S2. If we come from S1, then we can have (e.g.) , but if we come from S2, we expect .

Let’s walk through our algorithm again. I’ll start with step 3a.

The lane table for state S3 will look like this:

Now if we union each column, we see that both C1 and C2 wind up with lookahead . This is our problem. We have to isolate things better. Therefore, step 3b (“update lookahead”) does not apply. Instead we attempt step 3c.

This part of the algorithm is only loosely described in the paper, but I think it works as follows. We will employ a union-find data structure. With each set, we will record a “context set”, which records for each conflict the set of lookahead tokens (e.g., ).

A context set tells us how to map the lookahead to an action; therefire, to be self-consistent, the lookaheads for each conflict must be mutually disjoint. In other words, is valid, and says to do C1 if we see a “d” and C2 if we see a “c”. But is not, because there are two actions.

Initially, each state in the lane table is mapped to itself, and the conflict set is derived from its column in the lane table:

We designate “beachhead” states as those states in the table that are not reachable from another state in the table (i.e., using the successors). In this case, those are the states S1 and S2. We will be doing a DFS through the table and we want to use those as the starting points.

So we begin by iterating over the beachhead states.

When we visit a state X, we will examine each of its successors Y. We consider whether the context set for Y can be merged with the context set for X. So, in our case, X will be S1 to start and Y will be S3. In this case, the context set can be merged, and hence we union S1, S3 and wind up with the following union-find state:

Next we examine the edge S3 -> S3. Here the contexts are already merged and everything is happy, so we stop. (We already visited S3, after all.)

This finishes our first beachhead, so we proceed to the next edge, S2 -> S3. Here we find that we cannot union the context: it would produce an inconsistent state. So what we do is we clone S3 to make a new state, S3’, with the initial setup corresponding to the row for S3 from the lane table:

This also involves updating our LR(0-1) state set to have a new state S3’. All edges from S2 that led to S3 now lead to S3’; the outgoing edges from S3’ remain unchanged. (At least to start.)

Therefore, the edge is now . We can now merge the conflicts:

Now we examine the outgoing edge S3’ -> S3. We cannot merge these conflicts, so we search (greedily, I guess) for a clone of S3 where we can merge the conflicts. We find one in S3’, and hence we redirect the S3 edge to S3’ and we are done. (I think the actual search we want is to make first look for a clone of S3 that is using literally the same context as us (i.e., same root node), as in this case. If that is not found, then we search for one with a mergable context. If that fails, then we clone a new state.)

The final state thus has two copies of S3, one for the path from S1, and one for the path from S2, which gives us enough context to proceed.

As I wrote, I’ve been experimenting with the Lane Table algorithm in LALRPOP and I now have a simple prototype that seems to work. It is not by any means exhaustively tested – in fact, I’d call it minimally tested – but hopefully I’ll find some time to play around with it some more and take it through its paces. It at least handles the examples in the paper.

The implementation is also inefficient in various ways. Some of them are minor – it clones more than it needs to, for example – and easily corrected. But I also suspect that one can do a lot more caching and sharing of results. Right now, for example, I construct the lane table for each inconsistent state completely from scratch, but perhaps there are ways to preserve and share results (it seems naively as if this should be possible). On the other hand, constructing the lane table can probably be made pretty fast: it doesn’t have to traverse that much of the grammar. I’ll have to try it on some bigger examples and see how it scales.|||

For some time now I’ve been interested in better ways to construct LR(1) parsers. LALRPOP currently allows users to choose between the full LR(1) algorithm o...