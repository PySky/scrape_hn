has taken a lot of heat over the past decade, and not without reason. However I believe it is way past time to draw back some of the criticism and explore this area with a dispassionate scrutiny of detail.

There are really two issues here:

I have strong opinions on both of these questions. And to get this out of the way up front:

The array of bits data structure is a wonderful data structure. It is often both a space and speed optimization over the array of bools data structure if properly implemented. However it does not behave exactly as an array of bools, and so should not pretend to be one.

Because holds bits instead of bools, it can't return a from its indexing operator or iterator dereference. This can play havoc on quite innocent looking generic code. For example:

The above code works for all except . When instantiated with , you will receive a compile time error along the lines of:

This is not a great error message. But it is about the best the compiler can do. The user is confronted with implementation details of and in a nutshell says that the is not working with a perfectly valid ranged-based for statement. The conclusion the client comes to here is that the implementation of is broken. And he would be at least partially correct.

But consider if instead of being a specialization instead there existed a separate class template and the coder had written:

Now one gets a similar error message:

And although the error message is similar, the coder is far more likely to see that he is using a dynamic array of bits data structure and it is understandable that you can't form a reference to a bit.

I.e. names are important. And creating a specialization that has different behavior than the primary, when the primary template would have worked, is poor practice.

For example consider this code:

How long does take in the above example for:

I'm testing on an Intel Core i5 in 64 bit mode. I am normalizing all answers such that the speed of A is 1 (smaller is faster):

An array of bits can be a very fast data structure for a sequential search! The optimized is inspecting 64 bits at a time. And due to the space optimization, it is much less likely to cause a cache miss. However if the implementation fails to do this, and naively checks one bit at a time, then this giant 75X optimization turns into a significant pessimization.

can be optimized much like to process a word of bits at a time:

Here the results are not quite as dramatic as for the case. However any time you can speed up your code by a factor of 20, one should do so!

is yet another example:

The optimized fill is over twice as fast as the non-specialized . But if the vendor neglects to specialize for bit-iterators the results are disastrous! Naturally the results are identical for the closely related .

is yet another example:

The optimized copy is approaches three times as fast as the non-specialized . But if the vendor neglects to specialize for bit-iterators the results are not good. If the copy is not aligned on word boundaries (as in the above example), then the optimized copy slows down to the same speed as the copy for A. Results for , and are similar.

is yet another example:

Yet another example of really good results with an optimized algorithm and really poor results without this extra attention.

is yet another example:

Yet another example of good results with an optimized algorithm and very poor results without this extra attention.

is yet another example:

Yet another example of excellent results with an optimized algorithm and very poor results without this extra attention.

The dynamic array of bits is a very good data structure if attention is paid to optimizing algorithms that can process up to a word of bits at a time. In this case it becomes not only a space optimization but a very significant speed optimization. If such attention to detail is not given, then the space optimization leads to a very significant speed pessimization.

But it is a shame that the C++ committee gave this excellent data structure the name and that it gives no guidance nor encouragement on the critical generic algorithms that need to be optimized for this data structure. Consequently, few std::lib implementations go to this trouble.|||

