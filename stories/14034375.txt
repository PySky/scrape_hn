While briefly dwelling on different UNIX interprocess communication mechanisms during the lesson, we have seen how powerful shared memory is: a chunk of logical memory (which will be the same physical page pointed by processes' page tables) arranged by the OS memory pool is given to the process which requested it, then another process maps the segment into its own virtual address space so that both can read/write data of the same memory region. It's cool, especially if you think that virtual addresses are not mandatory to be the same between processes. One value, is that it is also said to be by far the fastest form of IPC available. Now I was wondering how much of this is actually true. Surely, the fact of accessing userland memory at a fast rate and not even being required a kernel mediation when exchanging data ensures high performance, even though this could come much more in useful when sharing quite large block of memory and besides that, coordinating access between processes (with some synchronization mechanism) is needed. Likewise, message passing may be pretty much fast when dealing with a not so big amount of data and it doesn't commit excessive resources either, even if implementation might seem more complicated. 



 So let's try to see how they differ in terms of both speed and implementation on OS X (in reality they almost always complement one another and are closely related). POSIX API includes to send a message, whereas, will be invoked to receive the message. OS X kernel instead, which ultimately relies on message passing principles and whose design is a lot object-oriented, provides some user-transparent low-level primitives abstractions such as tasks (sort of BSD processes which have threads executing in it), ports (kernel-maintained messages queues, that's what allows tasks/threads to communicate with each other), a set of rights (specific capabilities which define whether a task should send messages to a port or receive messages on that port), indeed, raw mach messages (the means of communication which are referenced via ports) and some other objects. Almost all other forms of IPC (e.g. CFMessagePort and CFMachPort, XPC) end up using mach messages (XPC is much funnier though). So say we want to exchange something between two processes (client/server model). We allocate a new port in our own current task ( ), and we give the task receive and send right to the new port, so that we can send messages to it:

Ports are specific to the process though, so we need to make them reachable by the client. One way to let the client know the port it will have to reply to can be to register the service globally via the bootstrap server (which is provided by launchd, which manages all other system services) by calling (since is marked as deprecated), which provides a send right for the and – after having looked up the service – will give the client a send right as well (so is kinda redundant), and after getting the bootstrap port with , we will have something like this:

On the other side, the client – after having allocated a new client port too – will have to search for the server:

Note that we could have also retrieved the task port of the client task given the process ID ( ), or made a port well-known ( , both of them however require elevated privileges. Messages are delivered and received to other userland processes through the trap , which takes as input a pointer to a fixed-length message header ( structure), some scalars such as whether should be used to send or receive, bytes of the message to be sent or to be received, ports where it should be received or notified and a time to wait (if none is indefinite). So as for the server-side, we first need to prepare the message which will wait for the client task port, and then reply to the client with some appropriate data:

Where and structures describe the message to send and to receive respectively (they both consist of an header and inline data, except for the fact that must be greater than since some bytes are added by the kernel so to ensure that message sent doesn't fail we end up adding a trailer to the receiver); and as regards the client-side, we first let the server know that our local port is going to reach the server and then we wait for the data coming from the server:

The client, which should have successfully received the message on its port, can then read the buffer ( ), which in this case was an array of characters. Now say we want to exchange something more than just a string. Turns out that on OS X, when passing not simple inline data (like a memory object, just a pointer), a message can include a pointer to out-of-line (OOL) data, that's a memory address location of a region of the sender's virtual address space. Clearly, the bigger the chunk gets, the more inefficient copying all data from the sender to the receiver is, therefore memory pages are transferred with virtual copy (copy-on-write) techniques so that most of the times actual copying is not even performed. The way I figured it out, depending on whether we basically want to pass a (e.g. to map a file or device into memory via syscall) or share a pair of , we need to include additional structures, which are either to pass out-of-line data or to pass around a port right. The latter allows a way to implement shared memory, which implies reserving a region of memory ( ) and getting a named reference ( ) to the given memory object to send to other tasks; after sending the handle, receiving task will be able to map it into its own address space ( ). In both cases, message needs to be marked as complex ( ). We go with the first one, so sending and receving message structures are changed as follows:

Where the header is unchanged and outlines the beginning of kernel-processed data. We fill a 4MB region of memory with e.g. :

Maybe not exactly a great idea, at least we are pretty sure memory has been granted (comparing the state of the process at the beginning and the end of the process with , has risen), next we set up the fields of the previously modified structures sending-side:

And by doing so, client will be happily reading the chunk of memory. 



 Concerning shared memory, since on OS X it is mostly accomplished through message passing – being indeed, as a microkernel, a message-oriented kernel –, just as messages extensively use shared memory, we will take advantage of the BSD layer (to get rid of messages) by using ( is just a number both sides agree to use) to obtain an identifier of the shared memory segment and (where identifies the given segment) to get a pointer to it. Issue here is just to set up a proper way of synchronization between the two processes. File locking would be easier to implement, System V semaphores are more complicated instead, although they allow for more control and flexibility. Anyway just keep in mind that we can perform atomically operations on the semaphore of wait and signal kind (the semaphore is of type , which contains the semaphore number, the operation, and some flags) through , after having initialized its value to 0:

For the sake of laziness, we just time a process which forks and waits for the child: the child forks again so that this and the new child execute the client and server instances each.

I have done multiple tests and then took an average, don't know how much these results can be reliable (since to get statistics more plausible we perhaps should have allocated much more memory). Looks like as if they are roughly equivalent, but yes, message passing is slightly faster when dealing with small amount of data, shared memory remains mostly costant in time. Times notwithstanding, it's been interesting to see how one method actually uses the other to achieve the exchange of data (and vice versa). Code will be published soon.|||

