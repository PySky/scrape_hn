Let's pretend you're changing the way you handle permissions in a large web app. Tests can help guide your refactoring, but you really want to compare the current and refactored behaviors under load.

Wrap a block around the code's original behavior, and wrap around the new behavior. will always return whatever the block returns, but it does a bunch of stuff behind the scenes:

The block is called the control. The block is called the candidate.

Creating an experiment is wordy, but when you include the module, the helper will instantiate an experiment and call for you:

If you don't declare any blocks, none of the Scientist machinery is invoked and the control value is always returned.

The examples above will run, but they're not really doing anything. The blocks run every time and none of the results get published. Replace the default experiment implementation to control execution and reporting:

Now calls to the helper will load instances of .

Scientist compares control and candidate values using . To override this behavior, use to define how to compare observed values instead:

Results aren't very useful without some way to identify them. Use the method to add to or retrieve the context for an experiment:

takes a Symbol-keyed Hash of extra data. The data is available in via the method. If you're using the helper a lot in a class, you can provide a default context:

The and experiments will both have a key in their contexts.

If an experiment requires expensive setup that should only occur when the experiment is going to be run, define it with the method:

Sometimes you don't want to store the full value for later analysis. For example, an experiment may return instances, but when researching a mismatch, all you care about is the logins. You can define how to clean these values in an experiment:

And this cleaned value is available in observations in the final published result:

During the early stages of an experiment, it's possible that some of your code will always generate a mismatch for reasons you know and understand but haven't yet fixed. Instead of these known cases always showing up as mismatches in your metrics or analysis, you can tell an experiment whether or not to ignore a mismatch using the method. You may include more than one block if needed:

The ignore blocks are only called if the values don't match. If one observation raises an exception and the other doesn't, it's always considered a mismatch. If both observations raise different exceptions, that is also considered a mismatch.

Sometimes you don't want an experiment to run. Say, disabling a new codepath for anyone who isn't staff. You can disable an experiment by setting a block. If this returns , the experiment will merely return the control value. Otherwise, it defers to the experiment's configured method.

As a scientist, you know it's always important to be able to turn your experiment off, lest it run amok and result in villagers with pitchforks on your doorstep. In order to control whether or not an experiment is enabled, you must include the method in your implementation.

This code will be invoked for every method with an experiment every time, so be sensitive about its performance. For example, you can store an experiment in the database but wrap it in various levels of caching such as memcache or per-request thread-locals.

What good is science if you can't publish your results?

You must implement the method, and can publish data however you like. For example, timing data can be sent to graphite, and mismatches can be placed in a capped collection in redis for debugging later.

The method is given a instance with its associated s:

When running your test suite, it's helpful to know that the experimental results always match. To help with testing, Scientist defines a class attribute when you include . Only do this in your test suite!

Scientist will raise a exception if any observations don't match.

If an exception is raised within any of scientist's internal helpers, like , , or , the method is called with the symbol name of the internal operation that failed and the exception that was raised. The default behavior of is to simply re-raise the exception. Since this halts the experiment entirely, it's often a better idea to handle this error and continue so the experiment as a whole isn't canceled entirely:

The operations that may be handled here are:

Because and determine when a candidate runs, it's impossible to guarantee that it will run every time. For this reason, Scientist is only safe for wrapping methods that aren't changing data.

When using Scientist, we've found it most useful to modify both the existing and new systems simultaneously anywhere writes happen, and verify the results at read time with . has also been useful to ensure that the correct data was written during tests, and reviewing published mismatches has helped us find any situations we overlooked with our production data at runtime. When writing to and reading from two systems, it's also useful to write some data reconciliation scripts to verify and clean up production data alongside any running experiments.

As your candidate behavior converges on the controls, you'll start thinking about removing an experiment and using the new behavior.

Sometimes scientists just gotta do weird stuff. We understand.

Science is useful even when all you care about is the timing data or even whether or not a new code path blew up. If you have the ability to incrementally control how often an experiment runs via your method, you can use it to silently and carefully test new code paths and ignore the results altogether. You can do this by setting , or for greater efficiency, .

This will still log mismatches if any exceptions are raised, but will disregard the values entirely.

It's not usually a good idea to try more than one alternative simultaneously. Behavior isn't guaranteed to be isolated and reporting + visualization get quite a bit harder. Still, it's sometimes useful.

To try more than one alternative at once, add names to some blocks:

When the experiment runs, all candidate behaviors are tested and each candidate observation is compared with the control in turn.

Define the candidates with named blocks, omit a , and pass a candidate name to :

The helper also knows this trick:

Be on a Unixy box. Make sure a modern Bundler is available. runs the unit tests. All development dependencies are installed automatically. Science requires Ruby 1.9 or newer.|||

scientist - :microscope: A Ruby library for carefully refactoring critical paths.