What is a wave? A wave in my parlance is a set of articles that make the same new (and possibly erroneous) claim, plus associated social media posts. A wave is significant if it is growing in engagement. Since the cost of human intervention is high, it only makes sense to flag significant waves that have traits that suggest misinformation.

The goal of the detection algorithm is to flag suspicious waves before they cross an exposure threshold, so that human responders can do something about it.

To make this concrete: Let us say that a social media platform has decided that it wants to fully address fake news by the time it gets 10,000 shares. To achieve this they may want to have the wave flagged at 1,000 shares, so that human evaluators have time to study it and respond. For search, you would count queries and clicks rather than shares and the thresholds could be higher, but the overall logic is the same.

To detect anomalous behavior we have to look below the surface and see what’s not happening. This, from a Sherlock Holmes story captures the essence of our strategy.

What makes detecting fake news tractable is that platforms are able to observe articles and posts, not just in isolation, but in the context of all else that is being said on that subject in real-time. This expanded and timely context makes all the difference.

If you are an average Facebook user and the article was shared to you by a friend, you have no reason to disbelieve it. We have a truth bias that makes us want to believe things typeset in the format of a newspaper, especially if it is endorsed by someone you know. Hence, the outgrowth of newly minted fake news sites that are trying to look legitimate. Some by Macedonian teenagers, purely for profit, or by political professionals or foreign actors seeking to influence elections. As they get tagged and put on blacklists new sites are created out of necessity.

A skeptic would ask: How likely is it that endingthefed.com, a relatively obscure source, is one of the first to report a story about the Pope endorsing Trump, while established sources like the New York Times, Washington Post, BBC, Fox News, CNN, etc. and even the Vatican News Service, have nothing to say on the subject? That would seem unnatural. It would be even more suspicious if the set of news sites talking about this are all newly registered or have a history of running fake news. This is the logic we are going to employ, but with some automation.

To do this at scale, an algorithm would look at all recent articles (from known and obscure sources) that have been getting some play in the last 6–12 hours on a particular social network or search engine. To limit the scope we could require a match with some trigger terms (e.g., names of politicians, controversial topics) or news categories (e.g., politics, crime, immigration). This would reduce the set to around 10,000 articles. These articles can be analyzed and grouped into story buckets, based on common traits — significant keywords, dates, quotes, phrases, etc. None of this is technically complex. Computer Scientists have been doing this for decades and call this “document clustering.”

Articles that land in a given story bucket would be talking about the same story. This technique has been used successfully in Google News and Bing News, to group articles by story and to compare publishing activity between stories. If two different sources mention “pope” and “Trump” and some variant of the term “endorsed” within a short time window then their articles will end up in the same bucket. This essentially helps us capture the full coverage of a story, across various news sources. Add in the social context, i.e., the posts that refer to these articles, and you have the full wave. Most importantly this allows us to figure out comprehensively which sources and authors are propagating this news and which are not.

To assess whether the wave needs to be flagged as suspicious, the algorithm will need to look at traits of both the story cluster and the social media cloud surrounding it. Specifically:

Each of the above can be assessed by computers. Not perfectly perhaps, but sufficiently well to serve as a signal. Some carefully constructed logic will combine these signals to produce a final score to rate how suspicious the wave is.

When a wave has the traits of a fake news story the algorithm can flag it for human attention, and potentially put temporary brakes on it. This will buy time and ensure that it does not cross the high water mark of say, 10,000 shares or views, while the evaluation is in progress.

With every wave that is evaluated by human judges — and there may be several dozen a day — the system will get feedback. This in turn allows algorithmic/neural net parameters to be tuned and helps extend the track record for sources, authors and forums. Even waves that could not be stopped in time, but eventually proved to be misinformation, could contribute to improve the model. Over time this should make detection more accurate, reducing the incidence of false alarms in the flagging step.

Trading free expression for security is a slippery slope and inevitably a bad idea.

It is important that the policing of fake news by platforms happen in a way that is both defensible and transparent. Defensible, in the sense that they explain what they are policing and how this being executed, and operate in a manner that the public is comfortable with. I would expect them to target fake news narrowly to only encompass factual claims that are demonstrably wrong. They should avoid policing opinion or claims that cannot be checked. Platforms like to avoid controversy and a narrow, crisp definition will keep them out of the woods.

In terms of transparency, I would expect all news that has been identified as false and slowed down or blocked to be revealed publicly. They may choose to delay this to avoid tipping their hand during the news cycle, but they should disclose within a reasonable period (say, 15 days) all news that was impacted. This, above all else, will prevent abuse by the platform. Google, Facebook and others have transparency reports that disclose censorship and surveillance requests by governments and law enforcement. It’s only appropriate that they too are transparent about actions that limit speech.

Having been on the other side of this issue, I can think of reasons why details of the detection algorithm may need to be kept secret. A platform, in an arms race with fake news producers, may find that their strategy stops working if too much is made public. A compromise would be to document details of the implementation and make it available for internal scrutiny by (a panel of) employees. Also, for auditing by an ombudsman or authorized external lawyers. When it comes to encouraging good corporate behavior employees are the first line of defense. They are technically capable and come from across the political spectrum. They can confirm there is no political bias in the implementation.|||

Last November, a friend told me about his extended family of Filipino-Americans in the Fresno area. In a matter of days they went from feeling conflicted about Trump’s candidacy to voting for him en…