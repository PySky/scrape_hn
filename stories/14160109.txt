I have been doing Kaggle’s Quora Question Pairs competition for about a month now, and by reading the discussions on the forums, I’ve noticed a recurring topic that I’d like to address. People seem to be struggling with getting the performance of their models past a certain point. The usual approach is to use XGBoost, ensembles and stacking. While those can generally give good results, I’d like to talk about why it is still important to do feature importance analysis.

As an example, I will be using the Quora Question Pairs dataset. The dataset has 404,290 pairs of questions, and 37% of them are semantically the same (“duplicates”). The goal is to find out which ones.

Examples of duplicate and non-duplicate question pairs are shown below.

This is the word cloud inspired by a Kaggle kernel for data exploration. The cloud shows which words are popular (most frequent). The word cloud is created from words used in both questions. As you can see, the prevalent words are ones you would expect to find in a question (e.g. “best way”, “lose weight”, “difference”, “make money”, etc.)

We now have some idea about what our dataset looks like.

I created 24 features, some of which are shown below. All code is written in python using the standard machine learning libraries (pandas, sklearn, numpy). You can get the full code from my github notebook. Examples of some features:

To get the model performance, we first split the dataset into the train and test set. The test set contains 20% of the total data. To evaluate the model’s performance, we use the created test set (X_test and y_test).

The model is evaluated with the logloss function. It is the same metric which is used in the competition.

To test the model with all the features, we use the Random Forest classifier. It is a powerful “out of the box” ensemble classifier. No hyperparameter tuning was done – they can remain fixed because we are testing the model’s performance against different feature sets. A simple model gives a logloss score of 0.62923, which would put us at the 1371th place of a total of 1692 teams at the time of writing this post. Now let’s see if doing feature selection could help us lower the logloss.

To get the feature importance scores, we will use an algorithm that does feature selection by default – XGBoost. It is the king of Kaggle competitions. If you are not using a neural net, you probably have one of these somewhere in your pipeline. XGBoost uses gradient boosting to optimize creation of decision trees in the ensemble. Each tree contains nodes, and each node is a single feature. The number of instances of a feature used in XGBoost decision tree’s nodes is proportional to its effect on the overall performance of the model.

Looking at the graph below, we see that some features are not used at all, while some (word_share) impact the performance greatly. We can reduce the number of features by taking a subset of the most important features.

Using the feature importance scores, we reduce the feature set. The new pruned features contain all features that have an importance score greater than a certain number.  In our case, the pruned features contain a minimum importance score of 0.05.

As a result of using the pruned features, our previous model – Random Forest – scores better. With little effort, the algorithm gets a lower loss, and it also trains more quickly and uses less memory because the feature set is reduced.

Playing a bit more with feature importance score (plotting the logloss of our classifier for a certain subset of pruned features) we can lower the loss even more. In this particular case, Random Forest actually works best with only one feature! Using only the feature “word_share” gives a logloss of 0.5544. If you are interested to see this step in detail, the full version is in the notebook.

As I have shown, utilising feature importance analysis has a potential to increase the model’s performance. While some models like XGBoost do feature selection for us, it is still important to be able to know the impact of a certain feature on the model’s performance because it gives you more control over the task you are trying to accomplish. The “no free lunch” theorem (there is no solution which is best for all problems) tells us that even though XGBoost usually outperforms other models, it is up to us to discern whether it is really the best solution. Using XGBoost to get a subset of important features allows us to increase the performance of models without feature selection by giving that feature subset to them. Using feature selection based on feature importance can greatly increase the performance of your models.|||

Feature importance in machine learning using examples in Python with xgboost. Getting better performance from a model with feature pruning.