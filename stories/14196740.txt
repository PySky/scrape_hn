A comprehensive log management and analysis strategy is mission critical, enabling organisations to understand the relationship between operational, security, and change management events and maintain a comprehensive understanding of their infrastructure. Log files from web servers, applications, and operating systems also provide valuable data, though in different formats, and in a random and distributed fashion.

Why is Apache Web Server so popular? It’s free and open source, and open source is becoming vastly more popular than proprietary software. It’s maintained by dedicated developers, it provides security, is well suited for small and large websites alike, can be easily set up on all major operating systems, as well as being extremely powerful and flexible. Does that sound about right?

Provisioning an Elasticsearch cluster in Qbox is easy. In this article, we walk you through the initial steps and show you how simple it is to start and configure your cluster. We then install and configure logstash to ship our apache logs to elasticsearch. Apache logs shipped to elasticsearch can then be visualized and analyzed via Kibana dashboards.

The goal of the tutorial is to use Qbox as a Centralized Logging and Monitoring solution for Apache logs. Qbox provides out-of-box solutions for Elasticsearch, Kibana and many of Elasticsearch analysis and monitoring plugins. We will set up Logstash in a separate node to gather apache logs from single or multiple servers, and use Qbox’s provisioned Kibana to visualize the gathered logs.

Our ELK stack setup has three main components:

For this post, we will be using hosted Elasticsearch on Qbox.io. You can sign up or launch your cluster here, or click "Get Started" in the header navigation. If you need help setting up, refer to "Provisioning a Qbox Elasticsearch Cluster."

Let’s review how the ELK pipeline will work:

The amount of CPU, RAM, and storage that your Elasticsearch Server will require depends on the volume of logs that you intend to gather. For this tutorial, we will be using a Qbox provisioned Elasticsearch with the following minimum specs:

The above specs can be changed per your desired requirements. Please select the appropriate names, versions, regions for your needs. For this example, we used , the most current version is . We support all versions of Elasticsearch on Qbox. (To learn more about the major differences between 2.x and 5.x, click here.)

In addition to our Elasticsearch Server, we will require a separate logstash server to process incoming apache logs from client servers and ship them to Elasticsearch. There can be a single or multiple client servers for which you wish to ship logs to Elasticsearch. For simplicity or testing purposes, the logstash server can also act as the client server itself. The Endpoint and Transport addresses for our Qbox provisioned Elasticsearch cluster are as follows:

Note: Please make sure to whitelist the logstash server IP from Qbox Elasticsearch cluster. Also, the elasticsearch server must have access to all client servers to collect apache logs from.

Apache is a free open source software which runs over 50% of the world’s web servers.

To install apache, open terminal and type in these commands:

That’s it. To check if Apache is installed, direct your browser to your server’s IP address (eg. ). The page should display the words “It works!".

How to find your server’s IP address

You can run the following command to reveal your server’s IP address.

The Apache File Hierarchy in Ubuntu and Debian



On Ubuntu and Debian, Apache keeps its main configuration files within the "/etc/apache2" folder:

There are a number of plain text files and some sub-directories in this directory. These are some of the more useful locations to be familiar with:

We will use the Logstash version 2.4.x as compatible with our Elasticsearch version 5.1.x. The Elastic Community Product Support Matrix can be referred in order to clear any version issues.

Add the repository definition to your /etc/apt/sources.list file:

Run sudo apt-get update and the repository is ready for use. You can install it with:

Alternatively, logstash tar can also be downloaded from Elastic Product Releases Site. Then, the steps of setting up and running logstash are pretty simple:

Logstash configuration files are in the JSON-format, and reside in /etc/logstash/conf.d. The configuration consists of three sections: inputs, filters, and outputs.

Let's create a configuration file called and set up our " " input:

NOTE : Apache logs file path may differ based upon your environment and underlying OS.



Save and quit. This specifies a apache input that will listen on tcp port . Now let's create a configuration file called , where we will add a filter for apache messages:

Save and quit. This filter looks for logs that are labeled as " " type and it will try to use grok to parse incoming apache logs to make it structured and queryable.

Save and exit. This output basically configures Logstash to store the logs data in Elasticsearch which is running at , in an index named after the apache.

If you have downloaded logstash tar or zip, you can create a file having input, filter and output all in one place.

Insert the following input, filter and output configuration in :

If you want to add filters for other applications that use the apache input, be sure to name the files so they sort between the input and the output configuration (i.e. between 02- and 30-).

Test your Logstash configuration with this command:

It should display Configuration OK if there are no syntax errors. Otherwise, try and read the error output to see what's wrong with your Logstash configuration.

Restart Logstash, and enable it, to put our configuration changes into effect:

If you have downloaded logstash tar or zip, it can be run using following command

Next, we'll load the sample Kibana dashboards.

When you are finished setting Logstash server to collect logs from client servers, let's look at Kibana, the web interface provisioned by Qbox. Kibana User interface can be used for filtering, sorting, discovering and visualizing logs that are stored in Elasticsearch. Go ahead and click on Visualize data with Kibana from your cluster configuration dashboard.

Go ahead and select from the Index Patterns menu (left side), then click the Star (Set as default index) button to set the apache index as the default.

Now click the Discover link in the top navigation bar. By default, this will show you all of the log data over the last 15 minutes. You should see a histogram with log events, with log messages below:

Right now, there won't be much in there because you are only gathering apache logs from your client servers. Here, you can search and browse through your logs. You can also customize your dashboard.

Try the following things:

Kibana has many other features, such as graphing and filtering, so feel free to poke around!

Qbox provisioned Elasticsearch makes it very easy for us to visualize centralized logs using logstash and Kibana. Remember that we can send pretty much any type of log or indexed data to Logstash, but the data becomes even more useful if it is parsed and structured with grok.

What do we look for in centralized logging? As it happens, many things, but the most important are as follows.

Log analysis for operational intelligence, business intelligence and technical SEO are just three examples of why Apache users need to monitor logs. There are many more use cases, such as log driven development and application monitoring. The ELK stack (Logstash, Elasticsearch, and Kibana) can do all that and it can easily be extended to satisfy the particular needs we’ll set in front of us.

It's easy to spin up a standard hosted Elasticsearch cluster on any of our 47 Rackspace, Softlayer, Amazon, or Microsoft Azure data centers. And you can now provision your own AWS Credits on Qbox Private Hosted Elasticsearch.

Questions? Drop us a note, and we'll get you a prompt response.

Not yet enjoying the benefits of a hosted ELK stack enterprise search on Qbox? We invite you to create an account today and discover how easy it is to manage and scale your Elasticsearch environment in our cloud hosting service.|||

