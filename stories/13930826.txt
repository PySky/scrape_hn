In the first part of the series I've briefly introduced the BF source language and went on to present four interpreters with increasing degree of optimization. That post should serve as a good backgroud before diving into actual JIT-ing.

Another important part of the background puzzle is my How to JIT - an introduction post from 2013; there, I discuss some of the basic tools needed to emit executable x64 machine code at run-time and actually run it on Linux. Please go through it quickly if these things are new to you.

The two phases of JIT As I wrote previously, the JIT technique is easier to understand when divided into two distinct phases: Execute that machine code, also at program run-time. Phase 2 for our BF JIT is exactly identical to the method described in that introductory post. Take a look at the class in jit_utils for details. We'll be more focused on phase 1, which will be translating BF to x64 machine code; per the definition quoted in part 1 of the series, we're going to develop an actual BF compiler (compiling from BF source to x64 machine code).

Traditionally, compilation was divided into several stages. The actual source language compiler would translate some higher-level language to target-specific assembly; then, an assembler would translate assembly to actual machine code . There's a number of important benefits assembly language provides over raw machine code. Salient examples include: Instruction encoding: it's certainly nicer to write to increment the contents of register than to write . Instruction encoding for the popular architectures is notoriously complicated. Naming labels and procedures for jumps/calls: it's easier to write than to figure out the encoding for the instruction, along with the relative position of the label and encoding the delta to it (not to mention this delta changes every time we add instructions in between and needs to be recomputed). Similarly for functions, instead of doing it by address. One of my guiding principles through the field of programming is that before diving into the possible solutions for a problem (for example, some library for doing X) it's worth working through the problem manually first (doing X by hand, without libraries). Grinding your teeth over issues for a while is the best way to appreciate what the shrinkwrapped solution/library does for you. In this spirit, our first JIT is going to be completely hand-written.

Out first JIT for this post is simplejit.cpp. Similarly to the interpreters of part 1, all the action happens in a single function (here called ) invoked from . goes through the BF source and emits x64 machine code into a memory buffer; in the end, it jumps to this machine code to run the BF program. // Registers used in the program: // r13: the data pointer -- contains the address of memory.data() // rax, rdi, rsi, rdx: used for making system calls, per the ABI. // Throughout the translation loop, this stack contains offsets (in the As usual, we have our BF memory buffer in a . The comments reveal some of the conventions used througout the emitted program: our "data pointer" will be in . is a very simple utility to append bytes and words to a vector of bytes. Its full code is here. It's platform independent except the assumption of little-endian (for it will write the lowest byte of the 64-bit word first, then the second lowest byte, etc.) Our first bit of actual machine code emission follows: And it's a cool one, mixing elements from the host (the C++ program doing the emission) and the JITed code. First note the usage of , a x64 instruction useful for placing 64-bit immediates in a register. This is exactly what we're doing here - placing the address of the data buffer of in . The call to with a cryptic sequence of hex values is preceded by a snippet of assembly in a comment - the assembly conveys the meaning for human readers, the hex values are the actual encoding the machine will understand. Then comes the BF compilation loop, which looks at the next BF instruction and emits the appropriate machine code for it. Our compiler works in a single pass; this means that there's a bit of trickiness in handling the jumps, as we will soon see. : : : // Our memory is byte-addressable, so using addb/subb for modifying it. : These are pretty straightforward; since is the data pointer, and increment and decrement it, while and increment and decrement what it's pointing to. One slightly subtle aspect is that I chose a byte-value memory for our BF implementations; this means we have to be careful when reading or writing to memory and do byte-addressing (the suffixes on and above) rather than the default 64-bit-addressing. The code emitted for and is a bit more exciting; in the effort of avoiding any external dependencies, we're going to invoke Linux system calls directly. for ; for . We're using the x64 ABI here with the syscall identifier in : // To emit one byte to stdout, call the write syscall with fd=1 (for : // To read one byte from stdin, call the read syscall with fd=0 (for The comments certainly help, don't they? I hope these snippets are a great motivation for using assembly language rather than encoding instructions manually :-) The jump instructions are always the most interesting in BF. For we do: : // For the jumps we always emit the instruciton for 32-bit pc-relative // jump, without worrying about potentially short jumps and relaxation. // Save the location in the stack, and emit JZ (with 32-bit relative // offset) with 4 placeholder zeroes that will be fixed up later. Note that we don't know where this jump leads at this point - it will go to the matching , which we haven't encountered yet! Therefore, to keep our compilation in a single pass we use the time-honored technique of backpatching by emitting a placeholder value for the jump and fixing it up once we encounter the matching label. Another thing to note is always using a 32-bit pc-relative jump, for simplicity; we could save a couple of bytes with a short jump in most cases (see my article on assembler relaxation for the full scoop), but I don't think it's worth the effort here. Compiling the matching is a bit trickier; I hope the comments do a good job explaining what's going on, and the code itself is optimized for readability rather than cleverness: : // open_bracket_offset points to the JZ that jumps to this closing // bracket. We'll need to fix up the offset for that JZ, as well as emit a // JNZ with a correct offset back. Note that both [ and ] jump to the // instruction *after* the matching bracket if their condition is // Compute the offset for this jump. The jump start is computed from after // the jump instruction, and the target is the instruction after the one // Also fix up the forward jump at the matching [. Note that here we don't // need to add the size of this jmp to the "jump to" offset, since the jmp // was already emitted and the emitter size was bumped forward. This concludes the compiler loop; we end up with a bunch of potentially executable machine code in . This code refers to the host program (the address of ), but that's OK since the host program's lifetime wraps the lifetime of the JITed code. What's remaining is to actually invoke this machine code: // The emitted code will be called as a function from C++; therefore it has to // use the proper calling convention. Emit a 'ret' for orderly return to the // Load the emitted code to executable memory and run it. // JittedFunc is the C++ type for the JIT function emitted here. The emitted // function is callable from C++ and follows the x64 System V ABI. The call should be familiar from reading the How to JIT post. Note that here we opted for the simplest function possible - no arguments, no return value; in future sections we'll spice it up a bit.

In part 1, I presented a trivial BF program that prints the numbers 1 to 5 to the screen: Let's see what our compiler translates it to. Even though the code vector inside is ephemeral (lives only temporarily in memory), we can serialize it to a binary file which we can then disassemble (with ). The following is the disassembly listing with comments I embedded to explain what's going on: # The runtime address of memory.data() goes into r13; note that this will # likely be a different value in every invocation of the JIT. 0: 49 bd f0 54 e3 00 00 movabs $0xe354f0,%r13 7: 00 00 00 # A sequence of 48 instructions that all do the same, for the initial sequence # of +s; this makes me miss our optimizing interpreter, by worry not - we'll # make this go away later in the post. a: 41 80 45 00 01 addb $0x1,0x0(%r13) f: 41 80 45 00 01 addb $0x1,0x0(%r13) # [...] 46 more 'addb' # >+++++ fa: 49 ff c5 inc %r13 fd: 41 80 45 00 01 addb $0x1,0x0(%r13) 102: 41 80 45 00 01 addb $0x1,0x0(%r13) 107: 41 80 45 00 01 addb $0x1,0x0(%r13) 10c: 41 80 45 00 01 addb $0x1,0x0(%r13) 111: 41 80 45 00 01 addb $0x1,0x0(%r13) # Here comes the loop! Note that the relative jump offset is already inserted # into the 'je' instruction by the backpatching process. 116: 41 80 7d 00 00 cmpb $0x0,0x0(%r13) 11b: 0f 84 35 00 00 00 je 0x156 121: 49 ff cd dec %r13 124: 41 80 45 00 01 addb $0x1,0x0(%r13) # The '.' is translated into a syscall to WRITE 129: 48 c7 c0 01 00 00 00 mov $0x1,%rax 130: 48 c7 c7 01 00 00 00 mov $0x1,%rdi 137: 4c 89 ee mov %r13,%rsi 13a: 48 c7 c2 01 00 00 00 mov $0x1,%rdx 141: 0f 05 syscall 143: 49 ff c5 inc %r13 146: 41 80 6d 00 01 subb $0x1,0x0(%r13) 14b: 41 80 7d 00 00 cmpb $0x0,0x0(%r13) # Jump back to beginning of loop 150: 0f 85 cb ff ff ff jne 0x121 # We're done 156: c3 retq

How does it perform? It's time to measure the performance of our JIT against the interpreters from part 1. was about 10x faster than the naive interpreter - how will this JIT measure up? Note that it has no optimizations (except not having to recompute the jump destination for every loop iteration as the naive interpreter did). Can you guess? The results may surprise you... The simple JIT runs in 2.89 seconds, and in 0.94 seconds - much faster still than ; here's the comparison plot (omitting the slower interpreters since they skew the scale): Why is this so? is heavily optimized - it folds entire loops into a single operation; does none of this - we've just seen the embarrassing sequence of s it emits for a long sequence of s. The reason is that the baseline performance of the JIT is vastly better. I've mentioned this briefly in part 1 - imagine what's needed to interpret a single instruction in the fastest interpreter. Advance and compare it to program size. Switch on the value of the instruction to the right . This requires a whole sequence of machine instructions, with at least two branches (one for the loop, one for the ). On the other hand, the JIT just emits a single instruction - no branches. I would say that - depending on what the compiler did while compiling the interpreter - the JIT is between 4 and 8 times faster at running any given BF operation. It has to run many more BF operations because it doesn't optimize, but this difference is insufficient to close the huge baseline gap. Later in this post we're going to see an optimized JIT which performs even better. But first, let's talk about this painful instruction encoding business.

As promised, is completely self-contained. It doesn't use any external libraries, and encodes all the instructions by hand. It's not hard to see how painful that process is, and the code is absolutely unreadable unless accompanied by detailed comments; moreover, changing the code is a pain, and changes happen in unexpected ways. For example, if we want to use some other register in an instruction, the change to emitted code won't be intuitive. is encoded as , but is ; since registers are specified in sub-byte nibbles, one needs very good memory and tons of experience to predict what goes where. Would you expect related instructions to look somewhat similar? They don't. is encoded as , for example. To put it bluntly - unless you're Mel, you're going to have a hard time. Now imagine that you have to support emitting code for multiple architectures! This is why all compilers, VMs and related projects have their own layers to help with this encoding task, along with related tasks like labels and jump computations. Most are not exposed for easy usage outside their project; others, like DynASM (developed as part of the LuaJIT project) are packaged for separate usage. DynASM is an example of a low-level framework - providing instruction encoding and not much else; some frameworks are higher-level, doing more compiler-y things like register allocation. One example is libjit; another is LLVM.

While looking for a library to help me encode instructions, I initially tried DynASM. It's an interesting approach - and you can see Josh Haberman's post about using it for a simple BF JIT, but I found it to be a bit too abandonware-ish for my taste. Besides, I don't like the funky preprocessor approach with a dependency on Lua. So I found another project that seemed to fit the bill - asmjit - a pure C++ library without any preprocessing. began about 3 years ago to ease its author's development of fast kernels for graphics code. Its documentation isn't much better than 's, but being just a C++ library I found it easier to dive into the source when questions arose the docs couldn't answer. Besides, the author is very active and quick in answering questions on Github and adding missing featuers. Therefore, the rest of this post shows BF JITs that use - these can also serve as a non-trivial tutorial for the library.

Enter simpleasmjit.cpp - the same simple JIT (no optimizations) as , but using for the instruction encoding, labels and so on. Just for fun, we'll mix things up a bit. First, we'll change the JITed function signature from to ; the address of the BF memory buffer will be passed as argument into the JITed function rather than hard-coded into it. Second, we'll use actual C functions to emit / input characters, rather than system calls. Moreover, since and may be macros on some systems, taking their address can be unsafe. So we'll wrap them in actual C++ functions, whose address it is safe to take in emitted code: starts by initializing an runtime, code holder and assembler : Next, we'll give a mnemonic name to our data pointer, and emit a copy of the address of the memory buffer into it (it's in initially, as the first function argument in the x64 ABI): // We pass the data pointer as an argument to the JITed function, so it's // expected to be in rdi. Move it to r13. Then we get to the usual BF processing loop that emits code for every BF op: Notice the difference! No more obscure hex codes - is so much nicer than , isn't it? For input and output we emit calls to our wrapper functions: : : // Store only the low byte to memory to avoid overwriting unrelated data. The magic is in the modifier, which places the address of the function in the emitted code. Finally, the code handling and is also much simpler due to asmjit's labels, which can be used before they're actually emitted: : // yet (it will be bound when we handle the matching ']'), but asmjit lets // us emit the jump now and will handle the back-patching later. // open_label is bound past the jump; all in all, we're emitting: // Save both labels on the stack. : We just have to remember which label we used for the jump and emit the exact same object - handles the backpatching on its own! Moreover, all the jump offset computations are performed automatically. Finally, after emitting the code we can call it: // Call it, passing the address of memory as a parameter. That's it. This JIT emits virtually the same exact code as , and thus we don't expect it to perform any differently. The main point of this exercise is to show how much simpler and more pleasant emitting code is with a library like . It hides all the icky encoding and offset computations, letting us focus on what's actually unique for our program - the sequence of instructions emitted.

Finally, it's time to combine the clever optimizations we've developed in part 1 with the JIT. Here, I'm essentially taking from part 1 and bolting a JIT backend onto it. The result is optasmjit.cpp. Recall that instead of the 8 BF ops, we have an extended set, with integer arguments, that conveys higher-level ops in some cases: The translation phase from BF ops to a sequence of is exactly the same as it was in . Let's take a look at how a couple of the new ops are implemented now: As before with the interpreters, an increment of 1 is replaced by the addition of an argument. We use a different instruction for this - instead of . How about something more interesting: : // Only move if the current data isn't 0: // Use rax as a temporary holding the value of at the original pointer; // then use al to add it to the new location, so that only the target I'll just note again how much simpler this code is to write with than without it. Also note the careful handling of the byte-granulated data when touching memory - I ran into a number of nasty bugs when developing this. In fact, using the native machine word size (64 bits in this case) for BF memory cells would've made everything much simpler; 8-bit cells are closer to the common semantics of the language and provide an extra challenge.

I find writing JITs lots of fun. It's really nice to be able to hand-craft every instruction emitted by the compiler. While this is quite painful to do without any encoding help, libraries like make the process much more pleasant. We've done quite a bit in this part of the series. is a genuine optimizing JIT for BF! It: Translates it to a sequence of higher-level ops Compiles the ops to tight x64 assembly in memory and runs them Let's connect these steps to some real compiler jargon. ops can be seen as the compiler IR. Translation of human-readable source code to IR is often the first step in compilation (though it in itself is sometimes divided into multiple steps for realistic languages). The translation/compilation of ops to assembly is often called "lowering"; in some compilers this involves multiple steps and intermediate IRs. I left a lot of code out of the blog post - otherwise it would be huge! I encourage you to go back through the full source files discussed here and understand what's going on - every JIT is a single standalone C++ file.|||

