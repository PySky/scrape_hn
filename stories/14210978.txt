So you‚Äôve decided it‚Äôs time to add some of this hot new machine learning or deep learning stuff into your app‚Ä¶ Great! But what are your options?

For many machine learning solutions, the approach is as follows:

Let‚Äôs say you want to make a ‚Äúcelebrity match‚Äù app that tells people which famous person they most look alike.

You need to first gather a lot of photos of celebrities‚Äô faces. Then you train a deep learning network on these photos to teach it what each celebrity looks like.

The model you‚Äôre using would be some kind of convolutional neural network and you‚Äôve trained it for the specific purpose of comparing people‚Äôs faces with the faces of celebrities.

Training is a difficult and expensive process. But once you have the trained model, performing ‚Äúinference‚Äù ‚Äî or in other words, making predictions ‚Äî is easy.

You can give the model a selfie and it will instantly say, ‚ÄúYou look 85% like George Clooney but you have Lady Gaga‚Äôs eyes!‚Äù

Exactly what data you need, what sort of model you should design, and how you should train this model all depend on the particular application you have in mind.

But you also need to make some choices about how to put your machine learning system into production. That‚Äôs what this blog post is about.

The main things you need to decide are:

In other words, should you use a cloud service to do the deep learning, or maybe roll your own? Let‚Äôs find out!

The first question to ask is: Do you even need your own model? By far the easiest way to get started is to use someone else‚Äôs model.

There is a growing number of companies that provide machine learning services tailored for specific purposes, such as speech recognition, text analysis, or image classification.

You don‚Äôt get access to their models directly ‚Äî that‚Äôs their secret sauce ‚Äî but you can use these models through an API.

and many others‚Ä¶ these kinds of services are popping up all over.

If your app needs to perform one of the specific tasks offered by these services, you should definitely consider using them.

How this works: Your mobile app simply sends an HTTPS request to a web service along with any required data, such as a photo taken by the device‚Äôs camera, and within seconds the service replies with the prediction results.

Of course, you need to pay for this privilege, usually by the request. But you have nothing else to worry about: the only thing you need to do is talk to the service‚Äôs API endpoint from within your app. Often there is an SDK that makes the service easy to integrate.

The service provider keeps their model up-to-date by re-training it (using their own data), but this all happens behind the scenes. Any time they improve their model, you automatically benefit from it. You don‚Äôt have to know anything about machine learning in order to use these services.

Upsides of using such a ‚Äúcanned‚Äù machine learning service:

Using a fully-managed machine learning service is a great option if an existing model suits all your needs. For a lot of mobile apps this is the absolute right thing to do!

If your data is unique in some sense or existing solutions are not satisfactory, you‚Äôll need to train your own model.

Data is key to doing successful machine learning. It‚Äôs the quality and quantity of the data that makes all the difference. If you‚Äôre going to do your own training, you need lots of data.

Once you have gathered your training data, the next decision is where and how to train. It depends on the complexity of your model and the amount of training data that you have.

Unless you have your own data center or are rolling in cash, it makes most sense to rent computer power. There are many cloud platforms that are happy to take your money. These days you can even rent GPUs-in-the-cloud for training deep learning systems.

So it‚Äôs for you to decide: is renting cheaper than buying? However, there are other considerations besides price. Let‚Äôs look at some of those.

There are two options here:

We‚Äôll look at general purpose cloud computing first.

How this works: You rent one or more computers in someone else‚Äôs data center. Whatever you do on these computers is completely up to you. To train your model, you give the cloud computer access to your training data and run your favorite training software on it.

When you‚Äôre done training, you download the learned parameters for the model and delete the compute instance. You only pay for the compute hours used to train the model. Now you have a trained model that you can use anywhere you like.

Examples of this kind of service are Amazon EC2 and Azure Virtual Machines. For deep learning you can even rent instances with fast GPUs.

The other cloud option is hosted machine learning.

Several companies, such as Amazon, Microsoft, and Google, now offer machine learning as a service on top of their existing cloud services.

How this works: You don‚Äôt need to have the expertise to train models. You simply upload your data, choose the kind of model you want to use, and let the machine learning service take care of all the details.

This option is the step between using a fully managed service and doing everything yourself. It‚Äôs definitely a lot easier than doing your own training, especially if you‚Äôre not really comfortable training your own models.

However‚Ä¶ most of these services do not let you download your trained models. So for the inference part of your app you have no choice but to use their platform as well. You can‚Äôt take the trained model and load it onto a mobile device and run the predictions on the device itself. You always need to call their API and send along the user‚Äôs data for doing inference.

This may not be a problem for your app but it is something to be aware of before you get started. For example, when using Microsoft Azure Machine Learning you‚Äôre basically locked into using Azure forever. If you want to switch to another service, you can‚Äôt take your trained model with you ‚Äî you‚Äôll have to train from scratch on the new platform and eat the cost for that again.

These kinds of services charge you for the compute hours you use during training, plus any storage for your training data. Since the service provides the API that your app uses to request predictions, you also pay for each prediction request.

How this works: This is really no different from training in the cloud, except that you use one or more computers that you own. You load your favorite machine learning package on the computers, give them access to your data, and set them off training.

If you‚Äôre really serious about deep learning, or if you happen to have a number of spare computers lying around anyway, then this option might be cheaper (in the long run) than renting someone else‚Äôs computer.

One issue with training in the cloud is that you need to upload your data to the cloud service. Since digital storage is their business, these cloud companies usually provide good security for your data. However, your data may be so sensitive that you don‚Äôt want it to leave your premises. In that case, you‚Äôll need to do the training in-house as well.

If your model is small enough, training on your own hardware is a sensible choice. However, for big models with lots of training data, using a cloud service makes it easier to scale up quickly should you need more resources.

Using a ‚Äúhosted‚Äù machine learning service ‚Äî where you provide your own data but the service takes care of training ‚Äî has the big disadvantage that you don‚Äôt own the trained model. If you use such as service, you‚Äôll also have to use their web API for doing predictions.

So if you want to train on your own data and be able use the trained model offline, then you are limited to these options:

You‚Äôll definitely want to compare pricing between the different service providers. Most cloud services are very similar in the features they offer, so you might as well pay as little as possible. üí∞üí∞üí∞

For training it‚Äôs pretty obvious that it should happen off-device, whether it‚Äôs on your own computers or on computers that you rent.

But for inference you have the choice of doing it on the device, without requiring a network connection.

Let‚Äôs look at the options:

As with everything, whether it‚Äôs better to use a server or to do inference locally on the device, depends on a few tradeoffs.

One tradeoff could be speed: is it faster to run inference on the (relatively slow) mobile device, or is it quicker to send a network request to a fast and beefy server, do the inference there, and then send a reply back?

And some inference tasks are just going to be impossible on the device ‚Äî it simply may not have enough processing power or RAM, or it may be limited by some other constraint.

Which option is more practical totally depends on your use case.

How this works: You set up a server ‚Äî either your own machine or one that you rent in the cloud ‚Äî and load your trained model onto it. The server publishes the endpoint for a web API that your app talks to over the internet.

Suppose you have an app that turns a photo into digital art using deep learning. Users can choose different effects for their photos. The app sends the photo to the server, the server feeds the photo through the deep learning network to apply the desired effect, and a few seconds later it sends the modified image back to the app.

Using a server for inference keeps the mobile app simple. All the complexity is on the server, which is always under your control. You can improve the model or add new features any time you want. To deploy the improved model, simply update the server ‚Äî you don‚Äôt have to update the app itself.

With a hosted machine learning service it only takes a click on a button to deploy the trained model to a web API. By creating and hosting your own API, you get more flexibility but the obvious downside is‚Ä¶ well, that you need to do everything yourself.

If your app is a big success and has millions of (paying) users then it might be worth running your own inference backend. For many apps it‚Äôs probably cheaper and a lot less hassle to use a full-service machine learning in-the-cloud solution.

How this works: You load the model‚Äôs learned parameters into the app. To make a prediction the app runs all the inference computations locally on the device, on its own CPU or GPU ‚Äî it does not need to talk to a server.

This is the domain of frameworks such as BNNS and Metal CNN on iOS, but some machine learning packages such as TensorFlow and Caffe also run on the device.

Speed is the major reason for doing inference directly on the device. You don‚Äôt need to send a request over the internet and wait for the reply ‚Äî instead, the prediction happens (almost) instantaneously.

Take that same example of turning photos into ‚Äúdeep art‚Äù: what if you wanted to do this to a live camera feed in realtime? There is no way you can do this by sending network requests ‚Äî it must be done directly on the device.

One of the big benefits of doing inference on a server is that you can put improved models into action immediately: upload the new model to the server and you‚Äôre done. With a mobile app this is not as straightforward, since you need to push out the improved model to all existing app installs somehow.

If you re-train the model often then you may need to set up some infrastructure for distributing the updated model parameters to the users‚Äô devices. So you may need to keep a server around after all.

Upsides of doing inference on the device:

There is another potential issue you need to be aware of: other developers can dig around inside your app bundle. It‚Äôs easy to copy over the learned parameters, and if you‚Äôre including a TensorFlow graph definition or caffemodel file it‚Äôs very simple for unscrupulous people to steal your entire model. You may want to obfuscate this data if having it gives you a competitive advantage.

As you can tell, there are lots of options to choose from!

And no doubt we‚Äôll see many more machine learning services appear on the market in the coming months and years.

What‚Äôs best for your app and your business ‚Äî and your users! ‚Äî really depends on the kind of machine learning you‚Äôre doing, so it‚Äôs impossible to give tailored advice without knowing the exact details.

But at least I hope this blog post has given you an idea of the possibilities!

If you‚Äôre interested in learning more about doing deep learning on iOS devices, then get in touch and let‚Äôs chat. üòÑ|||

The choices to make when you decide to add deep learning to your mobile app