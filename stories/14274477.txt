Neural networks are no less than machines themselves with several buttons and throttles. It takes so much delicate and intricate pieces to run inside in harmony with careful supervision to build a robust, expressive, stable and, most importantly, generalizing neural networks. Understanding neural networks is challenging and their behaviour is affected by so many possible alterations. As a practitioner, one must know about some of the heuristics underlying to better make decisions and gain better clarity while dealing with them.

This post discusses about only some obscure corners of neural networks that significantly regulate the overall behaviour of neural networks. I talk about few of the internal dynamics inside neural networks while leaving out some of the obvious and analytical stuffs. I will rant mostly about the intuitions and effects of various possibilities inside neural networks.

What is expressivity of neural networks? It is the characterization of how different knobs inside  a neural architecture changes the properties of the function it is  supposed to approximate or learn. By “knobs”, I mean the hyper-parameters, activation functions, depth and width of the network. Our point of interest is a learning machine which is more than just a function approximator.

A neural network with non-linearity stretches and squishes the data over all the layers in such a way that at the end the classes in the final representation is separable linearly. The same process can, otherwise, be described like: the network learns complex rules from the data and firing of a neuron is more like a complex if-condition. I have seen few people getting confused here when they look at visualizations: they think that the network learns complex decision boundaries. I think more accurate is that they learn complex representations of the data and in the end the separation is still linear.

During traversal (or forward pass), at each layer the different regions of the input get bind to a common output region and the same thing happens with this output at the next layer. A compositional structure is built in this way.

Space-folding: Bengio et. al. have argued that each layer in a deep neural network acts like a folding operator and the operation it perform resembles folding a piece of paper along a particular axis. As a result, any function computed on the folded space will be applied successively to all the internal overlaying regions.

In the above diagram, if you create a hole in (3), the hole will get applied on the two regions in (2) and successively all the four regions in (1).

The no. of piecewise linear segments an input space can be split into grows exponentially with the depth but only grows polynomially along the width. To be more clear, look at the diagram below and the explanation that follows it.

To explain visually, see the leftmost figure which represents the hypeplanes drawn in a single-layered neural network. The central diagram represents boundaries after the first two layers. Clearly, inside every first layer region, the second layer has drawn few boundaries of it’s own leading to notable growth in number of linear regions. If layer 0 had contained the same number of neurons which layer 0 and layer 1 combined contains now, the number of linear regions in layer 0 would have been only few more.

Does that mean width is insignificant ? Or keeping depth constant and varying width doesn’t have an impact ? Think of the neural network as a pyramid. You can’t just stack thin layers. Each layer must have sufficient number of units for the higher layers to perform. Let’s provide a simple but widely-talked example here: the following data (left fig.) has two classes A (blue) and B (red).

As you can observe, no hyperplane can classify the above raw data. We need a neural network that can represent the data in a way that A and B can be separable. Having said that, that network  must have at least one layer with at least 3 hidden units. In such case, the data can be finally represented by a bowl shaped curve (right fig. above) with blue occupying the base and red running through the sides and a hyperplane can cut the bowl somewhere between the red bottom and blue sides! As you can see there is a 3-d representation (and hence at least three neuron units) of the 2-d data that will contain linearly separable classes. A structure of this type is impossible to attain with a network without at least 3 units in any layer and we will end up in some sort of local minima.

What was not separable in two dimensions in now separable in three dimensions!

It is a well known fact that depth contributes to generalization while width contributes to memorization. Deep neural networks are expressive (having greater linear regions) but wider and shallow ones are more stable due to same reasons that effect of perturbations grows along the depth.

Sensitivity of neural networks to it’s weights: In a DNN with multiple layers, the weights belonging to initial layers matters more. In other words, the changes (or perturbation in NN nomenclature) in these weights brings out more change in the overall behaviour of the network than the weights in the higher layers (closer to the output).  If you make a change in the parameters of a layers, the effect of this change grows exponentially in the following layers. Empirically, introduction of noise to the initial weights results in more drop in accuracy than the same operation on the higher layer weights. Also note that during backward pass, the initial weights are the ones that get the weakest signal from the gradient. Hence, higher learning rate for these weights are recommended when your depth is considerable.

Imagine a situation where you are performing some sort of text classification using recurrent neural networks coupled with a softmax layer after the last state of the RNN (which is basically a single layer neural network). It is very common that RNN weights capture the sequential information in text and stores them in hidden states. If we replace the singular softmax layer at the end with a multi-layer neural network for the classification, will the RNN be able to recieve strong gradients for it’s weights as it was receiving in the initial setup ? Does that mean increasing the depth of the feed-forward network, which lack the power RNN has, at the end now causes deterioration in performance ?

Generally speaking, every parameter choice or perturbation in any layer causes the effect to grow exponentially in the layers that follow. It follows conclusively that depth plays an important role in the expressivity of neural networks.

One funny (but obvious) thing is that during forward pass, initial weights are key role player but during backward pass, their gradients are the weakest. Injustice!

Designing a network for efficient backprop has no rules but certain theoretical heuristics can help a practitioner to take better decisions.

Generalization: Imagine multiple small samples of data collected from a large population. Due to the measurement noise in each sample (which is very common), each sample differ with the others to some extent. A model trained on sample A will perform poorly on sample B. Also, each of these samples might have different local minima which is much different from the true global minima of the whole data. The trick is not to fall in  any such local one. The goal of generalization is to train on few samples but  perform good on the rest too by not modelling noises as much as possible.

Bias and variance: The error can be divided into two terms: bias and variance. The bias  indicates how much the network is far from the true function when  computed over all the samples while variance how different the network behaves when treated with each sample. Before training (or at underfit stage), bias is very large as we are so far from the original function whereas variance is small as there is no data (or sample here) – dependent influence on the model. On successful training, bias reduces to a satisfactory level. So does the variance. But if we overfit, we also model the noise in the sample we used to train. In that case, the network behaves differently for new sample leading to high variance. The main adversaries to backpropagation are: multiple local minima, non-quadratic, non-convex and high-dimensional cost surfaces.

Why stochastic learning triumphs over batch learning? : Besides being faster than batch learning, stochastic learning has one more significant property. Since, it is being trained on one example at a time, the updates are quite noisy. The update for one example can be a little bit unfavourable for the next example. This noise is negligible in batch learning where you pass the whole data and gradient is computed over the mean error. However, this noise in stochastic learning is advantageous.  In a non-linear network, there are multiple local minima generally present which means there are multiple basins with differing depths. After the weight initialization, we are generally placed near to one of the basins and in batch learning, we tend to fall in the nearest basin or the nearest local minima. The noisy updates in stochastic learning helps in letting us jump into a different basin or finding a different minima (preferably with higher depth and better solution). There are few other advantages too: when we are getting new data into our training system or when the function we are trying to learn is changing over time. Hence, stochastic learning which is a means of online learning can try to capture the change.

Fluctuations due to noisy updates: Let’s talk about disadvantages too. This jumping, we talked about, also makes the model dance around the minima and sometimes without actually falling into it. This is the price we pay! This fluctuations are basically erroneous weight updates which can be mitigated to a lot extent by tweaking the learning rate. Another method, which is widely practiced now is instead of taking a single-sized example we take a small group of examples.

To speed up learning, we often deploy second-order gradient learning which apart from computing gradients also estimate the curvature of the cost surface which, in turn, helps to approximate the location of the minima.

Another trick!  Shuffle the examples in such a way that the network don’t see a class frequently. If we have two classes and we give example like “AAAAAAAABABBB”, by the time the network gets to see an example of class B, it has greatly trained itself to perform well for A and finally performs a very insignificant  weight update towards performing well for B.

Activation functions: Remember the example above where we transformed the 2-d data to a 3-d bowl ? These are transformations that are aided by activation functions like sigmoid, tanh or relu. It is a common fact that one must not initialize large weights when using activations like sigmoid because the outputs will get saturated around either 0 or 1. Functions like tanh are symmetric about the origin i.e. they output values between -1 and 1 and mostly centered around 0. This paves the way for more or less normalized inputs to hidden layers and hence leads to faster convergence. tanh still saturates, but lesser. A cheaper alternative is Relu functions that doesn’t saturate on the upper bound and hence greatly accelerates training. But Relu outputs are still sparse if there are lot of negative values in their inputs.

Large weights saturates activation functions and, thus, leading to smaller gradients. Smaller weights, by virtue of being small, leads to smaller gradients. Smaller gradients slows down learning as gradients are the main source of knowledge for learning. Weights that make the neuron outputs lie over the linear part of the activation functions are optimal. Achieving such condition can be ensured by normalization of inputs, proper activation function and appropriate weight initialization. It is preferable that the data at all level inside the neural network have  .

When weights start near zero, they fall mostly in the linear region of activation functions (sigmoid and tanh are linear when the inputs to them are around zero). But, when there is need for non-linearity, the weights can shift themselves to extreme values (or move away from zero).

We generally choose a single learning rate for the whole network. As we have seen previously, in deep networks lower layer weights tend to learn slowly due to poor gradients. Hence, it is justified from our part to assign higher LR to them than the layer that follows. So, LRs can be assigned in a descending fashion across the depth. The main objective of giving different LRs to different layer weights is to make all weights move at a equal speed. Differentiating the cost w.r.t to each weight is equivalent to computing velocity of the weights (analogously in motion, velocity is derivative of distance w.r.t time!)

Moreover, in general, higher LR will cause divergence, i.e. you will dance around the minima but never approaching towards it whereas  lower LR will slow down convergence by taking very tiny steps! A balanced situations is often achieved by modifying the LR from time to time depending on the current error and it’s relationship with previous error.

In the above picture, is the optimal learning rate that will take just one step to reach the minima (fig. (b)). We start in a more or less favourable condition i.e. near the minima. In (a), we assign  , a L.R. which is much lesser than  and hence we will be requiring multiple steps to reach the bottom. In (c), our  lies between  and  . In such scenario, we will make zig-zag moves but still reach minima at some point. Application of momentum is useful in such cases. In (d), we assign  higher than  which will move the weights far away from minima or causing very high error. Analytical method to approximate  is given in [2] in section 5.1.

Neural networks are quite sensitive to it’s configurations. Nowadays, we have more or less reached a stable environment where we seldom need to peek at what the configurations are. We mainly deal with regulating depth, LR etc. while leaving the rest of the intricacies to themselves. But knowing your model helps in not only debugging scenarios but also when you are a researcher and attempting to fit some new approach or architecture. Knowing why network isn’t learning is divine according to me. Having said that, neural networks are still very robust and often can compensate for the mistakes you do automatically!|||

Neural networks are no less than machines themselves with several buttons and throttles. It takes so much delicate and intricate pieces to run inside in harmony with careful supervision to build a robust, expressive, stable and, most importantly, generalizing neural networks. Understanding neural networks is challenging and their behaviour is affected by so many possible…