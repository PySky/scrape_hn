Even though it’s been superseded by serbench, I wanted to give it some attention because it illustrates a lot of oversights even though on first glance it seems to be a valuable source of information.

And probably some other factors that I’ve missed. It’s no surprise then that many existing benchmarks can be criticised for missing one or more of these points. Let’s take a look at some:

However, it’s incredibly hard to create reproducible, consistent benchmarks. That’s not so surprising, as Hanselman points out, there are many factors to control for:

There exist various articles on benchmarks of serializers already. Even various github projects exist without articles attached to them. This goes for every language, a testament to how much performance matters to some people.

This apparently needs a manual DLL download as described by a comment in the code. Fixing that, don’t forget to input “100” as a program argument, as is stated in the article. This produces the following output:

As you can see, not all tests run succesfully. There are plenty of exceptions and checks that are failing, making comparison between other benchmarks, even its own, harder. Not to mention that the article hasn’t given the full output to compare against.

Aside from problems running it, there are a couple of measuring problems I noticed:

There has been a lot of work to add a lot, though not all, Serializers used in the .NET environment. But I wouldn’t trust this benchmark to output very consistent measurements.

GLD.SerializerBenchmark’s README points to this project. However, at the time of writing, some of the shortcomings of the GLD project appear to also exist in Serbench. The Jil Serializer contains the same unnecessary stringreader/writer, Jil is still not initialized properly and CPU Affinity and Process Priority are not set.

It does seem to have collect the garbage before every run as well as having removed the duplicate StringWriter and giving Jil a new Options object every run as well as adding the Apolyton.FastJson reference directly in the repository. Despite that, there is no mention of having done a Memory Profile to ascertain if collection is done exclusively induced or that it happens during test runs. Also, serbench creates an extra thread upon which to run all the tests, which strikes me as odd.

However, I couldn’t get serbench to work. Apparently it requires something called NFX, which is capable of writing the results into a variety of outputs, such as an RDBMS. It seems they’re aiming for a benchmark that emulates running serializers in parallel, supposedly making it more real-life than the synthetic benchmarks usually shown. While chain-benchmarking would certainly be interesting, I’m quite skeptical as to what they’re trying to prove. Isolating serializers in synthetic benchmarks makes them easily comparable, but once you introduce a whole chain of software stacks, you’re bound to run into the issue that everyone has their own combination. Soon, you’ll have to create an all-encompassing benchmark suite that runs on multiple platforms, has pluggable frameworks and pluggable serializers.

Admittedly, I did not spend too much time getting it to work, but I do expect being able to open the project and press start and it should work. I hope they’ll make it easier to use in the future.

While not a complete benchmark for a vast array of serializers, it provides a relatively easy setup to create one.

One of the examples shows how to use it, and in it you can see that it doesn’t have the smaller oversights found in the GLD and serbench projects. It only uses streams where necessary, though doesn’t compare the same library with and without streams. It also does garbage collection, but like GLD, it averages all the results into one number.

While the results will be reasonably trustworthy, here too, I am missing CPU Affinity and Process Priority. I can see this working for a quick one-on-one comparison, but not for a full benchmark suite.

I’m sure I’ve not looked at all possible benchmark software for serializers in the .NET environment, my time is limited after all. However, I did create a project which aims to account for as many factors as possible, for a wide variety of serializers, in an isolated, synthetic benchmark style.

As is usual in proper benchmarks, to minimize variance and increase reproducability, one states the used hardware and software.

As you can see in these screenshots, I run all tests on a non-overclocked i5–4570

With 8 GB of RAM at the following clock/timings.

As for the software, I’m running a 64-bit Windows 10 Professional, with the latest updates, all applications except file explorer closed, but with automatic updates and all privacy sensitive settings disabled. I noticed that automatic updates easily takes up all your disk I/O and one whole CPU core while downloading and installing updates in the background. This would not be viable for a benchmark.

I’ve used Visual Studio 2017 (not an update version) with .NET Framework 4.6.2 to compile the C# solution.

For used libraries in C# see the packages.config.

For C++ I used Visual Studio 2017 (not an update version) with Cereal 1.2.2 (which uses rapidjson and rapidxml), protobuf 3.2.0 (static library can be found on the repository)

What most other benchmarks do, is create a couple of objects to serialize and deserialize, run those a number of times in a row and calculate the average. While this gives you a representation of total time required, it does lose some valuable data.

For this project, I want to create a moderately large object, measure serialization and deserialization separately and store each single run in a list of measurements. This way I can create an OHLC graph. However, I’m going to alter the definition of the various points. High and low will be the highest time and lowest time measured respectively, but the open point will be the 20/100th measurement and the close point will be the 80/100th measurement.

On the left you can see an example with 250 repetitions. The fastest sample(L for Low) is 2117 µs, the slowest sample(H for High) is 2888 µs. All measurements are sorted from lowest to fastest. The 20/100th measurement(O) is measurement #50, which is 2117 µs the 80/100th measurement(C) is measurement #200, which is 2302 µs. You can see that most of the samples (60% of them) are in the 2117–2302 µs range, with only a couple of outliers below that but more outliers above that.

This way, you have information about how jittery/consistent the library is as well as having a general idea how the library will perform.

Further, all benchmark processes will be run on CPU #0 (so CPU affinity is set), having Process Priority High and will be run as administrator, so that the previous two settings will be possible to do by the process itself.

As a last step, I’ll profile the memory to see if garbage collections are strictly occurring when I want them to and not during a test.

My first test will be Jil in various setups (normal, streaming, with and without attributes on the data object class, with and without options) on x86 and x64.

My second test will be a few JSON serializers in C# on x86 and x64.

Third test will be a few binary serializers in C# on x86 and x64.

My third test will be comparing a couple of serializers in C# to ones in C++ and NodeJS.

Just to make sure I’ve done it correctly, I’d like to run you through some of my code.

First thing the program does is set Affinity and Priority.

This is the code used to run a single measurement. .NET 4.6 introduces the GC.TryStartNoGCRegion function, which allows you to tell the garbage collector to pre-allocate memory and tell it to try and not run garbage collection until you end the region. I try to allocate 1M before calling the action.

Each test is warmed up, so we don’t measure cold startup time. Then the tests are run for 250 repetitions, which is hard-coded. Technically, this is the only “measurement” that is thrown out for all tests.

For all tests, I create an object with 1000 documents. I tried to give a representative object, containing datetimes, UTF-8 strings and an integer. I realise that many more combinations are possible, but I’m not sure if they would add much.

Of course, there are some variations of the specific action, such as when streams are required or when a specific file with preloaded json/xml/binary contents are read into memory before running the test, or when a different type of Person/Document is required for the library, but this is the basic structure for all types.|||

There exist various articles on benchmarks of serializers already. Even various github projects exist without articles attached to them. This goes for every language, a testament to how much…