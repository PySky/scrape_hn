A little while ago I wrote a series of blog posts about Deep Learning and Kubernetes, using the Canonical Distribution of Kubernetes (CDK) on AWS and Bare Metal to deploy Tensorflow. These posts were highly technical, fairly long and difficult to replicate.

Yesterday I published another post, explaining how the recent addition of GPUs as first class citizens in CDK changed the game. This last post has had a lot of success. Thank you all for that, it is always pleasant to see that content reaches its audience and that people are interested in your work.

The wide audience it reached also led me to conclude that I should revisit my previous work in light of all the comments and feedback I got, and explain how having trivialized GPUs access in Kubernetes can help scientists to deploy scalable, long running Tensorflow jobs (or any other deep learning afaiac).

This post is the result of that thought process.

So today, we will:

For the sake of clarity, there is nothing new here, it is an updated, condensed version of my series about Tensorflow on Kubernetes Part 1, Part 2 and Part 3 that benefits from the latest and greatest additions to the Canonical Distribution of Kubernetes.

To replicate this post, you will need:

If you experience any issue in deploying this, or if you have specific requirements (non default VPCs, subnets‚Ä¶), connect with us on IRC. I am SaMnCo on Freenode #juju, and the rest of the CDK team is also available there to help.

First of all let‚Äôs deploy Juju on your machine as well as a couple of useful tools:

Now let‚Äôs add credentials for AWS so you can deploy there:

The above line is interactive and will guide you through the process. Finally, let‚Äôs download kubectl and helm

Clone this repository to access the source documents:

OK! We‚Äôre good to go.

First of all, we need to bootstrap in a region that is GPU-enabled, such as us-east-1

This will take some time. You can track how the deployment converges with¬†:

At the end, you should see all units ‚Äúidle‚Äù, all machines ‚Äústarted‚Äù and it should look green.

At this point in time, as CUDA enablement is now fully automated, we can safely assume that our cluster is ready to operate GPU workloads. We can download the configuration:

The default username:password for the dashboard or Graphana is ‚Äúadmin:admin‚Äù.

First of all you need to know the VPC ID where you are deploying. You can access it from the AWS UI, or with

Now we need to know the subnets where our instances are deployed:

And now we need the list of Security Groups to add to our EFS access:

At this point, if have an EFS already deployed, just store its ID in EFS_ID, and make sure you delete all its mount points. If you do not have an EFS, create one with¬†:

From the UI, you can now check that the EFS has mount points available from the Juju Security Group, which should look like juju-3d57d67a-8603‚Äì4161‚Äì8fc2-dc7e1ee08eef.

Note: users of this Tensorflow use case have reported that EFS is a little slow, and that other methods such as SSD EBS are faster. Consider this the easy demo path. If you have an advanced, IO intensive training, then let me know, and we‚Äôll sort you out. This is something we are already working on.

We are now done with the Juju and AWS side, and can focus on deploying applications via the Helm Charts. Let‚Äôs switch to our charts folder:

Now let‚Äôs create our configuration file for the EFS volume. Copy efs/values.yaml to¬†./efs.yaml

and update the id of the EFS volume. It shall look like:

Now deploy it with Helm

It will output something like:

In the KubeUI, you can see that the PVC is ‚Äúbound‚Äù, which means it is connected and available for consumption by other pods.

There is a non GPU workload available in the charts repo called ‚Äúdistributed-cnn‚Äù. It is an example I took from a Google Tensorflow Workshop and adapted to port it to Helm.

Let‚Äôs copy the values.yaml file and adapt it:

and update the id of the EFS volume. The different sections are:

which will leverage the Xip.io service to connect later on. The section of the file itself:

Now deploy it with Helm

depending on the number of workers and parameter servers, the output will be more or less extensive. At the end, you can see the indications:

That‚Äôs it, you can connect to your tensorboard and see the output.

If you want to compare results, just create several values.yaml files and upgrade your cluster:

When you are happy and this and ready to get rid of it, you can delete it with

It‚Äôs always nice to understand things by copying what others have already done. That‚Äôs open source. It‚Äôs also the fastest way to get things done in 90% of the time. Don‚Äôt reinvent the wheel.

Then the next step is the DIY, and, if you are reading this post now, you probably want to run your own Tensorflow code.

So start a new Dockerfile, and fill it with:

Adapt this to your need (version‚Ä¶), and write this worker.py file that you‚Äôll need. Make sure of:

The example Google gives looks like:

Once you have that ready, build and publish your images. You can have a CPU only image for the Parameter Servers, and a GPU one for the workers, or just one of them. Up to you to decide.

Note: Example Dockerfiles are available in the docker-image repository for this, with several version of Tensorflow

Then copy the values.yaml file from the tensorflow chart, and adapt it by pointing the cluster to your images. Then follow the same method as before

To tear down the Juju cluster once you are done:

This blog shows the progress that has been made in 3 months in the Canonical Distribution of Kubernetes to deploy GPU workloads. What used to take 3 posts now takes only 1 üòÅ

The Tensorflow chart is being use by various teams right now, and they are all giving very useful feedback. We‚Äôre now looking into

Let me how that goes for you, and if you have a use case for Tensorflow and would like to use this, let me know, so we collectively end up with a useful chart for everyone.

And again, don‚Äôt hesitate to come to us on Freenode #juju to discuss and onboard the Kubernetes+GPU+Tensorflow train!|||

A little while ago I wrote a series of blog posts about Deep Learning and Kubernetes, using the Canonical Distribution of Kubernetes (CDK) on AWS and Bare Metal to deploy Tensorflow. These posts were‚Ä¶